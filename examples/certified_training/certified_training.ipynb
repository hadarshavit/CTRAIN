{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Training with CTRAIN\n",
    "\n",
    "In this example, we train the standard CNN7 Architecture proposed by Shi et al. on the MNIST dataset using CTRAIN. We want to train for certifiable robustness against perturbations in the $l_\\infty$ norm ball around inputs with radius $\\epsilon=0.1$.\n",
    "For that, we utilise IBP training with the improvements by Shi et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the `torch` library as well as the required functions from the `CTRAIN` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from CTRAIN.model_definitions import CNN7_Shi\n",
    "from CTRAIN.model_wrappers import ShiIBPModelWrapper\n",
    "from CTRAIN.data_loaders import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the MNIST dataset using `CTRAIN` and define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset - Min value: -0.4242129623889923, Max value: 2.821486711502075\n"
     ]
    }
   ],
   "source": [
    "in_shape = [1, 28, 28]\n",
    "train_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")\n",
    "\n",
    "model = CNN7_Shi(in_shape=in_shape, n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network certifiably, we have to wrap it in a `CTRAIN` model wrapper. If you desire to use another certified training method, please import the respective wrapper from the `CTRAIN.model_wrappers` package. We initialise the wrapper with the required arguments of the training process, such as the number of warm up epochs, i.e. the number of epochs where the model is trained on natural loss, or the number of ramp up epochs, i.e. the number of epochs where the epsilon value is gradually increased to the final training epsilon. Please consult the documentation to set the other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = ShiIBPModelWrapper(\n",
    "    model, \n",
    "    input_shape=in_shape, \n",
    "    eps=0.1,\n",
    "    num_epochs=70,\n",
    "    warm_up_epochs=0,\n",
    "    ramp_up_epochs=40,\n",
    "    lr_decay_milestones=(50, 60),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate the training process by calling the `train_model` function of the wrapped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.train_model(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the resulting model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wrapped_model.state_dict(), '../../mnist_0.1_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
