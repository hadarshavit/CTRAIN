{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation using CTRAIN\n",
    "\n",
    "CTRAIN offers seamless integration of sophisticated hyperparameter optimisation using SMAC3.\n",
    "First, we import the necessary `torch` library and `CTRAIN` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding complete_verifier to sys.path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from CTRAIN.model_definitions import CNN7_Shi\n",
    "from CTRAIN.model_wrappers import ShiIBPModelWrapper\n",
    "from CTRAIN.data_loaders import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter, we load the MNIST dataset and define the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset - Min value: -0.4242129623889923, Max value: 2.821486711502075\n"
     ]
    }
   ],
   "source": [
    "in_shape = [1, 28, 28]\n",
    "train_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")\n",
    "\n",
    "model = CNN7_Shi(in_shape=in_shape, n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform HPO, we have to wrap the network around one of the model wrappers of `CTRAIN`. Here, we choose the Shi IBP wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = ShiIBPModelWrapper(\n",
    "    model, \n",
    "    input_shape=in_shape, \n",
    "    eps=0.1,\n",
    "    num_epochs=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter we perform the parameter tuning, while evaluating probed configurations on the test set. Furthermore, we provide sensible defaults to guide the optimisation. To save resources, we do not execute the HPO in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.hpo(train_loader=train_loader, val_loader=test_loader, defaults={\n",
    "    'warm_up_epochs': 0,\n",
    "    'ramp_up_epochs': 50,\n",
    "    'lr_decay_factor': 0.2,\n",
    "    'lr_decay_epoch_1': 10, # added unto warm_up and ramp_up epochs\n",
    "    'lr_decay_epoch_2': 10, # added unto warm_up, ramp_up and lr_decay_1 epochs\n",
    "    'l1_reg_weight': 1e-06,\n",
    "    'shi_reg_weight': 1,\n",
    "    'shi_reg_decay': True, \n",
    "    'train_eps_factor': 1,\n",
    "    'optimizer_func': 'adam',\n",
    "    'learning_rate': 5e-04,\n",
    "    'start_kappa': 1,\n",
    "    'end_kappa': 0\n",
    "}, output_dir='./smac/shi_mnist_0.1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the model trained on the optimal configuration and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wrapped_model.state_dict(), './shi_incumbent_cifar10_2_255.pt')\n",
    "\n",
    "wrapped_model.eval()\n",
    "\n",
    "std_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader=test_loader, test_samples=1_000)\n",
    "\n",
    "print(f\"Std Acc: {std_acc}, Cert. Acc: {cert_acc}, Adv. Acc: {adv_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
