{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CTRAIN","text":"<p>CTRAIN is a unified, modular and comprehensive package for training and evaluating certified training techniques for neural networks.</p>"},{"location":"#overview","title":"Overview","text":"<p>CTRAIN integrates multiple state-of-the-art certified training approaches, which are:</p> <ul> <li>Interval Bound Propagation (IBP) (Gowal et al., 2018 with the Improvements of Shi et al., 2021)</li> <li>CROWN-IBP (Zhang et al., 2020)</li> <li>SABR (M\u00fcller et al., 2023)</li> <li>TAPS (Mao et al., 2023)</li> <li>STAPS (combination of SABR and TAPS, Mao et al., 2023)</li> <li>MTL-IBP (De Palma et al., 2023)</li> </ul> <p>Furthermore, CTRAIN enables easy evaluation of the adversarial and certified robustness of a given neural network, using the following methods:</p> <ul> <li>PGD (Madry et al., 2018)</li> <li>IBP (Gowal et al., 2018)</li> <li>CROWN-IBP (Zhang et al., 2020)</li> <li>CROWN (Zhang et al., 2018)</li> <li>\u03b1,\u03b2-CROWN (Xu et al., 2020, Wang et al., 2021)</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>CTRAIN has the goal of providing an unified, comprehensive, accessible and flexible framework for certified training. Key features include:</p> <ul> <li>Standardised, modular and hihly-configurable implementations of popular and performant certified training methods based on the popular <code>auto_LiRPA</code> library</li> <li>Unified and accessible interface through model wrappers</li> <li>Based on PyTorch for easy integration into common machine learning pipelines</li> <li>Seamless integration of sophisticated hyperparameter optimisation using SMAC3 (Lindauer et al., 2022)</li> <li>Comprehensive evaluation tools for adversarial and certified robustness</li> <li>Support for both incomplete and complete verification methods, including the state-of-the-art complete verification system \u03b1,\u03b2-CROWN</li> <li>Detailed documentation with API reference, setup guide, and usage examples</li> <li>Open-source with a permissive MIT license</li> <li>Active development and maintenance by the Chair for Artificial Intelligence Methodology at RWTH Aachen University</li> </ul>"},{"location":"#installation-and-quick-start","title":"Installation and Quick Start","text":"<p>First, install CTRAIN using <code>pip</code>:</p> <pre><code>pip install CTRAIN\n</code></pre> <p>Or, to setup the package for development purposes, run:</p> <pre><code>git submodule init\ngit submodule update\npip install --no-deps git+https://github.com/KaidiXu/onnx2pytorch@8447c42c3192dad383e5598edc74dddac5706ee2\npip install --no-deps git+https://github.com/Verified-Intelligence/auto_LiRPA.git@cf0169ce6bfb4fddd82cfff5c259c162a23ad03c\"\npip install -e \".[dev]\"\n</code></pre> <p>Then, you can train and evaluate the standard CNN7 architecture proposed by Shi et al. on the CIFAR-10 dataset using the IBP certified training technique in 12 lines of code:</p> <pre><code>from CTRAIN.model_definitions import CNN7_Shi\nfrom CTRAIN.data_loaders import load_cifar10\nfrom CTRAIN.model_wrappers import ShiIBPModelWrapper\n\ntrain_loader, test_loader = load_cifar10(val_split=False)\nin_shape = [3, 32, 32]\n\nmodel = CNN7_Shi(in_shape=in_shape)\nwrapped_model = ShiIBPModelWrapper(model=model, input_shape=in_shape, eps=2/255, num_epochs=160)\n\nwrapped_model.train_model(train_loader)\nstd_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader)\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>CTRAIN/\n\u251c\u2500\u2500 attacks/            # Implementation of attacks\n\u251c\u2500\u2500 bound/             # Bound computation approaches\n\u251c\u2500\u2500 complete_verification/ # Complete verification\n\u251c\u2500\u2500 data_loaders/      # Dataset loading utilities  \n\u251c\u2500\u2500 eval/              # (Incomplete) Evaluation functionality\n\u251c\u2500\u2500 model_definitions/ # Neural network architectures\n\u251c\u2500\u2500 model_wrappers/    # Model wrappers for different approaches\n\u251c\u2500\u2500 train/            # Robust/Certified Training implementations\n\u251c\u2500\u2500 util/             # Utility functions\n\u2514\u2500\u2500 verification_systems/ # External verification tools\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available in the docs directory, including:</p> <ul> <li>API Reference</li> <li>Setup Guide</li> <li>Usage Examples</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"#maintainers","title":"Maintainers","text":"<p>This project was developed at the Chair for Artificial Intelligence Methodology at RWTH Aachen University by Konstantin Kaulen under the supervision of Prof. Holger H. Hoos. Konstantin Kaulen is the current core-maintainer of the project.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit issues and pull requests.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project incorporates the \u03b1,\u03b2-CROWN verifier which is developed by a multi-institutional team led by Prof. Huan Zhang. The \u03b1,\u03b2-CROWN components are licensed under the BSD 3-Clause license. Furthermore, CTRAIN depends heavily on the <code>auto_LiRPA</code> library, which is developed by the same team and also licensed under the BSD 3-Clause License.</p>"},{"location":"api/bound/","title":"Bound","text":""},{"location":"api/attacks/pgd/","title":"pgd","text":""},{"location":"api/attacks/pgd/#CTRAIN.attacks.pgd.pgd_attack","title":"<code>pgd_attack(model, data, target, x_L, x_U, restarts=1, step_size=0.2, n_steps=200, early_stopping=True, device='cuda', decay_factor=0.1, decay_checkpoints=())</code>","text":"<p>Performs the Projected Gradient Descent (PGD) attack on the given model and data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The neural network model to attack.</p> required <code>data</code> <code>Tensor</code> <p>The input data to perturb.</p> required <code>target</code> <code>Tensor</code> <p>The target labels for the input data.</p> required <code>x_L</code> <code>Tensor</code> <p>The lower bound of the input data.</p> required <code>x_U</code> <code>Tensor</code> <p>The upper bound of the input data.</p> required <code>restarts</code> <code>int</code> <p>The number of random restarts. Default is 1.</p> <code>1</code> <code>step_size</code> <code>float</code> <p>The step size for each gradient update. Default is 0.2.</p> <code>0.2</code> <code>n_steps</code> <code>int</code> <p>The number of steps for the attack. Default is 200.</p> <code>200</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop early if adversarial examples are found. Default is True.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to perform the attack on. Default is 'cuda'.</p> <code>'cuda'</code> <code>decay_factor</code> <code>float</code> <p>The factor by which to decay the step size at each checkpoint. Default is 0.1.</p> <code>0.1</code> <code>decay_checkpoints</code> <code>tuple</code> <p>The checkpoints at which to decay the step size. Default is ().</p> <code>()</code> <p>Returns:</p> Type Description <p>torch.Tensor: The generated adversarial examples.</p> Source code in <code>CTRAIN/attacks/pgd.py</code> <pre><code>def pgd_attack(model, data, target, x_L, x_U, restarts=1, step_size=.2, n_steps=200, early_stopping=True, device='cuda', decay_factor=.1, decay_checkpoints=()):\n    \"\"\"\n    Performs the Projected Gradient Descent (PGD) attack on the given model and data.\n\n    Args:\n        model (torch.nn.Module): The neural network model to attack.\n        data (torch.Tensor): The input data to perturb.\n        target (torch.Tensor): The target labels for the input data.\n        x_L (torch.Tensor): The lower bound of the input data.\n        x_U (torch.Tensor): The upper bound of the input data.\n        restarts (int, optional): The number of random restarts. Default is 1.\n        step_size (float, optional): The step size for each gradient update. Default is 0.2.\n        n_steps (int, optional): The number of steps for the attack. Default is 200.\n        early_stopping (bool, optional): Whether to stop early if adversarial examples are found. Default is True.\n        device (str, optional): The device to perform the attack on. Default is 'cuda'.\n        decay_factor (float, optional): The factor by which to decay the step size at each checkpoint. Default is 0.1.\n        decay_checkpoints (tuple, optional): The checkpoints at which to decay the step size. Default is ().\n\n    Returns:\n        torch.Tensor: The generated adversarial examples.\n    \"\"\"\n    x_L, x_U = x_L.to(device), x_U.to(device)\n    if data is None:\n        data = ((x_L + x_U) / 2).to(device)\n\n    lr_scale = torch.max((x_U-x_L)/2)\n\n    adversarial_examples = data.detach().clone()\n    example_found = torch.zeros(data.shape[0], dtype=torch.bool, device=device)\n    best_loss = torch.ones(data.shape[0], dtype=torch.float32, device=device)*(-np.inf)\n\n    # TODO: Also support margin loss (although not used in TAPS/SABR/MTL-IBP)\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n    for restart_idx in range(restarts):\n\n        if early_stopping and example_found.all():\n            break\n\n        random_noise = (x_L + torch.rand(data.shape, device=device) * (x_U - x_L)).to(device)\n        attack_input = data.detach().clone().to(device) + random_noise            \n\n        grad_cleaner = optim.SGD([attack_input], lr=1e-3)\n        with torch.enable_grad():\n            for step in range(n_steps):\n                grad_cleaner.zero_grad()\n\n                if early_stopping:\n                    attack_input = attack_input[~example_found]\n\n                attack_input.requires_grad = True\n\n                model_out = model(attack_input)\n\n                loss = loss_fn(model_out, target)\n\n                loss.sum().backward(retain_graph=False)\n\n                if len(decay_checkpoints) &gt; 0:\n                    no_passed_checkpoints = len([checkpoint for checkpoint in decay_checkpoints if step &gt;= checkpoint])\n                    decay = decay_factor ** no_passed_checkpoints\n                else:\n                    decay = 1\n\n                step_input_change = step_size * lr_scale * decay * attack_input.grad.data.sign()\n\n                attack_input = torch.clamp(attack_input.detach() + step_input_change, x_L, x_U)\n                adv_out = model(attack_input)\n\n                adv_loss = loss_fn(adv_out, target)\n\n                if early_stopping:\n                    improvement_idx = adv_loss &gt; best_loss[~example_found]\n                    best_loss[~example_found &amp; improvement_idx] = adv_loss[improvement_idx].detach()\n                    adversarial_examples[~example_found &amp; improvement_idx] = attack_input[improvement_idx].detach()\n\n                    example_found[~example_found][~torch.argmax(adv_out.detach(), dim=1).eq(target)] = True\n\n                else:\n                    improvement_idx = adv_loss &gt; best_loss\n                    best_loss[improvement_idx] = adv_loss[improvement_idx].detach()\n                    adversarial_examples[improvement_idx] = attack_input[improvement_idx].detach()\n\n                    example_found[~torch.argmax(adv_out.detach(), dim=1).eq(target)] = True\n\n                if early_stopping and example_found.all():\n                    break\n\n    return adversarial_examples.detach()\n</code></pre>"},{"location":"api/bound/crown/","title":"crown","text":""},{"location":"api/bound/crown/#CTRAIN.bound.crown.bound_crown","title":"<code>bound_crown(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False)</code>","text":"<p>Compute the lower and upper bounds of the model's output using the CROWN method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model for which bounds are to be computed.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation object defining the perturbation set.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes for classification. Default is 10.</p> <code>10</code> <code>bound_upper</code> <code>bool</code> <p>Whether to compute the upper bound. Default is False.</p> <code>False</code> <code>reuse_input</code> <code>bool</code> <p>Whether to reuse the input data from previous bounding operation. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>The lower and upper bounds of the model's output.</p> Source code in <code>CTRAIN/bound/crown.py</code> <pre><code>def bound_crown(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False):\n    \"\"\"\n    Compute the lower and upper bounds of the model's output using the CROWN method.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model for which bounds are to be computed.\n        ptb (auto_LiRPA.PerturbationLpNorm): The perturbation object defining the perturbation set.\n        data (Tensor): The input data tensor.\n        target (Tensor): The target labels tensor.\n        n_classes (int, optional): The number of classes for classification. Default is 10.\n        bound_upper (bool, optional): Whether to compute the upper bound. Default is False.\n        reuse_input (bool, optional): Whether to reuse the input data from previous bounding operation. Default is False.\n\n    Returns:\n        (Tuple[Tensor, Tensor]): The lower and upper bounds of the model's output.\n    \"\"\"\n    data = BoundedTensor(data, ptb=ptb)\n    c = construct_c(data, target, n_classes)\n    if reuse_input:\n        bound_input = None\n    else:\n        bound_input = (data,)\n    lb, ub = model.compute_bounds(x=bound_input, IBP=False, method=\"CROWN\", C=c, bound_upper=bound_upper)\n    return lb, ub\n</code></pre>"},{"location":"api/bound/crown/#CTRAIN.bound.crown.bound_crown_ibp","title":"<code>bound_crown_ibp(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False)</code>","text":"<p>Compute the lower and upper bounds of the model's output using the CROWN-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model for which bounds are to be computed.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation object defining the perturbation set.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes for classification. Default is 10.</p> <code>10</code> <code>bound_upper</code> <code>bool</code> <p>Whether to compute the upper bound. Default is False.</p> <code>False</code> <code>reuse_input</code> <code>bool</code> <p>Whether to reuse the input data from previous bounding operation. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>The lower and upper bounds of the model's output.</p> Source code in <code>CTRAIN/bound/crown.py</code> <pre><code>def bound_crown_ibp(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False):\n    \"\"\"\n    Compute the lower and upper bounds of the model's output using the CROWN-IBP method.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model for which bounds are to be computed.\n        ptb (auto_LiRPA.PerturbationLpNorm): The perturbation object defining the perturbation set.\n        data (Tensor): The input data tensor.\n        target (Tensor): The target labels tensor.\n        n_classes (int, optional): The number of classes for classification. Default is 10.\n        bound_upper (bool, optional): Whether to compute the upper bound. Default is False.\n        reuse_input (bool, optional): Whether to reuse the input data from previous bounding operation. Default is False.\n\n    Returns:\n        (Tuple[Tensor, Tensor]): The lower and upper bounds of the model's output.\n    \"\"\"\n    data = BoundedTensor(data, ptb=ptb)\n    c = construct_c(data, target, n_classes)\n    if reuse_input:\n        bound_input = None\n    else:\n        bound_input = (data,)\n    lb, ub = model.compute_bounds(x=bound_input, IBP=False, method=\"CROWN-IBP\", C=c, bound_upper=bound_upper)\n    return lb, ub\n</code></pre>"},{"location":"api/bound/ibp/","title":"ibp","text":""},{"location":"api/bound/ibp/#CTRAIN.bound.ibp.bound_ibp","title":"<code>bound_ibp(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False)</code>","text":"<p>Compute the lower and upper bounds of the model's output using the IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model for which bounds are to be computed.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation object defining the perturbation set.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor. Default is None.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes for classification. Default is 10.</p> <code>10</code> <code>bound_upper</code> <code>bool</code> <p>Whether to compute the upper bound. Default is False.</p> <code>False</code> <code>reuse_input</code> <code>bool</code> <p>Whether to reuse the input data from previous bounding operation. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>The lower and upper bounds of the model's output.</p> Source code in <code>CTRAIN/bound/ibp.py</code> <pre><code>def bound_ibp(model, ptb, data, target, n_classes=10, bound_upper=False, reuse_input=False):\n    \"\"\"\n    Compute the lower and upper bounds of the model's output using the IBP method.\n\n    Args:\n        model (auto_LiRPA.BoundedModule): The neural network model for which bounds are to be computed.\n        ptb (auto_LiRPA.PerturbationLpNorm): The perturbation object defining the perturbation set.\n        data (Tensor): The input data tensor.\n        target (Tensor, optional): The target labels tensor. Default is None.\n        n_classes (int, optional): The number of classes for classification. Default is 10.\n        bound_upper (bool, optional): Whether to compute the upper bound. Default is False.\n        reuse_input (bool, optional): Whether to reuse the input data from previous bounding operation. Default is False.\n\n    Returns:\n        (Tuple[Tensor, Tensor]): The lower and upper bounds of the model's output.\n    \"\"\"\n    data = BoundedTensor(data, ptb=ptb)\n    if target is not None:\n        c = construct_c(data, target, n_classes)\n    else:\n        c = None\n    if reuse_input:\n        bound_input = None\n    else:\n        bound_input = (data,)\n    lb, ub = model.compute_bounds(x=bound_input, IBP=True, method=\"IBP\", C=c, bound_upper=bound_upper)\n    return lb, ub\n</code></pre>"},{"location":"api/bound/sabr/","title":"sabr","text":""},{"location":"api/bound/sabr/#CTRAIN.bound.sabr.bound_sabr","title":"<code>bound_sabr(hardened_model, original_model, data, target, eps, subselection_ratio, device='cuda', n_classes=10, x_L=None, x_U=None, data_min=None, data_max=None, n_steps=8, step_size=0.5, restarts=1, early_stopping=True, intermediate_bound_model=None, decay_factor=0.1, decay_checkpoints=(4, 7), return_adv_output=False)</code>","text":"<p>Compute the lower and upper bounds of the model's output using the SABR method.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The auto_LiRPA model.</p> required <code>original_model</code> <code>Module</code> <p>The original neural network model.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>eps</code> <code>float</code> <p>The epsilon value for perturbation.</p> required <code>subselection_ratio</code> <code>float</code> <p>The ratio for subselection of the epsilon for the IBP bounding during SABR.</p> required <code>device</code> <code>str</code> <p>The device to run the computation on. Default is 'cuda'.</p> <code>'cuda'</code> <code>n_classes</code> <code>int</code> <p>The number of classes for classification. Default is 10.</p> <code>10</code> <code>x_L</code> <code>Tensor</code> <p>The lower bound of the input data. Default is None.</p> <code>None</code> <code>x_U</code> <code>Tensor</code> <p>The upper bound of the input data. Default is None.</p> <code>None</code> <code>data_min</code> <code>Tensor</code> <p>The minimum value of the input data. Default is None.</p> <code>None</code> <code>data_max</code> <code>Tensor</code> <p>The maximum value of the input data. Default is None.</p> <code>None</code> <code>n_steps</code> <code>int</code> <p>The number of steps for the attack. Default is 8.</p> <code>8</code> <code>step_size</code> <code>float</code> <p>The step size for the attack. Default is 0.5.</p> <code>0.5</code> <code>restarts</code> <code>int</code> <p>The number of restarts for the attack. Default is 1.</p> <code>1</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping. Default is True.</p> <code>True</code> <code>intermediate_bound_model</code> <code>Module</code> <p>The intermediate bound model. If provided, the SABR bounds of the intermediate model are returned. This is needed during STAPS bound calculation. Default is None.</p> <code>None</code> <code>decay_factor</code> <code>float</code> <p>The decay factor for the attack. Default is 0.1.</p> <code>0.1</code> <code>decay_checkpoints</code> <code>tuple</code> <p>The decay checkpoints for the attack. Default is (4, 7).</p> <code>(4, 7)</code> <code>return_adv_output</code> <code>bool</code> <p>Whether to return the adversarial output. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>The lower and upper bounds of the model's output, and the adversarial output if return_adv_output is True.</p> Source code in <code>CTRAIN/bound/sabr.py</code> <pre><code>def bound_sabr(hardened_model, original_model, data, target, eps, subselection_ratio, device='cuda', n_classes=10, x_L=None, x_U=None, data_min=None, data_max=None, n_steps=8, step_size=.5, restarts=1, early_stopping=True, intermediate_bound_model=None, decay_factor=0.1, decay_checkpoints=(4,7), return_adv_output=False):\n    \"\"\"\n    Compute the lower and upper bounds of the model's output using the SABR method.\n\n    Parameters:\n        hardened_model (autoLiRPA.BoundedModule): The auto_LiRPA model.\n        original_model (torch.nn.Module): The original neural network model.\n        data (Tensor): The input data tensor.\n        target (Tensor): The target labels tensor.\n        eps (float): The epsilon value for perturbation.\n        subselection_ratio (float): The ratio for subselection of the epsilon for the IBP bounding during SABR.\n        device (str, optional): The device to run the computation on. Default is 'cuda'.\n        n_classes (int, optional): The number of classes for classification. Default is 10.\n        x_L (Tensor, optional): The lower bound of the input data. Default is None.\n        x_U (Tensor, optional): The upper bound of the input data. Default is None.\n        data_min (Tensor, optional): The minimum value of the input data. Default is None.\n        data_max (Tensor, optional): The maximum value of the input data. Default is None.\n        n_steps (int, optional): The number of steps for the attack. Default is 8.\n        step_size (float, optional): The step size for the attack. Default is 0.5.\n        restarts (int, optional): The number of restarts for the attack. Default is 1.\n        early_stopping (bool, optional): Whether to use early stopping. Default is True.\n        intermediate_bound_model (torch.nn.Module, optional): The intermediate bound model. If provided, the SABR bounds of the intermediate model are returned. This is needed during STAPS bound calculation. Default is None.\n        decay_factor (float, optional): The decay factor for the attack. Default is 0.1.\n        decay_checkpoints (tuple, optional): The decay checkpoints for the attack. Default is (4, 7).\n        return_adv_output (bool, optional): Whether to return the adversarial output. Default is False.\n\n    Returns:\n        (Tuple[Tensor, Tensor, Tensor]): The lower and upper bounds of the model's output, and the adversarial output if return_adv_output is True.\n    \"\"\"\n    hardened_model.eval()\n    original_model.eval()\n\n    propagation_inputs, tau, x_adv = get_propagation_region(\n        model=hardened_model,\n        data=data,\n        data_min=data_min,\n        data_max=data_max,\n        target=target,\n        eps=eps if (x_L is None and x_U is None) else None,\n        subselection_ratio=subselection_ratio,\n        n_steps=n_steps,\n        step_size=step_size,\n        restarts=restarts,\n        early_stopping=early_stopping,\n        x_L=x_L,\n        x_U=x_U,\n        decay_checkpoints=decay_checkpoints, \n        decay_factor=decay_factor\n    )\n\n    hardened_model.train()\n    original_model.train()\n\n    ptb = PerturbationLpNorm(\n        eps=tau,\n        norm=np.inf,\n        x_L=torch.clamp(propagation_inputs - tau, data_min, data_max).to(device),\n        x_U=torch.clamp(propagation_inputs + tau, data_min, data_max).to(device)\n    )\n\n    # Pass input through network to set batch statistics\n    adv_output = hardened_model(x_adv)    \n\n    # Use intermediate_bound_model if provided and return intermediate bounds (as needed by STAPS), otherwise use hardened_model\n    lb, ub = bound_ibp(\n        model=hardened_model if intermediate_bound_model is None else intermediate_bound_model,\n        ptb=ptb,\n        data=propagation_inputs,\n        # Only provide target if intermediate_bound_model is not used (as we are not interested in final bound margins)\n        target=target if intermediate_bound_model is None else None,\n        n_classes=n_classes,\n        bound_upper=True,\n        reuse_input=False\n    )\n\n    if return_adv_output:\n        return lb, ub, adv_output\n    return lb, ub\n</code></pre>"},{"location":"api/bound/sabr/#CTRAIN.bound.sabr.get_propagation_region","title":"<code>get_propagation_region(model, data, target, subselection_ratio, step_size, n_steps, restarts, x_L=None, x_U=None, data_min=None, data_max=None, eps=None, early_stopping=True, decay_factor=0.1, decay_checkpoints=(4, 7))</code>","text":"<p>Get the shrinked propagation region for the SABR method. This is done by performing a PGD attack on the model and taking the resulting adversarial examples as the center of a smaller propagation region.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The neural network model.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>subselection_ratio</code> <code>float</code> <p>The ratio for subselection.</p> required <code>step_size</code> <code>float</code> <p>The step size for the attack.</p> required <code>n_steps</code> <code>int</code> <p>The number of steps for the attack.</p> required <code>restarts</code> <code>int</code> <p>The number of restarts for the attack.</p> required <code>x_L</code> <code>Tensor</code> <p>The lower bound of the input data. Default is None.</p> <code>None</code> <code>x_U</code> <code>Tensor</code> <p>The upper bound of the input data. Default is None.</p> <code>None</code> <code>data_min</code> <code>Tensor</code> <p>The minimum value of the input data. Default is None.</p> <code>None</code> <code>data_max</code> <code>Tensor</code> <p>The maximum value of the input data. Default is None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>The epsilon value for perturbation. Default is None.</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping. Default is True.</p> <code>True</code> <code>decay_factor</code> <code>float</code> <p>The decay factor for the attack. Default is 0.1.</p> <code>0.1</code> <code>decay_checkpoints</code> <code>tuple</code> <p>The decay checkpoints for the attack. Default is (4, 7).</p> <code>(4, 7)</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, float, Tensor]</code> <p>The propagation inputs, tau value, and adversarial examples.</p> Source code in <code>CTRAIN/bound/sabr.py</code> <pre><code>def get_propagation_region(model, data, target, subselection_ratio, step_size, n_steps, restarts, x_L=None, x_U=None, data_min=None, data_max=None, eps=None, early_stopping=True, decay_factor=.1, decay_checkpoints=(4, 7)):\n    \"\"\"\n    Get the shrinked propagation region for the SABR method. This is done by performing a PGD attack on the model and taking the resulting adversarial examples as the center of a smaller propagation region.\n\n    Parameters:\n        model (torch.nn.Module): The neural network model.\n        data (Tensor): The input data tensor.\n        target (Tensor): The target labels tensor.\n        subselection_ratio (float): The ratio for subselection.\n        step_size (float): The step size for the attack.\n        n_steps (int): The number of steps for the attack.\n        restarts (int): The number of restarts for the attack.\n        x_L (Tensor, optional): The lower bound of the input data. Default is None.\n        x_U (Tensor, optional): The upper bound of the input data. Default is None.\n        data_min (Tensor, optional): The minimum value of the input data. Default is None.\n        data_max (Tensor, optional): The maximum value of the input data. Default is None.\n        eps (float, optional): The epsilon value for perturbation. Default is None.\n        early_stopping (bool, optional): Whether to use early stopping. Default is True.\n        decay_factor (float, optional): The decay factor for the attack. Default is 0.1.\n        decay_checkpoints (tuple, optional): The decay checkpoints for the attack. Default is (4, 7).\n\n    Returns:\n        (Tuple[Tensor, float, Tensor]): The propagation inputs, tau value, and adversarial examples.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    assert (x_L is None and x_U is None and eps is not None) or (x_L is not None and x_U is not None and eps is None), \"Please only provide epsilon value OR upper and lower input bounds\"\n    tau = None\n    if eps.all() and data is not None:\n        x_L=torch.clamp(data - eps, data_min, data_max).to(device)\n        x_U=torch.clamp(data + eps, data_min, data_max).to(device)\n    else:\n        eps = torch.max((x_U - x_L))\n\n    tau =  subselection_ratio * eps\n\n    x_adv = pgd_attack(\n        model=model,\n        data=data,\n        target=target,\n        x_L=x_L,\n        x_U=x_U,\n        n_steps=n_steps,\n        step_size=step_size,\n        restarts=restarts,\n        early_stopping=early_stopping,\n        device=device,\n        decay_checkpoints=decay_checkpoints,\n        decay_factor=decay_factor\n    )\n\n    propagation_inputs = torch.clamp(x_adv, x_L + tau, x_U - tau) # called midpoints in SABR code\n    tau = torch.tensor(tau, device=device)\n    return propagation_inputs, tau, x_adv\n</code></pre>"},{"location":"api/bound/taps/","title":"taps","text":""},{"location":"api/bound/taps/#CTRAIN.bound.taps.GradExpander","title":"<code>GradExpander</code>","text":"<p>               Bases: <code>Function</code></p> <p>A custom autograd function that scales the gradient during the backward pass. This function allows you to define a custom forward and backward pass for a  PyTorch operation. The forward pass simply returns the input tensor, while  the backward pass scales the gradient by a specified factor <code>alpha</code>. Methods:     forward(ctx, x, alpha: float = 1):     backward(ctx, grad_x):</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>class GradExpander(torch.autograd.Function):\n    \"\"\"\n    A custom autograd function that scales the gradient during the backward pass.\n    This function allows you to define a custom forward and backward pass for a \n    PyTorch operation. The forward pass simply returns the input tensor, while \n    the backward pass scales the gradient by a specified factor `alpha`.\n    Methods:\n        forward(ctx, x, alpha: float = 1):\n        backward(ctx, grad_x):\n\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x, alpha:float=1):\n        \"\"\"\n        Forward pass for the custom operation.\n\n        Args:\n            ctx: The context object that can be used to stash information\n                for backward computation.\n            x: The input tensor.\n            alpha (float, optional): A scaling factor. Defaults to 1.\n\n        Returns:\n            (torch.Tensor): The input tensor `x`.\n        \"\"\"\n        ctx.alpha = alpha\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_x):\n        \"\"\"\n        Performs the backward pass for the custom autograd function.\n\n        Args:\n            ctx: The context object that can be used to stash information for backward computation.\n            grad_x: The gradient of the loss with respect to the output of the forward pass.\n\n        Returns:\n            (Tuple[Tensor, None]): A tuple containing the gradient of the loss with respect to the input of the forward pass and None (as there is no gradient with respect to the second input).\n        \"\"\"\n        return ctx.alpha * grad_x, None\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.GradExpander.backward","title":"<code>backward(ctx, grad_x)</code>  <code>staticmethod</code>","text":"<p>Performs the backward pass for the custom autograd function.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>The context object that can be used to stash information for backward computation.</p> required <code>grad_x</code> <p>The gradient of the loss with respect to the output of the forward pass.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, None]</code> <p>A tuple containing the gradient of the loss with respect to the input of the forward pass and None (as there is no gradient with respect to the second input).</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>@staticmethod\ndef backward(ctx, grad_x):\n    \"\"\"\n    Performs the backward pass for the custom autograd function.\n\n    Args:\n        ctx: The context object that can be used to stash information for backward computation.\n        grad_x: The gradient of the loss with respect to the output of the forward pass.\n\n    Returns:\n        (Tuple[Tensor, None]): A tuple containing the gradient of the loss with respect to the input of the forward pass and None (as there is no gradient with respect to the second input).\n    \"\"\"\n    return ctx.alpha * grad_x, None\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.GradExpander.forward","title":"<code>forward(ctx, x, alpha=1)</code>  <code>staticmethod</code>","text":"<p>Forward pass for the custom operation.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>The context object that can be used to stash information for backward computation.</p> required <code>x</code> <p>The input tensor.</p> required <code>alpha</code> <code>float</code> <p>A scaling factor. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The input tensor <code>x</code>.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>@staticmethod\ndef forward(ctx, x, alpha:float=1):\n    \"\"\"\n    Forward pass for the custom operation.\n\n    Args:\n        ctx: The context object that can be used to stash information\n            for backward computation.\n        x: The input tensor.\n        alpha (float, optional): A scaling factor. Defaults to 1.\n\n    Returns:\n        (torch.Tensor): The input tensor `x`.\n    \"\"\"\n    ctx.alpha = alpha\n    return x\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.RectifiedLinearGradientLink","title":"<code>RectifiedLinearGradientLink</code>","text":"<p>               Bases: <code>Function</code></p> <p>RectifiedLinearGradientLink is a custom autograd function that establishes a rectified linear gradient link  between the IBP bounds of the feature extractor (lb, ub) and the  PGD bounds (x_adv) of the classifier. This function is not a valid gradient with respect  to the forward function.</p> <p>Attributes:</p> Name Type Description <code>c</code> <code>float</code> <p>A constant used to determine the slope.</p> <code>tol</code> <code>float</code> <p>A tolerance value to avoid division by zero.</p> <p>Methods:</p> Name Description <code>forward</code> <p>float, tol: float)</p> <code>backward</code> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>class RectifiedLinearGradientLink(torch.autograd.Function):\n    \"\"\"\n    RectifiedLinearGradientLink is a custom autograd function that establishes a rectified linear gradient link \n    between the IBP bounds of the feature extractor (lb, ub) and the \n    PGD bounds (x_adv) of the classifier. This function is not a valid gradient with respect \n    to the forward function.\n\n    Attributes:\n        c (float): A constant used to determine the slope.\n        tol (float): A tolerance value to avoid division by zero.\n\n    Methods:\n        forward(ctx, lb, ub, x, c: float, tol: float)\n        backward(ctx, grad_x):\n    \"\"\"\n    @staticmethod\n    def forward(ctx, lb, ub, x, c:float, tol:float):\n        \"\"\"\n        Saves the input tensors and constants for backward computation.\n\n        Args:\n            ctx: Context object to save information for backward computation.\n            lb: Lower bound tensor.\n            ub: Upper bound tensor.\n            x: Input tensor.\n            c (float): A constant used to determine the slope.\n            tol (float): A tolerance value to avoid division by zero.\n\n        Returns:\n            (Tensor): The input tensor x.\n        \"\"\"\n        ctx.save_for_backward(lb, ub, x)\n        ctx.c = c\n        ctx.tol = tol\n        return x\n    @staticmethod\n    def backward(ctx, grad_x):\n        \"\"\"\n        Computes the gradient of the loss with respect to the input bounds (lb, ub).\n\n        Args:\n            ctx: Context object containing saved tensors and constants.\n            grad_x: Gradient of the loss with respect to the output of the forward function.\n\n        Returns:\n            (Tuple[Tensor, Tensor, None, None, None]): Gradients with respect to lb, ub, and None for other inputs.\n        \"\"\"\n        lb, ub, x = ctx.saved_tensors\n        c, tol = ctx.c, ctx.tol\n        slackness = c * (ub - lb)\n        # handle grad w.r.t. ub\n        thre = (ub - slackness)\n        Rectifiedgrad_mask = (x &gt;= thre)\n        grad_ub = (Rectifiedgrad_mask * grad_x * (x - thre).clamp(min=0.5*tol) / slackness.clamp(min=tol)).sum(dim=0, keepdim=True)\n        # handle grad w.r.t. lb\n        thre = (lb + slackness)\n        Rectifiedgrad_mask = (x &lt;= thre)\n        grad_lb = (Rectifiedgrad_mask * grad_x * (thre - x).clamp(min=0.5*tol) / slackness.clamp(min=tol)).sum(dim=0, keepdim=True)\n        # we don't need grad w.r.t. x and param\n        return grad_lb, grad_ub, None, None, None\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.RectifiedLinearGradientLink.backward","title":"<code>backward(ctx, grad_x)</code>  <code>staticmethod</code>","text":"<p>Computes the gradient of the loss with respect to the input bounds (lb, ub).</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>Context object containing saved tensors and constants.</p> required <code>grad_x</code> <p>Gradient of the loss with respect to the output of the forward function.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, None, None, None]</code> <p>Gradients with respect to lb, ub, and None for other inputs.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>@staticmethod\ndef backward(ctx, grad_x):\n    \"\"\"\n    Computes the gradient of the loss with respect to the input bounds (lb, ub).\n\n    Args:\n        ctx: Context object containing saved tensors and constants.\n        grad_x: Gradient of the loss with respect to the output of the forward function.\n\n    Returns:\n        (Tuple[Tensor, Tensor, None, None, None]): Gradients with respect to lb, ub, and None for other inputs.\n    \"\"\"\n    lb, ub, x = ctx.saved_tensors\n    c, tol = ctx.c, ctx.tol\n    slackness = c * (ub - lb)\n    # handle grad w.r.t. ub\n    thre = (ub - slackness)\n    Rectifiedgrad_mask = (x &gt;= thre)\n    grad_ub = (Rectifiedgrad_mask * grad_x * (x - thre).clamp(min=0.5*tol) / slackness.clamp(min=tol)).sum(dim=0, keepdim=True)\n    # handle grad w.r.t. lb\n    thre = (lb + slackness)\n    Rectifiedgrad_mask = (x &lt;= thre)\n    grad_lb = (Rectifiedgrad_mask * grad_x * (thre - x).clamp(min=0.5*tol) / slackness.clamp(min=tol)).sum(dim=0, keepdim=True)\n    # we don't need grad w.r.t. x and param\n    return grad_lb, grad_ub, None, None, None\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.RectifiedLinearGradientLink.forward","title":"<code>forward(ctx, lb, ub, x, c, tol)</code>  <code>staticmethod</code>","text":"<p>Saves the input tensors and constants for backward computation.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <p>Context object to save information for backward computation.</p> required <code>lb</code> <p>Lower bound tensor.</p> required <code>ub</code> <p>Upper bound tensor.</p> required <code>x</code> <p>Input tensor.</p> required <code>c</code> <code>float</code> <p>A constant used to determine the slope.</p> required <code>tol</code> <code>float</code> <p>A tolerance value to avoid division by zero.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The input tensor x.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>@staticmethod\ndef forward(ctx, lb, ub, x, c:float, tol:float):\n    \"\"\"\n    Saves the input tensors and constants for backward computation.\n\n    Args:\n        ctx: Context object to save information for backward computation.\n        lb: Lower bound tensor.\n        ub: Upper bound tensor.\n        x: Input tensor.\n        c (float): A constant used to determine the slope.\n        tol (float): A tolerance value to avoid division by zero.\n\n    Returns:\n        (Tensor): The input tensor x.\n    \"\"\"\n    ctx.save_for_backward(lb, ub, x)\n    ctx.c = c\n    ctx.tol = tol\n    return x\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps._get_bound_estimation_from_pts","title":"<code>_get_bound_estimation_from_pts(block, pts, dim_to_estimate, C=None)</code>","text":"<p>Estimate bounds for specified dimensions from given adversarial examples.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>BoundedModule</code> <p>The neural network block for which to estimate pivotal points.</p> required <code>pts</code> <code>Tensor</code> <p>Tensor of adversarial examples of shape (batch_size, num_pivotal, *shape_in[1:]).</p> required <code>dim_to_estimate</code> <code>Tensor</code> <p>Tensor indicating the dimensions to estimate, shape (batch_size, num_dims, dim_size).</p> required <code>C</code> <code>Tensor</code> <p>Matrix specifying the correct class for bound margin calculation. Must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>estimated_bounds</code> <code>Tensor</code> <p>Estimated bounds tensor of shape (batch_size, num_pivotal) if C is None,         otherwise shape (batch_size, n_class).</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>def _get_bound_estimation_from_pts(block, pts, dim_to_estimate, C=None):\n    \"\"\"\n    Estimate bounds for specified dimensions from given adversarial examples.\n\n    Parameters:\n        block (autoLiRPA.BoundedModule): The neural network block for which to estimate pivotal points.\n        pts (torch.Tensor): Tensor of adversarial examples of shape (batch_size, num_pivotal, *shape_in[1:]).\n        dim_to_estimate (torch.Tensor): Tensor indicating the dimensions to estimate, shape (batch_size, num_dims, dim_size).\n        C (torch.Tensor): Matrix specifying the correct class for bound margin calculation. Must be provided.\n\n    Returns:\n        estimated_bounds(torch.Tensor): Estimated bounds tensor of shape (batch_size, num_pivotal) if C is None,\n                    otherwise shape (batch_size, n_class).\n    \"\"\"\n\n    if C is None:\n        # pts shape (batch_size, num_pivotal, *shape_in[1:])\n        out_pts = block(pts.reshape(-1, *pts.shape[2:]))\n        out_pts = out_pts.reshape(*pts.shape[:2], -1)\n        dim_to_estimate = dim_to_estimate.unsqueeze(1)\n        dim_to_estimate = dim_to_estimate.expand(dim_to_estimate.shape[0], out_pts.shape[1], dim_to_estimate.shape[2])\n        out_pts = torch.gather(out_pts, dim=2, index=dim_to_estimate) # shape: (batch_size, num_pivotal, num_pivotal)\n        estimated_bounds = torch.diagonal(out_pts, dim1=1, dim2=2) # shape: (batch_size, num_pivotal)\n    else:\n        # # main idea: convert the 9 adv inputs into one batch to compute the bound at the same time; involve many reshaping\n        batch_C = C.unsqueeze(1).expand(-1, pts.shape[1], -1, -1).reshape(-1, *(C.shape[1:])) # may need shape adjustment\n        batch_pts = pts.reshape(-1, *(pts.shape[2:]))\n        out_pts = block(batch_pts)\n        out_pts = torch.bmm(batch_C, out_pts.unsqueeze(-1)).squeeze(-1)\n        out_pts = out_pts.reshape(*(pts.shape[:2]), *(out_pts.shape[1:]))\n        out_pts = - out_pts # the out is the lower bound of yt - yi, transform it to the upper bound of yi - yt\n        # the out_pts should be in shape (batch_size, n_class - 1, n_class - 1)\n        ub = torch.diagonal(out_pts, dim1=1, dim2=2) # shape: (batch_size, n_class - 1)\n        estimated_bounds = torch.cat([torch.zeros(size=(ub.shape[0],1), dtype=ub.dtype, device=ub.device), ub], dim=1) # shape: (batch_size, n_class)\n\n    return estimated_bounds\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps._get_pivotal_points","title":"<code>_get_pivotal_points(block, input_lb, input_ub, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, n_classes, C=None)</code>","text":"<p>Estimate pivotal points for the classifier block using Projected Gradient Descent (PGD).</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>BoundedModule</code> <p>The neural network block for which to estimate pivotal points.</p> required <code>input_lb</code> <code>Tensor</code> <p>Lower bound of the input to the network block.</p> required <code>input_ub</code> <code>Tensor</code> <p>Upper bound of the input to the network block.</p> required <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps to perform.</p> required <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts to perform.</p> required <code>pgd_step_size</code> <code>float</code> <p>Step size for PGD.</p> required <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD step size.</p> required <code>pgd_decay_checkpoints</code> <code>list of int</code> <p>Checkpoints at which to decay the PGD step size.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> required <code>C</code> <code>Tensor</code> <p>Matrix specifying the correct class for bound margin calculation. Must be provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>list of torch.Tensor</code> <p>List containing the concatenated pivotal points tensor.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>def _get_pivotal_points(block, input_lb, input_ub, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, n_classes, C=None):\n    \"\"\"\n    Estimate pivotal points for the classifier block using Projected Gradient Descent (PGD).\n\n    Parameters:\n        block (autoLiRPA.BoundedModule): The neural network block for which to estimate pivotal points.\n        input_lb (torch.Tensor): Lower bound of the input to the network block.\n        input_ub (torch.Tensor): Upper bound of the input to the network block.\n        pgd_steps (int): Number of PGD steps to perform.\n        pgd_restarts (int): Number of PGD restarts to perform.\n        pgd_step_size (float): Step size for PGD.\n        pgd_decay_factor (float): Decay factor for PGD step size.\n        pgd_decay_checkpoints (list of int): Checkpoints at which to decay the PGD step size.\n        n_classes (int): Number of classes in the classification task.\n        C (torch.Tensor, optional): Matrix specifying the correct class for bound margin calculation. Must be provided.\n\n    Returns:\n        (list of torch.Tensor): List containing the concatenated pivotal points tensor.\n    \"\"\"\n    assert C is not None # Should only estimate for the final block\n    lb, ub = input_lb.clone().detach(), input_ub.clone().detach()\n\n    pt_list = []\n    # split into batches\n    # TODO: Can we keep this fixed batch size?\n    bs = 128\n    lb_batches = [lb[i*bs:(i+1)*bs] for i in range(math.ceil(len(lb) / bs))]\n    ub_batches = [ub[i*bs:(i+1)*bs] for i in range(math.ceil(len(ub) / bs))]\n    C_batches = [C[i*bs:(i+1)*bs] for i in range(math.ceil(len(C) / bs))]\n    for lb_one_batch, ub_one_batch, C_one_batch in zip(lb_batches, ub_batches, C_batches):\n        pt_list.append(_get_pivotal_points_one_batch(block, lb_one_batch, ub_one_batch, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, n_classes=n_classes, C=C_one_batch))\n    pts = torch.cat(pt_list, dim=0)\n    return [pts, ]\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps._get_pivotal_points_one_batch","title":"<code>_get_pivotal_points_one_batch(block, lb, ub, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, C, n_classes, device='cuda')</code>","text":"<p>Estimate pivotal points for a batch using Projected Gradient Descent (PGD).</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>BoundedModule</code> <p>The neural network block for which to estimate pivotal points.</p> required <code>lb</code> <code>Tensor</code> <p>Lower bound of the input.</p> required <code>ub</code> <code>Tensor</code> <p>Upper bound of the input.</p> required <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps.</p> required <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts.</p> required <code>pgd_step_size</code> <code>float</code> <p>Step size for PGD.</p> required <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for learning rate.</p> required <code>pgd_decay_checkpoints</code> <code>list</code> <p>Checkpoints for learning rate decay.</p> required <code>C</code> <code>Tensor</code> <p>Matrix specifying the correct class for bound margin calculation. Must be provided.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>device</code> <code>str</code> <p>Device to perform computations on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Adversarial examples per class for whole batch.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>def _get_pivotal_points_one_batch(block, lb, ub, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, C, n_classes, device='cuda'):\n    \"\"\"\n    Estimate pivotal points for a batch using Projected Gradient Descent (PGD).\n\n    Args:\n        block (autoLiRPA.BoundedModule): The neural network block for which to estimate pivotal points.\n        lb (torch.Tensor): Lower bound of the input.\n        ub (torch.Tensor): Upper bound of the input.\n        pgd_steps (int): Number of PGD steps.\n        pgd_restarts (int): Number of PGD restarts.\n        pgd_step_size (float): Step size for PGD.\n        pgd_decay_factor (float): Decay factor for learning rate.\n        pgd_decay_checkpoints (list): Checkpoints for learning rate decay.\n        C (torch.Tensor): Matrix specifying the correct class for bound margin calculation. Must be provided.\n        n_classes (int): Number of classes.\n        device (str, optional): Device to perform computations on. Default is 'cuda'.\n\n    Returns:\n        (torch.Tensor): Adversarial examples per class for whole batch.\n    \"\"\"\n\n    num_pivotal = n_classes - 1 # only need to estimate n_class - 1 dim for the final output\n\n    def init_pts(input_lb, input_ub):\n        rand_init = input_lb.unsqueeze(1) + (input_ub-input_lb).unsqueeze(1)*torch.rand(input_lb.shape[0], num_pivotal, *input_lb.shape[1:], device=device)\n        return rand_init\n\n    def select_schedule(num_steps):\n        if num_steps &gt;= 20 and num_steps &lt;= 50:\n            lr_decay_milestones = [int(num_steps*0.7)]\n        elif num_steps &gt; 50 and num_steps &lt;= 80:\n            lr_decay_milestones = [int(num_steps*0.4), int(num_steps*0.7)]\n        elif num_steps &gt; 80:\n            lr_decay_milestones = [int(num_steps*0.3), int(num_steps*0.6), int(num_steps*0.8)]\n        else:\n            lr_decay_milestones = []\n        return lr_decay_milestones\n\n    lr_decay_milestones = pgd_decay_checkpoints\n    lr_decay_factor = pgd_decay_factor\n    init_lr = pgd_step_size\n\n    retain_graph = False\n    pts = init_pts(lb, ub)\n    variety = (ub - lb).unsqueeze(1).detach()\n    best_estimation = -1e5*torch.ones(pts.shape[:2], device=pts.device)\n    best_pts = torch.zeros_like(pts)\n    with torch.enable_grad():\n        for re in range(pgd_restarts):\n            lr = init_lr\n            pts = init_pts(lb, ub)\n            for it in range(pgd_steps+1):\n                pts.requires_grad = True\n                estimated_pseudo_bound = _get_bound_estimation_from_pts(block, pts, None, C=C)\n                improve_idx = estimated_pseudo_bound[:, 1:] &gt; best_estimation\n                best_estimation[improve_idx] = estimated_pseudo_bound[:, 1:][improve_idx].detach()\n                best_pts[improve_idx] = pts[improve_idx].detach()\n                # wants to maximize the estimated bound\n                if it != pgd_steps:\n                    loss = - estimated_pseudo_bound.sum()\n                    loss.backward(retain_graph=retain_graph)\n                    new_pts = pts - pts.grad.sign() * lr * variety\n                    pts = torch.max(torch.min(new_pts, ub.unsqueeze(1)), lb.unsqueeze(1)).detach()\n                    if (it+1) in lr_decay_milestones:\n                        lr *= lr_decay_factor\n    return best_pts.detach()\n</code></pre>"},{"location":"api/bound/taps/#CTRAIN.bound.taps.bound_taps","title":"<code>bound_taps(original_model, hardened_model, bounded_blocks, data, target, n_classes, ptb, device='cuda', pgd_steps=20, pgd_restarts=1, pgd_step_size=0.2, pgd_decay_factor=0.2, pgd_decay_checkpoints=(5, 7), gradient_link_thresh=0.5, gradient_link_tolerance=1e-05, propagation='IBP', sabr_args=None)</code>","text":"<p>Compute the bounds of the model's output using the TAPS method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original neural network model.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The auto_LiRPA model.</p> required <code>bounded_blocks</code> <code>list</code> <p>List of bounded blocks of the model.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes for classification.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation object defining the perturbation set.</p> required <code>device</code> <code>str</code> <p>The device to run the computation on. Default is 'cuda'.</p> <code>'cuda'</code> <code>pgd_steps</code> <code>int</code> <p>The number of steps for the PGD attack. Default is 20.</p> <code>20</code> <code>pgd_restarts</code> <code>int</code> <p>The number of restarts for the PGD attack. Default is 1.</p> <code>1</code> <code>pgd_step_size</code> <code>float</code> <p>The step size for the PGD attack. Default is 0.2.</p> <code>0.2</code> <code>pgd_decay_factor</code> <code>float</code> <p>The decay factor for the PGD attack. Default is 0.2.</p> <code>0.2</code> <code>pgd_decay_checkpoints</code> <code>tuple</code> <p>The decay checkpoints for the PGD attack. Default is (5, 7).</p> <code>(5, 7)</code> <code>gradient_link_thresh</code> <code>float</code> <p>The threshold for gradient linking. Default is 0.5.</p> <code>0.5</code> <code>gradient_link_tolerance</code> <code>float</code> <p>The tolerance for gradient linking. Default is 1e-05.</p> <code>1e-05</code> <code>propagation</code> <code>str</code> <p>The propagation method to use ('IBP' or 'SABR'). Default is 'IBP'.</p> <code>'IBP'</code> <code>sabr_args</code> <code>dict</code> <p>The arguments for the SABR method. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>taps_bound</code> <code>Tuple[Tensor, Tensor]</code> <p>The TAPS bounds of the model's output.</p> Source code in <code>CTRAIN/bound/taps.py</code> <pre><code>def bound_taps(original_model, hardened_model, bounded_blocks, data, target, n_classes, ptb, device='cuda', pgd_steps=20, pgd_restarts=1, pgd_step_size=.2, \n               pgd_decay_factor=.2, pgd_decay_checkpoints=(5,7),\n               gradient_link_thresh=.5, gradient_link_tolerance=1e-05, propagation=\"IBP\", sabr_args=None):\n    \"\"\"\n    Compute the bounds of the model's output using the TAPS method.\n\n    Parameters:\n        original_model (torch.nn.Module): The original neural network model.\n        hardened_model (autoLiRPA.BoundedModule): The auto_LiRPA model.\n        bounded_blocks (list): List of bounded blocks of the model.\n        data (Tensor): The input data tensor.\n        target (Tensor): The target labels tensor.\n        n_classes (int): The number of classes for classification.\n        ptb (auto_LiRPA.PerturbationLpNorm): The perturbation object defining the perturbation set.\n        device (str, optional): The device to run the computation on. Default is 'cuda'.\n        pgd_steps (int, optional): The number of steps for the PGD attack. Default is 20.\n        pgd_restarts (int, optional): The number of restarts for the PGD attack. Default is 1.\n        pgd_step_size (float, optional): The step size for the PGD attack. Default is 0.2.\n        pgd_decay_factor (float, optional): The decay factor for the PGD attack. Default is 0.2.\n        pgd_decay_checkpoints (tuple, optional): The decay checkpoints for the PGD attack. Default is (5, 7).\n        gradient_link_thresh (float, optional): The threshold for gradient linking. Default is 0.5.\n        gradient_link_tolerance (float, optional): The tolerance for gradient linking. Default is 1e-05.\n        propagation (str, optional): The propagation method to use ('IBP' or 'SABR'). Default is 'IBP'.\n        sabr_args (dict, optional): The arguments for the SABR method. Default is None.\n\n    Returns:\n        taps_bound(Tuple[Tensor, Tensor]): The TAPS bounds of the model's output.\n    \"\"\"\n    assert len(bounded_blocks) == 2, \"Split not supported!\"\n\n    if propagation == 'IBP':\n        lb, ub = bound_ibp(\n            model=bounded_blocks[0],\n            ptb=ptb,\n            data=data,\n            target=None,\n            n_classes=n_classes,\n        )\n    if propagation == 'SABR':\n        assert sabr_args is not None, \"Need to Provide SABR arguments if you choose SABR for propagation\"\n        lb, ub = bound_sabr(\n            # Intermediate Bound model instructs to return bounds after the first network block\n            **{**sabr_args, \"intermediate_bound_model\": bounded_blocks[0], \"return_adv_output\": False},\n        )\n\n    with torch.no_grad():\n        hardened_model.eval()\n        original_model.eval()\n        for block in bounded_blocks:\n            block.eval()\n        c = construct_c(data, target, n_classes)\n        with torch.no_grad():\n            grad_cleaner = torch.optim.SGD(hardened_model.parameters())\n            adv_samples = _get_pivotal_points(bounded_blocks[1], lb, ub, pgd_steps, pgd_restarts, pgd_step_size, pgd_decay_factor, pgd_decay_checkpoints, n_classes, C=c)\n            grad_cleaner.zero_grad()\n\n        hardened_model.train()\n        original_model.train()\n        for block in bounded_blocks:\n            block.train()\n\n    pts = adv_samples[0].detach()\n    pts = torch.transpose(pts, 0, 1)\n    pts = RectifiedLinearGradientLink.apply(lb.unsqueeze(0), ub.unsqueeze(0), pts, gradient_link_thresh, gradient_link_tolerance)\n    pts = torch.transpose(pts, 0, 1)\n    pgd_bounds = _get_bound_estimation_from_pts(bounded_blocks[1], pts, None, c)\n    # NOTE: VERY IMPORTANT CHANGES TO TAPS BOUND TO BE COMPATIBLE WITH CTRAIN WORKFLOW\n    pgd_bounds = pgd_bounds[:, 1:]\n    pgd_bounds = -pgd_bounds\n\n\n    ibp_lb, ibp_ub = bound_ibp(\n        model=bounded_blocks[1],\n        ptb=PerturbationLpNorm(x_L=lb, x_U=ub),\n        data=data,\n        target=target,\n        n_classes=n_classes,\n    )\n\n    return pgd_bounds, ibp_lb\n</code></pre>"},{"location":"api/complete_verification/abCROWN/","title":"abCROWN","text":""},{"location":"api/complete_verification/abCROWN/#CTRAIN.complete_verification.abCROWN.verify.abcrown_eval","title":"<code>abcrown_eval(config, seed, instance, vnnlib_path='../../vnnlib/', model_name='mnist_6_100', model_path='./abCROWN/complete_verifier/models/eran/mnist_6_100_nat.pth', model_onnx_path=None, input_shape=[-1, 1, 28, 28], timeout=600, no_cores=28, par_factor=10)</code>","text":"<p>Runs the abCROWN verification process with the given configuration.  abCROWN is invoked from inside the program code, so a crash/freeze can only be handled partially.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary for the verification process.</p> required <code>seed</code> <code>int</code> <p>Seed for random number generation.</p> required <code>instance</code> <code>str</code> <p>Path to the VNN-LIB instance file.</p> required <code>vnnlib_path</code> <code>str</code> <p>Path prefix for VNN-LIB files. Defaults to '../../vnnlib/'.</p> <code>'../../vnnlib/'</code> <code>model_name</code> <code>str</code> <p>Name of the model to be verified. Defaults to 'mnist_6_100'.</p> <code>'mnist_6_100'</code> <code>model_path</code> <code>str</code> <p>Path to the model file. Defaults to './abCROWN/complete_verifier/models/eran/mnist_6_100_nat.pth'.</p> <code>'./abCROWN/complete_verifier/models/eran/mnist_6_100_nat.pth'</code> <code>model_onnx_path</code> <code>str</code> <p>Path to the ONNX model file. Defaults to None.</p> <code>None</code> <code>input_shape</code> <code>list</code> <p>Shape of the input tensor. Defaults to [-1, 1, 28, 28].</p> <code>[-1, 1, 28, 28]</code> <code>timeout</code> <code>int</code> <p>Timeout for the verification process in seconds. Defaults to 600.</p> <code>600</code> <code>no_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel solvers, when abCROWN is configured to use MIP Solvers. Defaults to 28.</p> <code>28</code> <code>par_factor</code> <code>int</code> <p>Penalty factor for running time in case of timeout. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Running time of the verification process and the result of the verification (sat/unsat or timeout/unknown).</p> Source code in <code>CTRAIN/complete_verification/abCROWN/verify.py</code> <pre><code>def abcrown_eval(config, seed, instance, vnnlib_path='../../vnnlib/', model_name='mnist_6_100', model_path='./abCROWN/complete_verifier/models/eran/mnist_6_100_nat.pth', model_onnx_path=None, input_shape=[-1, 1, 28, 28], timeout=600, no_cores=28, par_factor=10):\n    \"\"\"\n    Runs the abCROWN verification process with the given configuration. \n    abCROWN is invoked from inside the program code, so a crash/freeze can only be handled partially.\n\n    Args:\n        config (dict): Configuration dictionary for the verification process.\n        seed (int): Seed for random number generation.\n        instance (str): Path to the VNN-LIB instance file.\n        vnnlib_path (str, optional): Path prefix for VNN-LIB files. Defaults to '../../vnnlib/'.\n        model_name (str, optional): Name of the model to be verified. Defaults to 'mnist_6_100'.\n        model_path (str, optional): Path to the model file. Defaults to './abCROWN/complete_verifier/models/eran/mnist_6_100_nat.pth'.\n        model_onnx_path (str, optional): Path to the ONNX model file. Defaults to None.\n        input_shape (list, optional): Shape of the input tensor. Defaults to [-1, 1, 28, 28].\n        timeout (int, optional): Timeout for the verification process in seconds. Defaults to 600.\n        no_cores (int, optional): Number of CPU cores to use for parallel solvers, when abCROWN is configured to use MIP Solvers. Defaults to 28.\n        par_factor (int, optional): Penalty factor for running time in case of timeout. Defaults to 10.\n\n    Returns:\n        (tuple): Running time of the verification process and the result of the verification (sat/unsat or timeout/unknown).\n    \"\"\"\n    print(config, seed, instance)\n    std_conf = config\n\n    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n\n    timestamp = time.time()\n\n    std_conf['model']['name'] = model_name\n    std_conf['model']['path'] = f'/tmp/{model_name}.pth' if model_name is not None else None\n    std_conf['model']['onnx_path'] = model_onnx_path if model_onnx_path is not None else None\n    std_conf['model']['input_shape'] = input_shape\n\n    std_conf['general']['device'] = device\n\n    std_conf['bab']['timeout'] = timeout\n\n    if not std_conf['solver'].get('mip'):\n        std_conf['solver']['mip'] = get_abcrown_standard_conf(timeout=timeout, no_cores=no_cores)['solver']['mip']\n    std_conf['solver']['mip']['parallel_solvers'] = no_cores\n\n    std_conf['specification']['vnnlib_path_prefix'] = vnnlib_path\n    std_conf['specification']['vnnlib_path'] = instance\n    std_conf['general']['output_file'] = f'/tmp/out_{timestamp}.pkl'\n\n    print(json.dumps(config, indent=2))\n\n    with open(f\"/tmp/conf_{timestamp}.yaml\", \"w\", encoding='u8') as f:\n        yaml.dump(std_conf, f)\n\n    abcrown_instance = ABCROWN(\n        ['--config', f'/tmp/conf_{timestamp}.yaml']\n    )\n\n    # Precompile VNN-LIB s.t. each run can access the cache\n    _ = read_vnnlib(instance)\n\n    start_time = time.time()\n    try:\n        verification_res = abcrown_instance.main()\n    except Exception as e:\n        print(type(e), e)\n        print(traceback.format_exc())\n        return MAX_LOSS, 'unknown'\n    end_time = time.time()\n\n    os.system(f'rm /tmp/conf_{timestamp}.yaml')\n\n    with open(f'/tmp/out_{timestamp}.pkl', 'rb') as f:\n        result_dict = pickle.load(f)\n\n    result = result_dict['results']\n\n    if result == 'unknown':\n        print(\"PENALISING RUNNING TIME DUE TO TIMEOUT!\")\n        running_time = timeout * par_factor if timeout &gt; (end_time - start_time) else (end_time - start_time) * par_factor\n    else:\n        running_time = end_time - start_time\n\n    return running_time, result\n</code></pre>"},{"location":"api/complete_verification/abCROWN/#CTRAIN.complete_verification.abCROWN.verify.limited_abcrown_eval","title":"<code>limited_abcrown_eval(work_dir, runner_path='CTRAIN/complete_verification/abCROWN/runner.py', *args, **kwargs)</code>","text":"<p>Executes the abCROWN verification using a specified verification process with a specified timeout and handles the results. This increases robustness, since a crash of abCROWN does not result in a crash of the  python script. This function serializes the provided arguments and keyword arguments, runs a  separate Python script to perform the verification, and handles the process  execution including timeout management. The results are deserialized and returned  if the process completes successfully within the timeout period.</p> <p>Parameters:</p> Name Type Description Default <code>work_dir</code> <code>str</code> <p>The working directory where temporary files will be stored.</p> required <code>runner_path</code> <code>str</code> <p>The path to the runner script that performs the  verification. Defaults to 'src/complete_verification/abCROWN/runner.py'.</p> <code>'CTRAIN/complete_verification/abCROWN/runner.py'</code> <code>*args</code> <p>Additional positional arguments to be passed to the runner script.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the runner script.  Must include 'timeout' (float) which specifies the timeout period in seconds.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the running time and the result (sat/unsat or timeout/unknown) if the verification          completes successfully. If the verification fails or times out, returns          (MAX_LOSS, 'unknown').</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error running the process.</p> Source code in <code>CTRAIN/complete_verification/abCROWN/verify.py</code> <pre><code>def limited_abcrown_eval(work_dir, runner_path='CTRAIN/complete_verification/abCROWN/runner.py', *args, **kwargs):\n    \"\"\"\n    Executes the abCROWN verification using\n    a specified verification process with a specified timeout and handles the results.\n    This increases robustness, since a crash of abCROWN does not result in a crash of the \n    python script.\n    This function serializes the provided arguments and keyword arguments, runs a \n    separate Python script to perform the verification, and handles the process \n    execution including timeout management. The results are deserialized and returned \n    if the process completes successfully within the timeout period.\n\n    Args:\n        work_dir (str): The working directory where temporary files will be stored.\n        runner_path (str, optional): The path to the runner script that performs the \n            verification. Defaults to 'src/complete_verification/abCROWN/runner.py'.\n        *args: Additional positional arguments to be passed to the runner script.\n        **kwargs: Additional keyword arguments to be passed to the runner script. \n            Must include 'timeout' (float) which specifies the timeout period in seconds.\n\n    Returns:\n        (tuple): A tuple containing the running time and the result (sat/unsat or timeout/unknown) if the verification \n                    completes successfully. If the verification fails or times out, returns \n                    (MAX_LOSS, 'unknown').\n\n    Raises:\n        (Exception): If there is an error running the process.\n    \"\"\"\n    outer_timeout = kwargs['timeout'] * 1.2\n\n    timestamp = time.time()\n\n    args_pkl_path = f'{work_dir}/args_{timestamp}.pkl'\n    result_path = f\"{work_dir}/result_{timestamp}.pkl\"\n\n    with open(f'{work_dir}/args_{timestamp}.pkl', \"wb\") as f:\n        pickle.dump((args, kwargs), f)\n\n    verification_ok = False\n\n    runner_args = [args_pkl_path, result_path]\n\n    try:\n        print(f\"Running {['python3', runner_path] + runner_args}\")\n        process = subprocess.Popen(\n            [\"python3\", runner_path] + runner_args,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        try:\n            stdout, stderr = process.communicate(timeout=outer_timeout)\n            print(\"Function finished successfully.\")\n            print(\"Output:\", stdout.decode())\n            print(\"Error Output:\", stderr.decode())\n            verification_ok = True\n\n        except subprocess.TimeoutExpired:\n            print(f\"Function exceeded timeout of {outer_timeout} seconds. Terminating...\")\n            process.terminate()\n            try:\n                process.wait(timeout=5)\n            except subprocess.TimeoutExpired:\n                print(\"Function did not terminate after SIGTERM. Killing...\")\n                process.kill()\n\n    except Exception as e:\n        print(f\"Error running the process: {e}\")\n\n    if verification_ok:\n        with open(result_path, 'rb') as f:\n            running_time, result = pickle.load(f)\n\n        return running_time, result\n\n    return MAX_LOSS, 'unknown'\n</code></pre>"},{"location":"api/complete_verification/abCROWN/#CTRAIN.complete_verification.abCROWN.util.get_abcrown_standard_conf","title":"<code>get_abcrown_standard_conf(timeout=600, no_cores=28)</code>","text":"<p>Generates the standard configuration for abCROWN. This function reads the standard configuration from a predefined string, updates the device setting based on the available hardware, and sets the  timeout and number of parallel solvers.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>The timeout value for the BAB solver in seconds. Defaults to 600.</p> <code>600</code> <code>no_cores</code> <code>int</code> <p>The number of parallel solvers to use. Defaults to 28.</p> <code>28</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated standard configuration dictionary.</p> Source code in <code>CTRAIN/complete_verification/abCROWN/util.py</code> <pre><code>def get_abcrown_standard_conf(timeout=600, no_cores=28):\n    \"\"\"\n    Generates the standard configuration for abCROWN.\n    This function reads the standard configuration from a predefined string,\n    updates the device setting based on the available hardware, and sets the \n    timeout and number of parallel solvers.\n\n    Args:\n      timeout (int, optional): The timeout value for the BAB solver in seconds. Defaults to 600.\n      no_cores (int, optional): The number of parallel solvers to use. Defaults to 28.\n\n    Returns:\n      (dict): The updated standard configuration dictionary.\n    \"\"\"\n    f = StringIO(STD_CONF)\n\n    std_conf = yaml.load(f, Loader=Loader)\n\n    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n\n    std_conf['general']['device'] = device\n    std_conf['bab']['timeout'] = timeout\n    std_conf['solver']['mip']['parallel_solvers'] = no_cores\n\n    return std_conf\n</code></pre>"},{"location":"api/complete_verification/abCROWN/#CTRAIN.complete_verification.abCROWN.util.instances_to_vnnlib","title":"<code>instances_to_vnnlib(indices, data, vnnlib_path, experiment_name, eps, eps_temp, data_min, data_max, no_classes)</code>","text":"<p>Converts a set of inputs and an epsilon value into VNN-LIB format files for adversarial robustness verification.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list of int</code> <p>List of indices of the data instances to be converted.</p> required <code>data</code> <code>Dataset</code> <p>Dataset containing the data instances.</p> required <code>vnnlib_path</code> <code>str</code> <p>Path where the VNN-LIB files will be saved.</p> required <code>experiment_name</code> <code>str</code> <p>Name of the experiment for documentation purposes.</p> required <code>eps</code> <code>float</code> <p>Epsilon value for the adversarial robustness property.</p> required <code>eps_temp</code> <code>float</code> <p>Temporary epsilon value used for domain calculation.</p> required <code>data_min</code> <code>float</code> <p>Minimum value for data normalization/clamping.</p> required <code>data_max</code> <code>float</code> <p>Maximum value for data normalization/clamping.</p> required <code>no_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of file paths to the generated VNN-LIB files.</p> Source code in <code>CTRAIN/complete_verification/abCROWN/util.py</code> <pre><code>def instances_to_vnnlib(indices, data, vnnlib_path, experiment_name, eps, eps_temp, data_min, data_max, no_classes):\n    \"\"\"\n    Converts a set of inputs and an epsilon value into VNN-LIB format files for adversarial robustness verification.\n\n    Args:\n      indices (list of int): List of indices of the data instances to be converted.\n      data (torch.utils.data.Dataset): Dataset containing the data instances.\n      vnnlib_path (str): Path where the VNN-LIB files will be saved.\n      experiment_name (str): Name of the experiment for documentation purposes.\n      eps (float): Epsilon value for the adversarial robustness property.\n      eps_temp (float): Temporary epsilon value used for domain calculation.\n      data_min (float): Minimum value for data normalization/clamping.\n      data_max (float): Maximum value for data normalization/clamping.\n      no_classes (int): Number of classes in the classification task.\n\n    Returns:\n      (list of str): List of file paths to the generated VNN-LIB files.\n    \"\"\"\n    vnnlib_list_test = []\n    for idx in indices:\n        x, ground_truth = data[idx]\n        ground_truth = ground_truth.numpy()\n        print(f'idx_{idx}_eps_{eps}')\n        # Record the results: vnn-lib specification.\n        filename = vnnlib_path + f\"idx_{idx}-eps{eps}.vnnlib\"\n        comment = f\"Adversarial robustness property for {experiment_name}. l_inf radius: {eps}, \" \\\n                f\"Test Image {idx}.\"\n        comment = ' '.join(comment.splitlines()).strip()\n        # ground_truth = np.argmax(ground_truth)\n        domain = torch.stack(\n            [(x - eps_temp).clamp(data_min, data_max),\n            (x + eps_temp).clamp(data_min, data_max)], dim=-1)\n        write_adversarial_robustness_vnnlib(filename, comment, domain, ground_truth, n_classes=no_classes)\n        vnnlib_list_test.append(filename)\n\n    return vnnlib_list_test\n</code></pre>"},{"location":"api/complete_verification/abCROWN/#CTRAIN.complete_verification.abCROWN.util.write_adversarial_robustness_vnnlib","title":"<code>write_adversarial_robustness_vnnlib(filename, initial_comment, input_domain, ground_truth, n_classes=10)</code>","text":"<p>Create a vnnlib specification for an adversarial robustness property.</p> The function writes the following to the specified file <ul> <li>A comment at the top of the file.</li> <li>Declarations for input and output variables.</li> <li>Input constraints based on the provided input domain.</li> <li>Output constraints encoding the conditions for a property counter-example.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to write the vnnlib specification to.</p> required <code>initial_comment</code> <code>str</code> <p>A comment to include at the top of the vnnlib file.</p> required <code>input_domain</code> <code>Tensor</code> <p>A tensor representing the input domain, i.e. lower und upper bounds per input, with shape (n, 2), where n is the number of input variables.</p> required <code>ground_truth</code> <code>int</code> <p>The index of the ground truth class.</p> required <code>n_classes</code> <code>int</code> <p>The number of output classes. Default is 10.</p> <code>10</code> Source code in <code>CTRAIN/complete_verification/abCROWN/util.py</code> <pre><code>def write_adversarial_robustness_vnnlib(filename, initial_comment, input_domain, ground_truth, n_classes=10):\n    \"\"\"\n    Create a vnnlib specification for an adversarial robustness property.\n\n    The function writes the following to the specified file:\n      - A comment at the top of the file.\n      - Declarations for input and output variables.\n      - Input constraints based on the provided input domain.\n      - Output constraints encoding the conditions for a property counter-example.\n\n    Parameters:\n      filename (str): The name of the file to write the vnnlib specification to.\n      initial_comment (str): A comment to include at the top of the vnnlib file.\n      input_domain (torch.Tensor): A tensor representing the input domain, i.e. lower und upper bounds per input, with shape (n, 2), where n is the number of input variables.\n      ground_truth (int): The index of the ground truth class.\n      n_classes (int, optional): The number of output classes. Default is 10.\n    \"\"\"\n\n    with open(filename, \"w\") as f:\n        f.write(f\"; {initial_comment}\\n\")\n\n        # Declare input variables.\n        f.write(\"\\n\")\n        linearized_domain = input_domain.view(-1, 2)\n        for i in range(linearized_domain.shape[0]):\n            f.write(f\"(declare-const X_{i} Real)\\n\")\n        f.write(\"\\n\")\n\n        # Declare output variables.\n        for i in range(n_classes):\n            f.write(f\"(declare-const Y_{i} Real)\\n\")\n        f.write(\"\\n\")\n\n        # Define input constraints.\n        f.write(f\"; Input constraints:\\n\")\n        for i in range(linearized_domain.shape[0]):\n            f.write(f\"(assert (&lt;= X_{i} {linearized_domain[i, 1]}))\\n\")  # UB\n            f.write(f\"(assert (&gt;= X_{i} {linearized_domain[i, 0]}))\\n\")  # LB\n            f.write(\"\\n\")\n        f.write(\"\\n\")\n\n        # Define output constraints, providing an unnecessary \"and\" to ease parsing in vnn-comp-21.\n        f.write(f\"; Output constraints (encoding the conditions for a property counter-example):\\n\")\n        f.write(f\"(assert (or\\n\")\n        for i in range(n_classes):\n            if i != ground_truth:\n                f.write(f\"\\t(and (&lt;= Y_{ground_truth} Y_{i}))\\n\")\n        f.write(f\"))\\n\")\n        f.write(\"\\n\")\n</code></pre>"},{"location":"api/data_loaders/data_loaders/","title":"data_loaders","text":""},{"location":"api/data_loaders/data_loaders/#CTRAIN.data_loaders.data_loaders.load_cifar10","title":"<code>load_cifar10(batch_size=64, normalise=True, train_transforms=[transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 2, padding_mode='edge')], val_split=True, data_root='./data')</code>","text":"<p>Load the CIFAR-10 dataset with optional normalization and data augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples per batch to load. Default is 64.</p> <code>64</code> <code>normalise</code> <code>bool</code> <p>Whether to normalize the dataset. Default is True.</p> <code>True</code> <code>train_transforms</code> <code>list</code> <p>List of transformations to apply to the training data. Default is [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 2, padding_mode='edge')].</p> <code>[RandomHorizontalFlip(), RandomCrop(32, 2, padding_mode='edge')]</code> <code>val_split</code> <code>bool</code> <p>Whether to split the training data into training and validation sets. Default is True.</p> <code>True</code> <code>data_root</code> <code>str</code> <p>Root directory where the dataset will be stored. Default is './data'.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).</p> The DataLoader objects have the following attributes <ul> <li>mean (torch.Tensor): The mean used for normalization.</li> <li>std (torch.Tensor): The standard deviation used for normalization.</li> <li>min (torch.Tensor): The minimum value in the dataset.</li> <li>max (torch.Tensor): The maximum value in the dataset.</li> <li>normalised (bool): Whether the dataset was normalized.</li> </ul> Source code in <code>CTRAIN/data_loaders/data_loaders.py</code> <pre><code>def load_cifar10(batch_size=64, normalise=True, train_transforms=[transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 2, padding_mode='edge')], val_split=True, data_root='./data'):\n    \"\"\"\n    Load the CIFAR-10 dataset with optional normalization and data augmentation.\n\n    Args:\n        batch_size (int, optional): The number of samples per batch to load. Default is 64.\n        normalise (bool, optional): Whether to normalize the dataset. Default is True.\n        train_transforms (list, optional): List of transformations to apply to the training data. Default is [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 2, padding_mode='edge')].\n        val_split (bool, optional): Whether to split the training data into training and validation sets. Default is True.\n        data_root (str, optional): Root directory where the dataset will be stored. Default is './data'.\n\n    Returns:\n        tuple: If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).\n\n    The DataLoader objects have the following attributes:\n        - mean (torch.Tensor): The mean used for normalization.\n        - std (torch.Tensor): The standard deviation used for normalization.\n        - min (torch.Tensor): The minimum value in the dataset.\n        - max (torch.Tensor): The maximum value in the dataset.\n        - normalised (bool): Whether the dataset was normalized.\n    \"\"\"\n    if normalise:\n        mean = torch.tensor([0.4914, 0.4822, 0.4465])\n        std = torch.tensor([0.247, 0.243, 0.261])\n    else:\n        mean = torch.tensor([.5, .5, .5])\n        std = torch.tensor([1, 1, 1])\n    if normalise:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n    else:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor(),\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n    train_dataset = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n    test_dataset = datasets.CIFAR10(root=data_root, train=False, transform=test_transform, download=True)\n    if val_split:\n        train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        val_loader.mean, val_loader.std = mean, std\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    # Initialize variables to track the min and max values\n    min_val = float('inf')\n    max_val = float('-inf')\n\n    # Loop over the dataset\n    for data, _ in train_loader:\n        min_val = torch.tensor(min(min_val, data.min().item()))\n        max_val = torch.tensor(max(max_val, data.max().item()))\n\n    print(f'CIFAR10 dataset - Min value: {min_val}, Max value: {max_val}')\n\n    train_loader.mean, train_loader.std, train_loader.min, train_loader.max, train_loader.normalised = mean, std, min_val, max_val, normalise\n    test_loader.mean, test_loader.std, test_loader.min, test_loader.max, test_loader.normalised = mean, std, min_val, max_val, normalise\n\n    if val_split:\n        val_loader.mean, val_loader.std, val_loader.min, val_loader.max, val_loader.normalised = mean, std, min_val, max_val, normalise\n        return train_loader, val_loader, test_loader\n    else:\n        return train_loader, test_loader\n</code></pre>"},{"location":"api/data_loaders/data_loaders/#CTRAIN.data_loaders.data_loaders.load_gtsrb","title":"<code>load_gtsrb(batch_size=64, normalise=True, train_transforms=[transforms.RandomRotation(10), transforms.RandomResizedCrop(size=(30, 30), scale=(0.85, 1.0)), transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=15)], val_split=True, data_root='./data')</code>","text":"<p>Load the German Traffic Sign Recognition Benchmark (GTSRB) dataset with optional normalization and data augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples per batch to load. Default is 64.</p> <code>64</code> <code>normalise</code> <code>bool</code> <p>Whether to normalize the dataset using predefined mean and std values. Default is True.</p> <code>True</code> <code>train_transforms</code> <code>list</code> <p>List of torchvision transforms to apply to the training data. Default includes random rotation, resized crop, and affine transformations.</p> <code>[RandomRotation(10), RandomResizedCrop(size=(30, 30), scale=(0.85, 1.0)), RandomAffine(degrees=0, translate=(0.1, 0.1), shear=15)]</code> <code>val_split</code> <code>bool</code> <p>Whether to split the training data into training and validation sets. Default is True.</p> <code>True</code> <code>data_root</code> <code>str</code> <p>Root directory where the dataset is stored or will be downloaded. Default is './data'.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).</p> The DataLoader objects returned have additional attributes <ul> <li>mean (torch.Tensor): The mean used for normalization.</li> <li>std (torch.Tensor): The standard deviation used for normalization.</li> <li>min (torch.Tensor): The minimum value in the dataset.</li> <li>max (torch.Tensor): The maximum value in the dataset.</li> <li>normalised (bool): Whether the dataset was normalized.</li> </ul> Source code in <code>CTRAIN/data_loaders/data_loaders.py</code> <pre><code>def load_gtsrb(batch_size=64, normalise=True, train_transforms=[transforms.RandomRotation(10), transforms.RandomResizedCrop(size=(30, 30), scale=(0.85, 1.0)), transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=15),], val_split=True, data_root='./data'):\n    \"\"\"\n    Load the German Traffic Sign Recognition Benchmark (GTSRB) dataset with optional normalization and data augmentation.\n\n    Args:\n        batch_size (int, optional): The number of samples per batch to load. Default is 64.\n        normalise (bool, optional): Whether to normalize the dataset using predefined mean and std values. Default is True.\n        train_transforms (list, optional): List of torchvision transforms to apply to the training data. Default includes random rotation, resized crop, and affine transformations.\n        val_split (bool, optional): Whether to split the training data into training and validation sets. Default is True.\n        data_root (str, optional): Root directory where the dataset is stored or will be downloaded. Default is './data'.\n\n    Returns:\n        tuple: If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).\n\n    The DataLoader objects returned have additional attributes:\n        - mean (torch.Tensor): The mean used for normalization.\n        - std (torch.Tensor): The standard deviation used for normalization.\n        - min (torch.Tensor): The minimum value in the dataset.\n        - max (torch.Tensor): The maximum value in the dataset.\n        - normalised (bool): Whether the dataset was normalized.\n    \"\"\"\n    if normalise:\n        mean = torch.tensor([0.3403, 0.3121, 0.3214])\n        std =  torch.tensor([0.2724, 0.2608, 0.2669])\n    else:\n        mean = torch.tensor([.5, .5, .5])\n        std = torch.tensor([1, 1, 1])\n    if normalise:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.Resize((30, 30)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((30, 30)),\n            transforms.Normalize(mean, std),\n        ])\n    else:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.Resize((30, 30)),\n            transforms.ToTensor(),\n        ])\n        test_transform = transforms.Compose([\n            transforms.Resize((30, 30)),\n            transforms.ToTensor(),\n        ])\n\n    train_dataset_ori = datasets.GTSRB(root=data_root, split='train', download=True, transform=train_transform)\n    test_dataset = datasets.GTSRB(root=data_root, split='test', download=True, transform=test_transform)\n\n    if val_split:\n        file_names = train_dataset_ori._samples\n        track_id_to_sample = [name[0].split(\"/\")[-2] + \"_\" + name[0].split(\"/\")[-1].split(\"_\")[0] for name in file_names]\n        track_ids = set(track_id_to_sample)\n        train_track_ids, val_track_ids = train_test_split(np.array(list(track_ids)), test_size=0.2, train_size=0.8)\n        train_ids = [i for i in range(len(track_id_to_sample)) if track_id_to_sample[i] in train_track_ids]\n        val_ids = [i for i in range(len(track_id_to_sample)) if track_id_to_sample[i] in val_track_ids]\n\n\n        train_dataset = Subset(train_dataset_ori, train_ids)\n        val_dataset = Subset(train_dataset_ori, val_ids)\n\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n        val_loader.mean, val_loader.std = mean, std\n\n    else:\n        train_dataset = train_dataset_ori\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Initialize variables to track the min and max values\n    min_val = float('inf')\n    max_val = float('-inf')\n\n    max_target = float('-inf')\n    # Loop over the dataset\n    for data, target in train_loader:\n        min_val = torch.tensor(min(min_val, data.min().item()))\n        max_val = torch.tensor(max(max_val, data.max().item()))\n        max_target = torch.tensor(max(max_target, target.max().item()))\n\n\n    print(f'GTSRB dataset - Min value: {min_val}, Max value: {max_val}, no_classes: {max_target}')\n\n    train_loader.mean, train_loader.std, train_loader.min, train_loader.max, train_loader.normalised = mean, std, min_val, max_val, normalise\n    test_loader.mean, test_loader.std, test_loader.min, test_loader.max, test_loader.normalised = mean, std, min_val, max_val, normalise\n\n    if val_split:\n        val_loader.mean, val_loader.std, val_loader.min, val_loader.max, val_loader.normalised = mean, std, min_val, max_val, normalise\n        return train_loader, val_loader, test_loader\n    else:\n        return train_loader, test_loader\n</code></pre>"},{"location":"api/data_loaders/data_loaders/#CTRAIN.data_loaders.data_loaders.load_mnist","title":"<code>load_mnist(batch_size=64, normalise=True, train_transforms=[], val_split=True, data_root='./data')</code>","text":"<p>Loads the MNIST dataset with specified transformations and returns data loaders.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples per batch to load. Default is 64.</p> <code>64</code> <code>normalise</code> <code>bool</code> <p>Whether to normalize the dataset. Default is True.</p> <code>True</code> <code>train_transforms</code> <code>list</code> <p>List of additional transformations to apply to the training data. Default is an empty list.</p> <code>[]</code> <code>val_split</code> <code>bool</code> <p>Whether to split the training data into training and validation sets. Default is True.</p> <code>True</code> <code>data_root</code> <code>str</code> <p>Root directory where the dataset will be stored. Default is './data'.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).</p> The returned DataLoader objects have the following additional attributes <ul> <li>mean (torch.Tensor): The mean value used for normalization.</li> <li>std (torch.Tensor): The standard deviation value used for normalization.</li> <li>min (torch.Tensor): The minimum value in the dataset.</li> <li>max (torch.Tensor): The maximum value in the dataset.</li> <li>normalised (bool): Whether the dataset was normalized.</li> </ul> Example <p>train_loader, val_loader, test_loader = load_mnist(batch_size=32, normalise=True, train_transforms=[transforms.RandomRotation(10)], val_split=True, data_root='./data')</p> Source code in <code>CTRAIN/data_loaders/data_loaders.py</code> <pre><code>def load_mnist(batch_size=64, normalise=True, train_transforms=[], val_split=True, data_root='./data'): # TODO replace to multi epoch pre-loader\n    \"\"\"\n    Loads the MNIST dataset with specified transformations and returns data loaders.\n\n    Args:\n        batch_size (int, optional): The number of samples per batch to load. Default is 64.\n        normalise (bool, optional): Whether to normalize the dataset. Default is True.\n        train_transforms (list, optional): List of additional transformations to apply to the training data. Default is an empty list.\n        val_split (bool, optional): Whether to split the training data into training and validation sets. Default is True.\n        data_root (str, optional): Root directory where the dataset will be stored. Default is './data'.\n\n    Returns:\n        tuple: If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).\n\n\n    The returned DataLoader objects have the following additional attributes:\n        - mean (torch.Tensor): The mean value used for normalization.\n        - std (torch.Tensor): The standard deviation value used for normalization.\n        - min (torch.Tensor): The minimum value in the dataset.\n        - max (torch.Tensor): The maximum value in the dataset.\n        - normalised (bool): Whether the dataset was normalized.\n\n    Example:\n        train_loader, val_loader, test_loader = load_mnist(batch_size=32, normalise=True, train_transforms=[transforms.RandomRotation(10)], val_split=True, data_root='./data')\n    \"\"\"\n    if normalise:\n        mean = torch.tensor([0.1307])\n        std = torch.tensor([0.3081])\n    else:\n        mean = torch.tensor([.5])\n        std = torch.tensor([1])\n    if normalise:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n    else:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor()    \n        ])\n        test_transform = transforms.Compose([transforms.ToTensor()])\n\n    train_dataset = datasets.MNIST(root=data_root, train=True, download=True, transform=train_transform)\n    test_dataset = datasets.MNIST(root=data_root, train=False, transform=test_transform)\n    if val_split:\n        train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    # Initialize variables to track the min and max values\n    min_val = float('inf')\n    max_val = float('-inf')\n\n    # Loop over the dataset\n    for data, _ in train_loader:\n        min_val = min(min_val, data.min().item())\n        max_val = max(max_val, data.max().item())\n\n    min_val, max_val = torch.tensor(min_val), torch.tensor(max_val)\n\n    print(f'MNIST dataset - Min value: {min_val}, Max value: {max_val}')\n\n    train_loader.mean, train_loader.std, train_loader.min, train_loader.max, train_loader.normalised = mean, std, min_val, max_val, normalise\n    test_loader.mean, test_loader.std, test_loader.min, test_loader.max, test_loader.normalised = mean, std, min_val, max_val, normalise\n\n    if val_split:\n        val_loader.mean, val_loader.std, val_loader.min, val_loader.max, val_loader.normalised = mean, std, min_val, max_val, normalise\n        return train_loader, val_loader, test_loader\n    else:\n        return train_loader, test_loader\n</code></pre>"},{"location":"api/data_loaders/data_loaders/#CTRAIN.data_loaders.data_loaders.load_tinyimagenet","title":"<code>load_tinyimagenet(batch_size=64, normalise=True, train_transforms=[transforms.RandomHorizontalFlip(), transforms.RandomCrop(64, 4, padding_mode='edge')], val_split=True, data_root='./data')</code>","text":"<p>Loads the TinyImageNet dataset with specified transformations and normalization.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples per batch. Default is 64.</p> <code>64</code> <code>normalise</code> <code>bool</code> <p>Whether to normalize the dataset. Default is True.</p> <code>True</code> <code>train_transforms</code> <code>list</code> <p>List of transformations to apply to the training data. Default is [transforms.RandomHorizontalFlip(), transforms.RandomCrop(64, 4, padding_mode='edge')].</p> <code>[RandomHorizontalFlip(), RandomCrop(64, 4, padding_mode='edge')]</code> <code>val_split</code> <code>bool</code> <p>Whether to split the training data into training and validation sets. Default is True.</p> <code>True</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Default is './data'.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).</p> The returned DataLoader objects have the following attributes <ul> <li>mean: The mean used for normalization.</li> <li>std: The standard deviation used for normalization.</li> <li>min: The minimum value in the dataset.</li> <li>max: The maximum value in the dataset.</li> <li>normalised: Whether the dataset was normalized.</li> </ul> Source code in <code>CTRAIN/data_loaders/data_loaders.py</code> <pre><code>def load_tinyimagenet(batch_size=64, normalise=True, train_transforms=[transforms.RandomHorizontalFlip(), transforms.RandomCrop(64, 4, padding_mode='edge'),], val_split=True, data_root='./data'):\n    \"\"\"\n    Loads the TinyImageNet dataset with specified transformations and normalization.\n\n    Args:\n        batch_size (int, optional): The number of samples per batch. Default is 64.\n        normalise (bool, optional): Whether to normalize the dataset. Default is True.\n        train_transforms (list, optional): List of transformations to apply to the training data. Default is [transforms.RandomHorizontalFlip(), transforms.RandomCrop(64, 4, padding_mode='edge')].\n        val_split (bool, optional): Whether to split the training data into training and validation sets. Default is True.\n        data_root (str, optional): Root directory of the dataset. Default is './data'.\n\n    Returns:\n        tuple: If val_split is True, returns a tuple of DataLoader objects (train_loader, val_loader, test_loader). Otherwise, returns (train_loader, test_loader).\n\n    The returned DataLoader objects have the following attributes:\n        - mean: The mean used for normalization.\n        - std: The standard deviation used for normalization.\n        - min: The minimum value in the dataset.\n        - max: The maximum value in the dataset.\n        - normalised: Whether the dataset was normalized.\n    \"\"\"\n    if normalise:\n        mean = torch.tensor([0.4802, 0.4481, 0.3975])\n        std = torch.tensor([0.2302, 0.2265, 0.2262])\n    else:\n        mean = torch.tensor([.5, .5, .5])\n        std = torch.tensor([1, 1, 1])\n\n    if normalise:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ])\n    else:\n        train_transform = transforms.Compose([\n            *train_transforms,\n            transforms.ToTensor(),\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n    train_dataset = datasets.ImageFolder(root=data_root + '/train', transform=train_transform)\n    test_dataset = datasets.ImageFolder(root=data_root + '/val', transform=test_transform)\n    if val_split:\n        train_dataset, val_dataset = random_split(train_dataset, [0.8, 0.2])\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        val_loader.mean, val_loader.std = mean, std\n\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    # Initialize variables to track the min and max values\n    min_val = float('inf')\n    max_val = float('-inf')\n\n    # Loop over the dataset\n    for data, _ in train_loader:\n        min_val = torch.tensor(min(min_val, data.min().item()))\n        max_val = torch.tensor(max(max_val, data.max().item()))\n\n    print(f'TinyImageNet dataset - Min value: {min_val}, Max value: {max_val}')\n\n    train_loader.mean, train_loader.std, train_loader.min, train_loader.max, train_loader.normalised = mean, std, min_val, max_val, normalise\n    test_loader.mean, test_loader.std, test_loader.min, test_loader.max, test_loader.normalised = mean, std, min_val, max_val, normalise\n\n    if val_split:\n        val_loader.mean, val_loader.std, val_loader.min, val_loader.max, val_loader.normalised = mean, std, min_val, max_val, normalise\n        return train_loader, val_loader, test_loader\n    else:\n        return train_loader, test_loader\n</code></pre>"},{"location":"api/eval/eval/","title":"eval","text":""},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_acc","title":"<code>eval_acc(model, test_loader, test_samples=np.inf)</code>","text":"<p>Evaluate the accuracy of a given model on a test dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be evaluated.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for the test dataset.</p> required <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <p>Returns:</p> Type Description <code>float</code> <p>The accuracy of the model on the test dataset.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_acc(model, test_loader, test_samples=np.inf):\n    \"\"\"\n    Evaluate the accuracy of a given model on a test dataset.\n\n    Args:\n        model (torch.nn.Module): The model to be evaluated.\n        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n\n    Returns:\n        (float): The accuracy of the model on the test dataset.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            if total &gt;= test_samples:\n                break\n            batch_indices = min(len(target), test_samples - total)\n            data = data[:batch_indices]\n            target = target[:batch_indices]\n\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n\n            correct += (predicted == target).sum().item()\n\n    test_samples = min(test_samples, total)\n    # print(f'Accuracy of the standard model on the first {test_samples} test images: {correct / test_samples:.4f}')\n    return correct / test_samples\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_adaptive","title":"<code>eval_adaptive(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda', methods=['IBP', 'CROWN_IBP', 'CROWN'])</code>","text":"<p>Evaluate the model in terms of certified accuracy in an adaptive method. This means, that all methods  passed in the methods parameter are used to certify the samples in ascending order of computational complexity (IBP &lt; CROWN-IBP &lt; CROWN).  If a sample is certified by one method, it is not evaluated by the following methods.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>eps</code> <code>float</code> <p>The perturbation bound.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to be evaluated.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <code>device</code> <code>str</code> <p>Device to run the evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the number of certified samples, total number of images evaluated, and a tensor holding per-instance certification results.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_adaptive(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda', methods=[\"IBP\", \"CROWN_IBP\", \"CROWN\"]):\n    \"\"\"\n    Evaluate the model in terms of certified accuracy in an adaptive method. This means, that all methods \n    passed in the methods parameter are used to certify the samples in ascending order of computational complexity (IBP &lt; CROWN-IBP &lt; CROWN). \n    If a sample is certified by one method, it is not evaluated by the following methods.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        eps (float): The perturbation bound.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to be evaluated.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n        device (str, optional): Device to run the evaluation on. Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the number of certified samples, total number of images evaluated, and a tensor holding per-instance certification results.\n    \"\"\"\n    assert methods is not None and len(methods) &gt; 1, \"Please provide at least one bounding method!\"\n\n    certified = torch.tensor([], device=device)\n    total_images = 0\n\n    crown_data_loader = DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n    crown_data_loader.max, crown_data_loader.min, crown_data_loader.std = data_loader.max, data_loader.min, data_loader.std\n\n    for batch_idx, (data, targets) in tqdm(enumerate(data_loader)):\n        certified_idx = torch.zeros(len(data), device=device, dtype=torch.bool)\n\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data + eps, data_loader.min, data_loader.max).to(device))\n        data, targets = data.to(device), targets.to(device)\n        if batch_idx * data_loader.batch_size &gt;= test_samples:\n            continue\n\n        total_images += len(targets)\n\n        if \"IBP\" in methods:\n            lb, ub = bound_ibp(\n                model=model,\n                ptb=ptb,\n                data=data,\n                target=targets,\n                n_classes=n_classes,\n                reuse_input=False\n            )\n            certified_idx[(lb &gt; 0).all(dim=1)] = True\n\n        data = data.to('cpu')\n        certified_idx = certified_idx.to(\"cpu\")\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data[~certified_idx] - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data[~certified_idx] + eps, data_loader.min, data_loader.max).to(device))\n        data = data.to(device)\n        certified_idx = certified_idx.to(device)\n\n        if len(certified_idx) &lt; len(targets) and \"CROWN-IBP\" in methods:        \n            lb, ub = bound_crown_ibp(\n                model=model,\n                ptb=ptb,\n                data=data[~certified_idx],\n                target=targets[~certified_idx],\n                n_classes=n_classes,\n                reuse_input=False\n            )\n            certified_idx[~certified_idx] = (lb &gt; 0).all(dim=1)\n\n        certified = torch.concatenate((certified, certified_idx))\n\n    print(f\"certified {torch.sum(certified).item()} / {len(certified)} using IBP\", flush=True)\n\n    for batch_idx, (data, targets) in tqdm(enumerate(crown_data_loader)):\n        if batch_idx &gt;= test_samples:\n            continue\n        if certified[batch_idx] or not (\"CROWN\" in methods):\n            continue\n\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data + eps, data_loader.min, data_loader.max).to(device))\n        data, targets = data.to(device), targets.to(device)\n\n        lb, ub = bound_crown(\n            model=model,\n            ptb=ptb,\n            data=data,\n            target=targets,\n            n_classes=n_classes,\n            reuse_input=False\n        )\n        instance_certified = (lb &gt; 0).all(dim=1).item()\n        certified[batch_idx] = instance_certified\n\n    if test_samples &lt; np.inf:\n        certified = certified[:test_samples]\n    no_certified = torch.sum(certified)            \n    total_images = len(certified)\n\n    if 'CROWN' in methods:\n        print(f\"certified {torch.sum(certified).item()} / {len(certified)} after using CROWN\", flush=True)\n\n    return no_certified, total_images, certified\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_adversarial","title":"<code>eval_adversarial(model, data_loader, eps, return_adv_indices=False, restarts=5, step_size=0.1, n_steps=40, early_stopping=False, n_classes=10, device='cuda', test_samples=np.inf)</code>","text":"<p>Evaluate the adversarial robustness of a model using the PGD attack.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to evaluate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader providing the dataset.</p> required <code>eps</code> <code>Tensor</code> <p>The perturbation radius for the PGD attack.</p> required <code>return_adv_indices</code> <code>bool</code> <p>Whether to return the indices of adversarial samples. Default is False.</p> <code>False</code> <code>restarts</code> <code>int</code> <p>Number of random restarts for the PGD attack. Default is 5.</p> <code>5</code> <code>step_size</code> <code>float</code> <p>Step size for the PGD attack. Default is 0.1.</p> <code>0.1</code> <code>n_steps</code> <code>int</code> <p>Number of steps for the PGD attack. Default is 40.</p> <code>40</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop early if an adversarial example is found. Default is False.</p> <code>False</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to perform computations on. Default is 'cuda'.</p> <code>'cuda'</code> <code>test_samples</code> <code>int or float</code> <p>Number of samples to test. Default is np.inf.</p> <code>inf</code> <p>Returns:</p> Type Description <code>float</code> <p>Adversarial accuracy of the model.</p> <code>ndarray</code> <p>Indices of unsafe indices if return_adv_indices is True.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_adversarial(model, data_loader, eps, return_adv_indices=False, restarts=5, step_size=0.1, n_steps=40, early_stopping=False, n_classes=10, device='cuda', test_samples=np.inf,):\n    \"\"\"\n    Evaluate the adversarial robustness of a model using the PGD attack.\n\n    Parameters:\n        model (torch.nn.Module): The model to evaluate.\n        data_loader (torch.utils.data.DataLoader): DataLoader providing the dataset.\n        eps (torch.Tensor): The perturbation radius for the PGD attack.\n        return_adv_indices (bool, optional): Whether to return the indices of adversarial samples. Default is False.\n        restarts (int, optional): Number of random restarts for the PGD attack. Default is 5.\n        step_size (float, optional): Step size for the PGD attack. Default is 0.1.\n        n_steps (int, optional): Number of steps for the PGD attack. Default is 40.\n        early_stopping (bool, optional): Whether to stop early if an adversarial example is found. Default is False.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        device (str, optional): Device to perform computations on. Default is 'cuda'.\n        test_samples (int or float, optional): Number of samples to test. Default is np.inf.\n\n    Returns:\n        (float): Adversarial accuracy of the model.\n        (np.ndarray): Indices of unsafe indices if return_adv_indices is True.\n    \"\"\"\n    model.eval()\n    adv_preds = np.array([])\n    labels = np.array([])\n    data_min = data_loader.min.to(device)\n    data_max = data_loader.max.to(device)\n\n    for batch_idx, (data, targets) in tqdm(enumerate(data_loader)):\n        if len(labels) &gt;= test_samples:\n            break\n\n        batch_indices = min(len(targets), test_samples - len(labels))\n        data = data[:batch_indices]\n        targets = targets[:batch_indices]\n\n        data, targets = data.to(device), targets.to(device)\n        eps = eps.to(device)\n\n        x_test_adv = pgd_attack(\n            model=model,\n            data=data,\n            target=targets,\n            x_L=torch.clamp(data - eps, data_min, data_max).to(device), \n            x_U=torch.clamp(data + eps, data_min, data_max).to(device),\n            restarts=restarts,\n            step_size=step_size,\n            n_steps=n_steps,\n            early_stopping=early_stopping,\n            device=device\n        )\n\n        adv_predictions_batch = model(x_test_adv)\n        adv_predictions_batch = torch.argmax(adv_predictions_batch, dim=1).cpu().numpy()\n        if len(labels) + len(targets) &gt; test_samples:\n            too_many_samples_no = (len(labels) + len(targets)) % test_samples\n            adv_predictions_batch = adv_predictions_batch[:-too_many_samples_no]\n            targets = targets[:-too_many_samples_no]\n\n        adv_preds = np.append(adv_preds, adv_predictions_batch)\n        labels = np.append(labels, targets.cpu())\n\n    test_samples = min(test_samples, len(labels))\n\n    adv_accuracy = accuracy_score(labels, adv_preds)\n\n    if return_adv_indices:\n        adv_sample_found = labels != adv_preds\n        return adv_accuracy, adv_sample_found\n\n    return adv_accuracy\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_certified","title":"<code>eval_certified(model, data_loader, eps, n_classes=10, test_samples=np.inf, method='IBP')</code>","text":"<p>Evaluate the certified robustness of a model using a given verification method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to be evaluated.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>eps</code> <code>float</code> <p>Perturbation radius for certification.</p> required <code>test_samples</code> <code>int or float</code> <p>Number of test samples to evaluate. Default is np.inf (all samples).</p> <code>inf</code> <code>method</code> <code>str or list</code> <p>The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', 'COMPLETE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'IBP'.</p> <code>'IBP'</code> <p>Returns:</p> Type Description <code>float</code> <p>The certified accuracy of the model on the test examples for the given epsilon.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_certified(model, data_loader, eps, n_classes=10, test_samples=np.inf, method='IBP'):\n    \"\"\"\n    Evaluate the certified robustness of a model using a given verification method.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to be evaluated.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        eps (float): Perturbation radius for certification.\n        test_samples (int or float, optional): Number of test samples to evaluate. Default is np.inf (all samples).\n        method (str or list, optional): The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', 'COMPLETE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'IBP'.\n\n    Returns:\n        (float): The certified accuracy of the model on the test examples for the given epsilon.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    model.eval()\n    certified = 0\n    total_images = 0\n\n    if method == \"CROWN\":\n        certified, total_images = eval_crown(model, eps, data_loader, n_classes, test_samples, device)\n    elif method == 'IBP':\n        certified, total_images = eval_ibp(model, eps, data_loader, n_classes, test_samples, device)\n    elif method == 'CROWN-IBP':\n        certified, total_images = eval_crown_ibp(model, eps, data_loader, n_classes, test_samples, device)\n    elif method == 'ADAPTIVE':\n        certified, total_images, cert_results = eval_adaptive(model, eps, data_loader, n_classes, test_samples, device)\n    elif isinstance(method, list):\n        certified, total_images, cert_results = eval_adaptive(model, eps, data_loader, n_classes, test_samples, device, methods=method)\n    elif method == 'COMPLETE':\n        # TODO: Infer input shape or pass it, pass timeout and no_cores!\n        eval_complete_abcrown(model, eps, data_loader, n_classes, input_shape=(1, 28, 28), test_samples=test_samples, timeout=1000, no_cores=28, device=device)\n    elif isinstance(method, (list, tuple, np.ndarray)):\n        certified, total_images, cert_results = eval_adaptive(model, eps, data_loader, n_classes, test_samples, device, methods=method)\n    else:\n        assert False, \"UNKNOWN BOUNDING METHOD!\"\n\n    return (certified / total_images).cpu().item() if torch.is_tensor(certified) else certified / total_images\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_complete_abcrown","title":"<code>eval_complete_abcrown(model, eps_std, data_loader, n_classes=10, input_shape=[1, 28, 28], test_samples=np.inf, timeout=1000, no_cores=28, abcrown_batch_size=512, separate_abcrown_process=False, device='cuda')</code>","text":"<p>Evaluate the model using the complete ABCROWN method. Attention, this evaluation may be very costly!</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>eps_std</code> <code>float</code> <p>The standard deviation of the perturbation bound.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to be evaluated.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>input_shape</code> <code>list</code> <p>Shape of the input data. Default is [1, 28, 28].</p> <code>[1, 28, 28]</code> <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <code>timeout</code> <code>int</code> <p>Timeout for the ABCROWN evaluation in seconds. Default is 1000.</p> <code>1000</code> <code>no_cores</code> <code>int</code> <p>Number of cores to use for MIP solving during the ABCROWN evaluation (if configured). Default is 28.</p> <code>28</code> <code>abcrown_batch_size</code> <code>int</code> <p>Batch size for the ABCROWN evaluation. Default is 512.</p> <code>512</code> <code>separate_abcrown_process</code> <code>bool</code> <p>Whether to run ABCROWN in a separate process. Default is False.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the certified accuracy and the adversarial accuracy.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_complete_abcrown(model, eps_std, data_loader, n_classes=10, input_shape=[1, 28, 28], test_samples=np.inf, timeout=1000, no_cores=28, abcrown_batch_size=512, separate_abcrown_process=False, device='cuda'):\n    \"\"\"\n    Evaluate the model using the complete ABCROWN method. Attention, this evaluation may be very costly!\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        eps_std (float): The standard deviation of the perturbation bound.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to be evaluated.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        input_shape (list, optional): Shape of the input data. Default is [1, 28, 28].\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n        timeout (int, optional): Timeout for the ABCROWN evaluation in seconds. Default is 1000.\n        no_cores (int, optional): Number of cores to use for MIP solving during the ABCROWN evaluation (if configured). Default is 28.\n        abcrown_batch_size (int, optional): Batch size for the ABCROWN evaluation. Default is 512.\n        separate_abcrown_process (bool, optional): Whether to run ABCROWN in a separate process. Default is False.\n        device (str, optional): Device to run the evaluation on. Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the certified accuracy and the adversarial accuracy.\n    \"\"\"\n    no_certified, total_images, certified = eval_adaptive(model=model, eps=eps_std, data_loader=data_loader, n_classes=n_classes, test_samples=test_samples, device=device)\n    adv_acc, adv_sample_found = eval_adversarial(model=model, data_loader=data_loader, eps=eps_std, n_classes=n_classes, device=device, test_samples=test_samples, return_adv_indices=True)\n    adv_sample_found = torch.tensor(adv_sample_found)\n    total_images = 0\n\n    batch_size = data_loader.batch_size\n\n    std_config = get_abcrown_standard_conf(timeout=timeout, no_cores=no_cores)\n    std_config['solver']['batch_size'] = abcrown_batch_size\n\n    os.makedirs('/tmp/abCROWN/', exist_ok=True)\n\n    export_onnx(\n        model=model,\n        file_name='/tmp/abCROWN_model.onnx',\n        batch_size=1, \n        input_shape=input_shape\n    )\n\n    for batch_idx, (data, targets) in tqdm(enumerate(data_loader)):\n        if total_images &gt;= test_samples:\n            break\n\n        batch_indices = min(len(targets), test_samples - total_images)\n        data = data[:batch_indices]\n        targets = targets[:batch_indices]\n        total_images += len(targets)\n\n        print(f\"BATCH {batch_idx}\")\n\n        clean_pred = torch.argmax(model(data.to(device)), dim=1)\n        clean_correct = clean_pred.cpu() == targets\n\n        if certified[batch_idx * batch_size:(batch_idx + 1) * batch_size].all():\n            continue\n\n        os.makedirs(f'/tmp/vnnlib_{batch_idx}/', exist_ok=True)\n\n        vnnlib_batch = instances_to_vnnlib(\n            indices=[i for i in range(len(targets)) if not certified[batch_idx * batch_size + i] and not adv_sample_found[batch_idx * batch_size + i] and clean_correct[i]],\n            data=[(img, target) for img, target in zip(data, targets)],\n            vnnlib_path=f'/tmp/vnnlib_{batch_idx}/',\n            experiment_name='Experiment',\n            eps=eps_std * data_loader.std,\n            eps_temp=eps_std,\n            data_min=data_loader.min,\n            data_max=data_loader.max,\n            no_classes=n_classes\n        )\n        vnnlib_indices = [batch_idx * batch_size + i for i in range(len(targets)) if not certified[batch_idx * batch_size + i] and not adv_sample_found[batch_idx * batch_size + i] and clean_correct[i]]\n        print(vnnlib_indices)\n        for idx, vnn_instance in zip(vnnlib_indices, vnnlib_batch):\n            if separate_abcrown_process:\n                running_time, result = limited_abcrown_eval(\n                    # work_dir='/tmp/abCROWN',\n                    config=std_config,\n                    seed=42,\n                    instance=vnn_instance,\n                    vnnlib_path=f'/tmp/vnnlib_{batch_idx}',\n                    model_path=None,\n                    model_name=None,\n                    model_onnx_path='/tmp/abCROWN_model.onnx',\n                    input_shape=[-1] + input_shape[1:4],\n                    timeout=timeout,\n                    no_cores=no_cores,\n                    par_factor=1\n                )\n            else:\n                running_time, result = abcrown_eval(\n                    # work_dir='/tmp/abCROWN',\n                    config=std_config,\n                    seed=42,\n                    instance=vnn_instance,\n                    vnnlib_path=f'/tmp/vnnlib_{batch_idx}',\n                    model_path=None,\n                    model_name=None,\n                    model_onnx_path='/tmp/abCROWN_model.onnx',\n                    input_shape=[-1] + input_shape[1:4],\n                    timeout=timeout,\n                    no_cores=no_cores,\n                    par_factor=1\n                )\n            print(running_time, result)\n            if result == 'unsat':\n                no_certified += 1\n                certified[idx] = True\n            if result == 'sat':\n                adv_sample_found[idx] = True\n\n        shutil.rmtree(f\"/tmp/vnnlib_{batch_idx}/\")\n\n    if test_samples &lt; np.inf:\n        certified = certified[:test_samples]\n    no_certified = torch.sum(certified)    \n    no_counterexample = torch.sum(adv_sample_found)\n\n    certified_acc = (no_certified / test_samples).cpu().item() if torch.is_tensor(certified) else certified / test_samples\n    adv_acc = (test_samples  - no_counterexample) / test_samples\n    if torch.is_tensor(adv_acc):\n        adv_acc = adv_acc.item()\n\n    return certified_acc, adv_acc\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_crown","title":"<code>eval_crown(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda')</code>","text":"<p>Evaluate the model using the CROWN method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>eps</code> <code>float</code> <p>The perturbation bound.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to be evaluated.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <code>device</code> <code>str</code> <p>Device to run the evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the number of certified samples and the total number of images evaluated.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_crown(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda'):\n    \"\"\"\n    Evaluate the model using the CROWN method.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        eps (float): The perturbation bound.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to be evaluated.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n        device (str, optional): Device to run the evaluation on. Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the number of certified samples and the total number of images evaluated.\n    \"\"\"\n    # IMPORTANT: Data Loader Batch Size must match Bounding Batch Size when using CROWN for evaluation (not important for IBP)\n    crown_data_loader = DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n    crown_data_loader.max, crown_data_loader.min, crown_data_loader.std = data_loader.max, data_loader.min, data_loader.std\n    certified = 0\n    total_images = 0\n    for batch_idx, (data, targets) in tqdm(enumerate(crown_data_loader)):\n        if total_images &gt;= test_samples:\n            continue\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data + eps, data_loader.min, data_loader.max).to(device))\n        data, targets = data.to(device), targets.to(device)\n        lb, ub = bound_crown(\n            model=model,\n            ptb=ptb,\n            data=data,\n            target=targets,\n            n_classes=n_classes,\n            reuse_input=False\n        )\n        no_certified = torch.sum((lb &gt; 0).all(dim=1)).item()\n        # no_falsified = torch.sum((ub &lt; 0).any(dim=1)).item()\n        certified += no_certified\n\n        total_images += len(targets)\n\n    return certified, total_images\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_crown_ibp","title":"<code>eval_crown_ibp(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda')</code>","text":"<p>Evaluate the model using the CROWN-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>eps</code> <code>float</code> <p>The perturbation bound.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to be evaluated.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <code>device</code> <code>str</code> <p>Device to run the evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the number of certified samples and the total number of images evaluated.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_crown_ibp(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda'):\n    \"\"\"\n    Evaluate the model using the CROWN-IBP method.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        eps (float): The perturbation bound.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to be evaluated.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n        device (str, optional): Device to run the evaluation on. Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the number of certified samples and the total number of images evaluated.\n    \"\"\"\n    certified = 0\n    total_images = 0\n    for batch_idx, (data, targets) in tqdm(enumerate(data_loader)):\n        if total_images &gt;= test_samples:\n            continue\n\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data + eps, data_loader.min, data_loader.max).to(device))\n        data, targets = data.to(device), targets.to(device)\n\n        lb, ub = bound_crown_ibp(\n            model=model,\n            ptb=ptb,\n            data=data,\n            target=targets,\n            n_classes=n_classes,\n            reuse_input=False\n        )\n        no_certified = torch.sum((lb &gt; 0).all(dim=1)).item()\n        # no_falsified = torch.sum((ub &lt; 0).any(dim=1)).item()\n        certified += no_certified\n\n        total_images += len(targets)\n\n    return certified, total_images\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_epoch","title":"<code>eval_epoch(model, data_loader, eps, n_classes, device='cuda', test_samples=1000, verification_method='IBP', results_path='./results')</code>","text":"<p>Evaluate the model during training. It computes the standard accuracy, certified accuracy,  and adversarial accuracy of the model. The results are saved to a specified path in JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be evaluated.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset to evaluate.</p> required <code>eps</code> <code>Tensor</code> <p>Perturbation radius used during evaluation of standard, certified and adversarial accuracy.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset.</p> required <code>device</code> <code>str</code> <p>Device to run the evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <code>test_samples</code> <code>int</code> <p>Number of samples to use for evaluation. Default is 1000.</p> <code>1000</code> <code>verification_method</code> <code>str</code> <p>Method to use for certification. Default is \"IBP\".</p> <code>'IBP'</code> <code>results_path</code> <code>str</code> <p>Path to save the evaluation results. Default is \"./results\".</p> <code>'./results'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing standard accuracy, certified accuracy, and adversarial accuracy.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_epoch(model, data_loader, eps, n_classes, device='cuda', test_samples=1000, verification_method=\"IBP\", results_path=\"./results\"):\n    \"\"\"\n    Evaluate the model during training. It computes the standard accuracy, certified accuracy, \n    and adversarial accuracy of the model. The results are saved to a specified path in JSON format.\n\n    Parameters:\n        model (torch.nn.Module): The model to be evaluated.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the dataset to evaluate.\n        eps (torch.Tensor): Perturbation radius used during evaluation of standard, certified and adversarial accuracy.\n        n_classes (int): Number of classes in the dataset.\n        device (str, optional): Device to run the evaluation on. Default is 'cuda'.\n        test_samples (int, optional): Number of samples to use for evaluation. Default is 1000.\n        verification_method (str, optional): Method to use for certification. Default is \"IBP\".\n        results_path (str, optional): Path to save the evaluation results. Default is \"./results\".\n\n    Returns:\n        tuple: A tuple containing standard accuracy, certified accuracy, and adversarial accuracy.\n    \"\"\"\n    os.makedirs(results_path, exist_ok=True)\n    model.eval()\n    std_acc = eval_acc(model, test_loader=data_loader, test_samples=test_samples)\n    if (eps == 0.).all():\n        cert_acc = adv_acc = std_acc\n    else:\n        with torch.no_grad():\n            cert_acc = eval_certified(model=model, data_loader=data_loader, n_classes=n_classes, eps=eps, test_samples=test_samples, method=verification_method)\n            adv_acc = eval_adversarial(model=model, data_loader=data_loader, n_classes=n_classes, eps=eps, device=device, test_samples=test_samples)\n\n    with open(f\"{results_path}/stats.json\", \"w\") as f:\n        json.dump(\n            {\"acc\": std_acc, \"cert_acc\": cert_acc, \"adv_acc\": adv_acc}, f\n        )\n    model.train()\n\n    return std_acc, cert_acc, adv_acc \n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_ibp","title":"<code>eval_ibp(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda')</code>","text":"<p>Evaluate a model using Interval Bound Propagation (IBP) for certification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The neural network model to be evaluated.</p> required <code>eps</code> <code>float</code> <p>The l_inf perturbation bound for for certification.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader providing the dataset to evaluate.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the classification task. Default is 10.</p> <code>10</code> <code>test_samples</code> <code>int</code> <p>Number of samples to test in order of the test loader. Default is np.inf (test all samples).</p> <code>inf</code> <code>device</code> <code>str</code> <p>Device to run the evaluation on ('cuda', 'mps', 'cpu'). Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the number of certified samples and the total number of images evaluated.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_ibp(model, eps, data_loader, n_classes=10, test_samples=np.inf, device='cuda'):\n    \"\"\"\n    Evaluate a model using Interval Bound Propagation (IBP) for certification.\n\n    Args:\n        model (auto_LiRPA.BoundedModule): The neural network model to be evaluated.\n        eps (float): The l_inf perturbation bound for for certification.\n        data_loader (torch.utils.data.DataLoader): DataLoader providing the dataset to evaluate.\n        n_classes (int, optional): Number of classes in the classification task. Default is 10.\n        test_samples (int, optional): Number of samples to test in order of the test loader. Default is np.inf (test all samples).\n        device (str, optional): Device to run the evaluation on ('cuda', 'mps', 'cpu'). Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the number of certified samples and the total number of images evaluated.\n    \"\"\"\n    certified = 0\n    total_images = 0\n    for batch_idx, (data, targets) in tqdm(enumerate(data_loader)):\n        if total_images &gt;= test_samples:\n            continue\n\n        ptb = PerturbationLpNorm(eps=eps, norm=np.inf, x_L=torch.clamp(data - eps, data_loader.min, data_loader.max).to(device), x_U=torch.clamp(data + eps, data_loader.min, data_loader.max).to(device))\n        data, targets = data.to(device), targets.to(device)\n\n        lb, ub = bound_ibp(\n            model=model,\n            ptb=ptb,\n            data=data,\n            target=targets,\n            n_classes=n_classes,\n            reuse_input=False\n        )\n        no_certified = torch.sum((lb &gt; 0).all(dim=1)).item()\n        # no_falsified = torch.sum((ub &lt; 0).any(dim=1)).item()\n        certified += no_certified\n\n        total_images += len(targets)\n\n    return certified, total_images\n</code></pre>"},{"location":"api/eval/eval/#CTRAIN.eval.eval.eval_model","title":"<code>eval_model(model, data_loader, eps, n_classes=10, test_samples=np.inf, method='ADAPTIVE', device='cuda')</code>","text":"<p>Evaluate the model on standard, certified, and adversarial accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The model to be evaluated.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the test dataset.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Default is 10.</p> <code>10</code> <code>eps</code> <code>float</code> <p>Perturbation size for adversarial and certified evaluation.</p> required <code>test_samples</code> <code>int or float</code> <p>Number of samples to test. Default is np.inf (all samples).</p> <code>inf</code> <code>method</code> <code>str or list</code> <p>The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', 'COMPLETE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'IBP'.</p> <code>'ADAPTIVE'</code> <code>device</code> <code>str</code> <p>Device to perform adversarial evaluation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing std_acc (float): Standard accuracy of the model, cert_acc (float): Certified accuracy of the model, adv_acc (float): Adversarial accuracy of the model.</p> Source code in <code>CTRAIN/eval/eval.py</code> <pre><code>def eval_model(model, data_loader, eps, n_classes=10, test_samples=np.inf, method='ADAPTIVE', device='cuda'):\n    \"\"\"\n    Evaluate the model on standard, certified, and adversarial accuracy.\n\n    Parameters:\n        model (auto_LiRPA.BoundedModule): The model to be evaluated.\n        data_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n        n_classes (int, optional): Number of classes in the dataset. Default is 10.\n        eps (float, optional): Perturbation size for adversarial and certified evaluation.\n        test_samples (int or float, optional): Number of samples to test. Default is np.inf (all samples).\n        method (str or list, optional): The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', 'COMPLETE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'IBP'.\n        device (str, optional): Device to perform adversarial evaluation on. Default is 'cuda'.\n\n    Returns:\n        tuple: A tuple containing std_acc (float): Standard accuracy of the model, cert_acc (float): Certified accuracy of the model, adv_acc (float): Adversarial accuracy of the model.\n    \"\"\"\n    std_acc = eval_acc(model, test_loader=data_loader, test_samples=test_samples)\n    cert_acc = eval_certified(model=model, data_loader=data_loader, n_classes=n_classes, eps=eps, test_samples=test_samples, method=method)\n    adv_acc = eval_adversarial(model=model, data_loader=data_loader, eps=eps, n_classes=n_classes, device=device, test_samples=test_samples)\n\n    return std_acc, cert_acc, adv_acc\n</code></pre>"},{"location":"api/model_definitions/model_definitions/","title":"model_definitions","text":""},{"location":"api/model_definitions/model_definitions/#CTRAIN.model_definitions.models_gowal.GowalConvLarge","title":"<code>GowalConvLarge</code>","text":"<p>               Bases: <code>Module</code></p> <p>A large convolutional neural network model based on the architecture proposed by Gowal et al.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple</code> <p>Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.</p> <code>(1, 28, 28)</code> <code>n_classes</code> <code>int</code> <p>Number of output classes. Default is 10.</p> <code>10</code> <code>dataset</code> <code>str</code> <p>The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.</p> <code>'mnist'</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>Sequential</code> <p>A sequential container of the layers in the network.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Defines the forward pass of the network.</p> Source code in <code>CTRAIN/model_definitions/models_gowal.py</code> <pre><code>class GowalConvLarge(nn.Module):\n    \"\"\"\n    A large convolutional neural network model based on the architecture proposed by Gowal et al.\n\n    Args:\n        in_shape (tuple): Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.\n        n_classes (int): Number of output classes. Default is 10.\n        dataset (str): The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.\n\n    Attributes:\n        layers (nn.Sequential): A sequential container of the layers in the network.\n\n    Methods:\n        forward(x):\n            Defines the forward pass of the network.\n    \"\"\"\n    def __init__(self, in_shape=(1, 28, 28), n_classes=10, dataset='mnist'):\n        super(GowalConvLarge, self).__init__()\n        in_channels = in_shape[0]\n        if dataset == 'mnist':\n            linear_in = 6272\n        elif dataset == 'cifar10':\n            linear_in = 10368\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=(3,3), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=1), \n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, kernel_size=(3,3), stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=(3,3), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Flatten(),\n            nn.Linear(linear_in, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre>"},{"location":"api/model_definitions/model_definitions/#CTRAIN.model_definitions.models_gowal.GowalConvMed","title":"<code>GowalConvMed</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolutional medium-sized neural network model based on the architecture proposed by Gowal et al.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple</code> <p>Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.</p> <code>(1, 28, 28)</code> <code>n_classes</code> <code>int</code> <p>Number of output classes. Default is 10.</p> <code>10</code> <code>dataset</code> <code>str</code> <p>The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.</p> <code>'mnist'</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>Sequential</code> <p>A sequential container of the layers in the network.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Defines the forward pass of the network.</p> Source code in <code>CTRAIN/model_definitions/models_gowal.py</code> <pre><code>class GowalConvMed(nn.Module):\n    \"\"\"\n    A convolutional medium-sized neural network model based on the architecture proposed by Gowal et al.\n\n    Args:\n        in_shape (tuple): Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.\n        n_classes (int): Number of output classes. Default is 10.\n        dataset (str): The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.\n\n    Attributes:\n        layers (nn.Sequential): A sequential container of the layers in the network.\n\n    Methods:\n        forward(x):\n            Defines the forward pass of the network.\n    \"\"\"\n    def __init__(self, in_shape=(1, 28, 28), n_classes=10, dataset='mnist'):\n        super(GowalConvMed, self).__init__()\n        in_channels = in_shape[0]\n        if dataset == 'mnist':\n            linear_in = 1024\n        elif dataset == 'cifar10':\n            linear_in = 1600\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=(3,3), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=(4, 4), stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=(4, 4), stride=2),\n            nn.ReLU(inplace=True),\n            nn.Flatten(),\n            nn.Linear(linear_in, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre>"},{"location":"api/model_definitions/model_definitions/#CTRAIN.model_definitions.models_gowal.GowalConvSmall","title":"<code>GowalConvSmall</code>","text":"<p>               Bases: <code>Module</code></p> <p>GowalConvSmall is a small convolutional neural network model designed for certified training, proposed by Gowal et al.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple</code> <p>Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.</p> <code>(1, 28, 28)</code> <code>n_classes</code> <code>int</code> <p>Number of output classes. Default is 10.</p> <code>10</code> <code>dataset</code> <code>str</code> <p>The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.</p> <code>'mnist'</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>Sequential</code> <p>A sequential container of the layers in the network.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Defines the forward pass of the network.</p> Source code in <code>CTRAIN/model_definitions/models_gowal.py</code> <pre><code>class GowalConvSmall(nn.Module):\n    \"\"\"\n    GowalConvSmall is a small convolutional neural network model designed for certified training, proposed by Gowal et al.\n\n    Args:\n        in_shape (tuple): Shape of the input tensor. Default is (1, 28, 28) for MNIST dataset.\n        n_classes (int): Number of output classes. Default is 10.\n        dataset (str): The dataset being used. Can be 'mnist' or 'cifar10'. Default is 'mnist'.\n\n    Attributes:\n        layers (nn.Sequential): A sequential container of the layers in the network.\n\n    Methods:\n        forward(x):\n            Defines the forward pass of the network.\n    \"\"\"\n    def __init__(self, in_shape=(1, 28, 28), n_classes=10, dataset='mnist'):\n        super(GowalConvSmall, self).__init__()\n        assert in_shape[1] == in_shape[2], \"We only support square inputs for now!\"\n        in_channels = in_shape[0]\n        in_dim  = in_shape[1]\n        if dataset == 'mnist':\n            linear_in = 3200\n        elif dataset == 'cifar10':\n            linear_in = 4608\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size=(4,4), stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(16, 32, kernel_size=(4, 4), stride=1),\n            nn.ReLU(inplace=True),\n            nn.Flatten(),\n            nn.Linear(linear_in, 100),\n            nn.ReLU(inplace=True),\n            nn.Linear(100, n_classes)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre>"},{"location":"api/model_definitions/model_definitions/#CTRAIN.model_definitions.models_shi.CNN7_Shi","title":"<code>CNN7_Shi</code>","text":"<p>               Bases: <code>Module</code></p> <p>CNN7_Shi is a convolutional neural network model designed for image classification tasks proposed by Shi et al. It is the defacto standard neural network architecture to evaluate certified training methods on.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple</code> <p>Shape of the input images. Default is (1, 28, 28).</p> <code>(1, 28, 28)</code> <code>width</code> <code>int</code> <p>Number of channels for the convolutional layers. Default is 64.</p> <code>64</code> <code>linear_size</code> <code>int</code> <p>Size of the fully connected layer. Default is 512.</p> <code>512</code> <code>n_classes</code> <code>int</code> <p>Number of output classes. Default is 10.</p> <code>10</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>Sequential</code> <p>Sequential container of the layers in the network.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Defines the forward pass of the network.</p> Source code in <code>CTRAIN/model_definitions/models_shi.py</code> <pre><code>class CNN7_Shi(nn.Module):\n    \"\"\"\n    CNN7_Shi is a convolutional neural network model designed for image classification tasks proposed by Shi et al.\n    It is the defacto standard neural network architecture to evaluate certified training methods on.\n\n    Args:\n        in_shape (tuple): Shape of the input images. Default is (1, 28, 28).\n        width (int): Number of channels for the convolutional layers. Default is 64.\n        linear_size (int): Size of the fully connected layer. Default is 512.\n        n_classes (int): Number of output classes. Default is 10.\n\n    Attributes:\n        layers (nn.Sequential): Sequential container of the layers in the network.\n\n    Methods:\n        forward(x):\n            Defines the forward pass of the network.\n    \"\"\"\n    def __init__(self, in_shape=(1, 28, 28), width=64, linear_size=512, n_classes=10):\n        super(CNN7_Shi, self).__init__()\n        assert in_shape[1] == in_shape[2], \"We only support square inputs for now!\"\n        in_channels = in_shape[0]\n        in_dim  = in_shape[1]\n\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, width, kernel_size=(3,3), stride=1, padding=1),\n            nn.BatchNorm2d(width),\n            nn.ReLU(),\n            nn.Conv2d(width, width, kernel_size=(3,3), stride=1, padding=1),\n            nn.BatchNorm2d(width),\n            nn.ReLU(),\n            nn.Conv2d(width, 2 * width, kernel_size=(3,3), stride=2, padding=1),\n            nn.BatchNorm2d(2 * width),\n            nn.ReLU(),\n            nn.Conv2d(2 * width, 2 * width, kernel_size=(3,3), stride=1, padding=1),\n            nn.BatchNorm2d(2 * width),\n            nn.ReLU(),\n            nn.Conv2d(2 * width, 2 * width, kernel_size=(3,3), stride=1, padding=1),\n            nn.BatchNorm2d(2 * width),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear((in_dim//2) * (in_dim//2) * 2 * width, linear_size),\n            nn.BatchNorm1d(linear_size),\n            nn.ReLU(),\n            nn.Linear(linear_size, n_classes)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre>"},{"location":"api/model_wrappers/crown_ibp_model_wrapper/","title":"CROWN IBP","text":""},{"location":"api/model_wrappers/crown_ibp_model_wrapper/#CTRAIN.model_wrappers.crown_ibp_model_wrapper.CrownIBPModelWrapper","title":"<code>CrownIBPModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using CROWN-IBP method. For details, see Zhang et al. (2020) \"Towards Stable and Efficient Training of Verifiably Robust Neural Networks\". https://arxiv.org/pdf/1906.06316</p> Source code in <code>CTRAIN/model_wrappers/crown_ibp_model_wrapper.py</code> <pre><code>class CrownIBPModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using CROWN-IBP method. For details, see Zhang et al. (2020) \"Towards Stable and Efficient Training of Verifiably Robust Neural Networks\". https://arxiv.org/pdf/1906.06316\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, start_beta=1, end_beta=0,\n                 checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n        \"\"\"\n        Initializes the CrownIBPModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): Shi regularization weight.\n            shi_reg_decay (bool): Whether to decay Shi regularization during the ramp up phase.\n            start_kappa (float): Starting value of kappa that trades-off IBP and clean loss.\n            end_kappa (float): Ending value of kappa.\n            start_beta (float): Starting value of beta that trades off IBP and CROWN-IBP loss.\n            end_beta (float): Ending value of beta.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'crown_ibp'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.start_kappa = start_kappa\n        self.end_kappa = end_kappa\n        self.start_beta = start_beta\n        self.end_beta = end_beta\n        self.optimizer_func = optimizer_func\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n        \"\"\"\n        Trains the model using the CROWN-IBP method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n            start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        trained_model = crown_ibp_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            start_epoch=start_epoch,\n            end_epoch=end_epoch,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            eps_scheduler_args={'start_kappa': self.start_kappa, 'end_kappa': self.end_kappa, 'start_beta': self.start_beta, 'end_beta': self.end_beta},\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        model_wrapper = CrownIBPModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            start_kappa=config['crown_ibp:start_kappa'],\n            end_kappa=config['crown_ibp:end_kappa'] * config['crown_ibp:start_kappa'],\n            start_beta=config['crown_ibp:start_beta'],\n            end_beta=config['crown_ibp:end_beta'],\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/crown_ibp_model_wrapper/#CTRAIN.model_wrappers.crown_ibp_model_wrapper.CrownIBPModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, start_beta=1, end_beta=0, checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the CrownIBPModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>Shi regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay Shi regularization during the ramp up phase.</p> <code>True</code> <code>start_kappa</code> <code>float</code> <p>Starting value of kappa that trades-off IBP and clean loss.</p> <code>1</code> <code>end_kappa</code> <code>float</code> <p>Ending value of kappa.</p> <code>0</code> <code>start_beta</code> <code>float</code> <p>Starting value of beta that trades off IBP and CROWN-IBP loss.</p> <code>1</code> <code>end_beta</code> <code>float</code> <p>Ending value of beta.</p> <code>0</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/crown_ibp_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, start_beta=1, end_beta=0,\n             checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n    \"\"\"\n    Initializes the CrownIBPModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): Shi regularization weight.\n        shi_reg_decay (bool): Whether to decay Shi regularization during the ramp up phase.\n        start_kappa (float): Starting value of kappa that trades-off IBP and clean loss.\n        end_kappa (float): Ending value of kappa.\n        start_beta (float): Starting value of beta that trades off IBP and CROWN-IBP loss.\n        end_beta (float): Ending value of beta.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'crown_ibp'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.start_kappa = start_kappa\n    self.end_kappa = end_kappa\n    self.start_beta = start_beta\n    self.end_beta = end_beta\n    self.optimizer_func = optimizer_func\n</code></pre>"},{"location":"api/model_wrappers/crown_ibp_model_wrapper/#CTRAIN.model_wrappers.crown_ibp_model_wrapper.CrownIBPModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/crown_ibp_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    model_wrapper = CrownIBPModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        start_kappa=config['crown_ibp:start_kappa'],\n        end_kappa=config['crown_ibp:end_kappa'] * config['crown_ibp:start_kappa'],\n        start_beta=config['crown_ibp:start_beta'],\n        end_beta=config['crown_ibp:end_beta'],\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/crown_ibp_model_wrapper/#CTRAIN.model_wrappers.crown_ibp_model_wrapper.CrownIBPModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0, end_epoch=None)</code>","text":"<p>Trains the model using the CROWN-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/crown_ibp_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n    \"\"\"\n    Trains the model using the CROWN-IBP method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n        start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    trained_model = crown_ibp_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        start_epoch=start_epoch,\n        end_epoch=end_epoch,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        eps_scheduler_args={'start_kappa': self.start_kappa, 'end_kappa': self.end_kappa, 'start_beta': self.start_beta, 'end_beta': self.end_beta},\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/","title":"CTRAIN","text":""},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper","title":"<code>CTRAINWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper base class for certifiably training models.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>class CTRAINWrapper(nn.Module):\n    \"\"\"\n    Wrapper base class for certifiably training models.\n    \"\"\"\n    def __init__(self, model: nn.Module, eps:float, input_shape: tuple, train_eps_factor=1, lr=0.0005, optimizer_func=torch.optim.Adam, bound_opts=dict(conv_mode='patches', relu='adaptive'), device='cuda', checkpoint_save_path=None, checkpoint_save_interval=10):\n        \"\"\"\n        Initialize the CTRAINWrapper Base Class.\n\n        Args:\n            model (nn.Module): The neural network model to be wrapped.\n            eps (float): The epsilon value for training.\n            input_shape (tuple): The shape of the input tensor.\n            train_eps_factor (float, optional): Factor to scale epsilon during training. Default is 1.\n            lr (float, optional): Learning rate for the optimizer. Default is 0.0005.\n            optimizer_func (torch.optim.Optimizer, optional): The optimizer function to use. Default is torch.optim.Adam.\n            bound_opts (dict, optional): Options for bounding the model. Default is {'conv_mode': 'patches', 'relu': 'adaptive'}.\n            device (str or torch.device, optional): The device to run the model on. Default is 'cuda'.\n            checkpoint_save_path (str, optional): Path to save checkpoints. Default is None.\n            checkpoint_save_interval (int, optional): Interval to save checkpoints. Default is 10.\n\n        Attributes:\n            original_model (nn.Module): The original neural network model.\n            eps (float): The epsilon value for training.\n            train_eps (float): The scaled epsilon value for training.\n            device (torch.device): The device to run the model on.\n            n_classes (int): The number of classes in the model's output.\n            bound_opts (dict): Options for bounding the model.\n            bounded_model (BoundedModule): The bounded version of the original model.\n            input_shape (tuple): The shape of the input tensor.\n            optimizer_func (torch.optim.Optimizer): The optimizer function.\n            optimizer (torch.optim.Optimizer): The optimizer instance.\n            epoch (int): The current epoch number.\n            checkpoint_path (str): Path to save checkpoints.\n        \"\"\"\n        super(CTRAINWrapper, self).__init__()\n        model = model.to(device)\n\n        original_train = model.training\n        self.original_model = model\n        self.eps = eps\n        self.train_eps = eps * train_eps_factor\n        if isinstance(device, torch.device):\n            self.device = device\n        else:\n            if device in ['cuda', 'cpu', 'mps']:\n                self.device = torch.device(device)\n            else:\n                print(\"Unknown device - falling back to device CPU!\")\n                self.device = torch.device('cpu')\n\n        if len(input_shape) &lt; 4:\n            input_shape = [1, *input_shape]\n        model.eval()\n        example_input = torch.ones(input_shape, device=device)\n        self.n_classes = len(model(example_input)[0])\n        self.bound_opts = bound_opts\n        self.bounded_model = BoundedModule(model=self.original_model, global_input=example_input, bound_opts=bound_opts, device=device)\n        self.input_shape = input_shape\n\n        self.optimizer_func = optimizer_func\n        self.optimizer = optimizer_func(self.bounded_model.parameters(), lr=lr)\n\n        self.epoch = 0\n\n        if original_train:\n            self.original_model.train()\n            self.bounded_model.train()\n\n        self.checkpoint_path = checkpoint_save_path\n        if checkpoint_save_path is not None:\n            os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_save_interval = checkpoint_save_interval\n\n    def train(self):\n        \"\"\"\n        Sets wrapper into training mode.\n\n        This method calls the `train` method on both the `original_model` and \n        the `bounded_model` to set them into training mode\n        \"\"\"\n        self.original_model.train()\n        self.bounded_model.train()\n\n    def eval(self):\n        \"\"\"\n        Sets the model to evaluation mode.\n\n        This method sets both the original model and the bounded model to evaluation mode.\n        In evaluation mode, certain layers like dropout and batch normalization behave differently\n        compared to training mode, typically affecting the model's performance and predictions.\n        \"\"\"\n        self.original_model.eval()\n        self.bounded_model.eval()\n\n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass through the LiRPA model.\n\n        Args:\n            x (torch.Tensor): Input tensor to be passed through the model.\n\n        Returns:\n            torch.Tensor: Output tensor after passing through the bounded model.\n        \"\"\"\n        return self.bounded_model(x)\n\n    def evaluate(self, test_loader, test_samples=np.inf, eval_method='ADAPTIVE'):\n        \"\"\"\n        Evaluate the model using the provided test data loader.\n\n        Args:\n            test_loader (DataLoader): DataLoader containing the test dataset.\n            test_samples (int, optional): Number of test samples to evaluate. Defaults to np.inf.\n            eval_method (str or list, optional): The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'ADAPTIVE'.\n\n        Returns:\n            (Tuple): Evaluation results in terms of std_acc, cert_acc and adv_acc.\n        \"\"\"\n        eps_std = self.eps / test_loader.std if test_loader.normalised else torch.tensor(self.eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        return eval_model(self.bounded_model, test_loader, n_classes=self.n_classes, eps=eps_std, test_samples=test_samples, method=eval_method, device=self.device)\n\n    def evaluate_complete(self, test_loader, test_samples=np.inf, timeout=1000, no_cores=4, abcrown_batch_size=512):\n        \"\"\"\n        Evaluate the model using the complete verification tool abCROWN.\n\n        Args:\n            test_loader (DataLoader): DataLoader for the test set.\n            test_samples (int, optional): Number of test samples to evaluate. Defaults to np.inf.\n            timeout (int, optional): Per-instance timeout for the verification process in seconds. Defaults to 1000.\n            no_cores (int, optional): Number of CPU cores to use for verification. Only relevant, if MIP refinement is used in abCROWN. Defaults to 4.\n            abcrown_batch_size (int, optional): Batch size for abCROWN. Defaults to 512. Decrease, if you run out of memory.\n\n        Returns:\n            (tuple): A tuple containing: std_acc (float): Standard accuracy of the model on the test set, certified_acc (float): Certified accuracy of the model on the test set and adv_acc (float): Adversarial accuracy of the model on the test set.\n        \"\"\"\n        eps_std = self.eps / test_loader.std if test_loader.normalised else self.eps\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        std_acc = eval_acc(self.bounded_model, test_loader=test_loader, test_samples=test_samples)\n        certified_acc, adv_acc = eval_complete_abcrown(\n            model=self.bounded_model,\n            eps_std=eps_std,\n            data_loader=test_loader,\n            n_classes=self.n_classes,\n            input_shape=self.input_shape,\n            test_samples=test_samples,\n            timeout=timeout,\n            no_cores=no_cores,\n            abcrown_batch_size=abcrown_batch_size,\n            device=self.device\n        )\n        return std_acc, certified_acc, adv_acc\n\n    def state_dict(self):\n        \"\"\"\n        Returns the state dictionary of the LiRPA model.\n\n        The state dictionary contains the model parameters and persistent buffers.\n\n        Returns:\n            dict: A dictionary containing the model's state.\n        \"\"\"\n        return self.bounded_model.state_dict()\n\n    def load_state_dict(self, state_dict, strict = True):\n        \"\"\"\n        Load the state dictionary into the bounded LiRPA model.\n\n        Args:\n            state_dict (dict): A dictionary containing model state parameters.\n            strict (bool, optional): Whether to strictly enforce that the keys \n                                     in `state_dict` match the keys returned by \n                                     the model's `state_dict()` function. \n                                     Defaults to True.\n\n        Returns:\n            (NamedTuple): A named tuple with fields `missing_keys` and `unexpected_keys`.\n                        `missing_keys` is a list of str containing the missing keys.\n                        `unexpected_keys` is a list of str containing the unexpected keys.\n        \"\"\"\n        return self.bounded_model.load_state_dict(state_dict, strict)\n\n    def parameters(self, recurse=True):\n        return self.bounded_model.parameters(recurse=recurse)\n    # TODO: Add onnx export/loading\n\n    def resume_from_checkpoint(self, checkpoint_path:str, train_loader, val_loader=None, end_epoch=None):\n        \"\"\"\n        Resume training from a given checkpoint.\n\n        Args:\n            checkpoint_path (str): Path to the checkpoint file.\n            train_loader (DataLoader): DataLoader for the training dataset.\n            val_loader (DataLoader, optional): DataLoader for the validation dataset. Defaults to None.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Loads the model and optimizer state from the checkpoint, sets the starting epoch, \n        and resumes training from that epoch.\n        \"\"\"\n        checkpoint = torch.load(checkpoint_path)\n        model_state_dict = checkpoint['model_state_dict']\n        self.load_state_dict(model_state_dict)\n        self.epoch = checkpoint['epoch']\n        optimizer_state_dict = checkpoint['optimizer_state_dict']\n        self.optimizer.load_state_dict(optimizer_state_dict)\n\n        self.train_model(train_loader, val_loader, start_epoch=self.epoch, end_epoch=end_epoch)\n\n    def hpo(self, train_loader, val_loader, budget=5*24*60*60, defaults=dict(), eval_samples=1000, output_dir='./smac_hpo', include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Perform hyperparameter optimization (HPO) using SMAC3 for the model. After the method returns, the model will have loaded the best hyperparameters found during the optimization and the according trained weights.\n\n        Args:\n            train_loader (DataLoader): DataLoader for the training dataset.\n            val_loader (DataLoader): DataLoader for the validation dataset.\n            budget (int, optional): Time budget for the HPO process in seconds. Default is 5 days (5*24*60*60).\n            defaults (dict, optional): Default hyperparameter values. Default is an empty dictionary.\n            eval_samples (int, optional): Number of samples to use for loss computation. Default is 1000.\n            output_dir (str, optional): Directory to store HPO results. Default is './smac_hpo'.\n            include_nat_loss (bool, optional): Whether to include natural loss in the optimization. Default is True.\n            include_adv_loss (bool, optional): Whether to include adversarial loss in the optimization. Default is True.\n            include_cert_loss (bool, optional): Whether to include certified loss in the optimization. Default is True.\n\n        Returns:\n            Configuration: The best hyperparameter configuration found during the optimization.\n        \"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        if os.listdir(output_dir):\n            assert False, 'Output directory for HPO is not empty!'\n\n        os.makedirs(f'{output_dir}/nets', exist_ok=True)\n        os.makedirs(f'{output_dir}/smac/', exist_ok=True)\n\n        eps_std = self.eps / train_loader.std\n        scenario = Scenario(\n            configspace=get_config_space(self, self.num_epochs, eps_std, defaults=defaults),\n            walltime_limit=budget,\n            n_trials=np.inf,\n            output_directory=f'{output_dir}/smac/',\n            use_default_config=True if len(defaults.values()) &gt; 0 else False\n        )\n        initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs_per_hyperparamter=1)\n        smac = HyperparameterOptimizationFacade(\n            scenario,\n            partial(self._hpo_runner, epochs=self.num_epochs, train_loader=train_loader, val_loader=val_loader, cert_eval_samples=eval_samples, output_dir=output_dir, include_nat_loss=include_nat_loss, include_adv_loss=include_adv_loss, include_cert_loss=include_cert_loss),\n            initial_design=initial_design,\n            overwrite=True,\n        )\n\n        inc = smac.optimize()\n\n        config_hash = get_config_hash(inc, 32)\n        self.load_state_dict(torch.load(f'{output_dir}/nets/{config_hash}.pt'))\n\n        return inc\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000):\n        raise NotImplementedError('HPO can only be run on the concrete Wrappers!')\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.__init__","title":"<code>__init__(model, eps, input_shape, train_eps_factor=1, lr=0.0005, optimizer_func=torch.optim.Adam, bound_opts=dict(conv_mode='patches', relu='adaptive'), device='cuda', checkpoint_save_path=None, checkpoint_save_interval=10)</code>","text":"<p>Initialize the CTRAINWrapper Base Class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The neural network model to be wrapped.</p> required <code>eps</code> <code>float</code> <p>The epsilon value for training.</p> required <code>input_shape</code> <code>tuple</code> <p>The shape of the input tensor.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor to scale epsilon during training. Default is 1.</p> <code>1</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Default is 0.0005.</p> <code>0.0005</code> <code>optimizer_func</code> <code>Optimizer</code> <p>The optimizer function to use. Default is torch.optim.Adam.</p> <code>Adam</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding the model. Default is {'conv_mode': 'patches', 'relu': 'adaptive'}.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>str or device</code> <p>The device to run the model on. Default is 'cuda'.</p> <code>'cuda'</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints. Default is None.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval to save checkpoints. Default is 10.</p> <code>10</code> <p>Attributes:</p> Name Type Description <code>original_model</code> <code>Module</code> <p>The original neural network model.</p> <code>eps</code> <code>float</code> <p>The epsilon value for training.</p> <code>train_eps</code> <code>float</code> <p>The scaled epsilon value for training.</p> <code>device</code> <code>device</code> <p>The device to run the model on.</p> <code>n_classes</code> <code>int</code> <p>The number of classes in the model's output.</p> <code>bound_opts</code> <code>dict</code> <p>Options for bounding the model.</p> <code>bounded_model</code> <code>BoundedModule</code> <p>The bounded version of the original model.</p> <code>input_shape</code> <code>tuple</code> <p>The shape of the input tensor.</p> <code>optimizer_func</code> <code>Optimizer</code> <p>The optimizer function.</p> <code>optimizer</code> <code>Optimizer</code> <p>The optimizer instance.</p> <code>epoch</code> <code>int</code> <p>The current epoch number.</p> <code>checkpoint_path</code> <code>str</code> <p>Path to save checkpoints.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def __init__(self, model: nn.Module, eps:float, input_shape: tuple, train_eps_factor=1, lr=0.0005, optimizer_func=torch.optim.Adam, bound_opts=dict(conv_mode='patches', relu='adaptive'), device='cuda', checkpoint_save_path=None, checkpoint_save_interval=10):\n    \"\"\"\n    Initialize the CTRAINWrapper Base Class.\n\n    Args:\n        model (nn.Module): The neural network model to be wrapped.\n        eps (float): The epsilon value for training.\n        input_shape (tuple): The shape of the input tensor.\n        train_eps_factor (float, optional): Factor to scale epsilon during training. Default is 1.\n        lr (float, optional): Learning rate for the optimizer. Default is 0.0005.\n        optimizer_func (torch.optim.Optimizer, optional): The optimizer function to use. Default is torch.optim.Adam.\n        bound_opts (dict, optional): Options for bounding the model. Default is {'conv_mode': 'patches', 'relu': 'adaptive'}.\n        device (str or torch.device, optional): The device to run the model on. Default is 'cuda'.\n        checkpoint_save_path (str, optional): Path to save checkpoints. Default is None.\n        checkpoint_save_interval (int, optional): Interval to save checkpoints. Default is 10.\n\n    Attributes:\n        original_model (nn.Module): The original neural network model.\n        eps (float): The epsilon value for training.\n        train_eps (float): The scaled epsilon value for training.\n        device (torch.device): The device to run the model on.\n        n_classes (int): The number of classes in the model's output.\n        bound_opts (dict): Options for bounding the model.\n        bounded_model (BoundedModule): The bounded version of the original model.\n        input_shape (tuple): The shape of the input tensor.\n        optimizer_func (torch.optim.Optimizer): The optimizer function.\n        optimizer (torch.optim.Optimizer): The optimizer instance.\n        epoch (int): The current epoch number.\n        checkpoint_path (str): Path to save checkpoints.\n    \"\"\"\n    super(CTRAINWrapper, self).__init__()\n    model = model.to(device)\n\n    original_train = model.training\n    self.original_model = model\n    self.eps = eps\n    self.train_eps = eps * train_eps_factor\n    if isinstance(device, torch.device):\n        self.device = device\n    else:\n        if device in ['cuda', 'cpu', 'mps']:\n            self.device = torch.device(device)\n        else:\n            print(\"Unknown device - falling back to device CPU!\")\n            self.device = torch.device('cpu')\n\n    if len(input_shape) &lt; 4:\n        input_shape = [1, *input_shape]\n    model.eval()\n    example_input = torch.ones(input_shape, device=device)\n    self.n_classes = len(model(example_input)[0])\n    self.bound_opts = bound_opts\n    self.bounded_model = BoundedModule(model=self.original_model, global_input=example_input, bound_opts=bound_opts, device=device)\n    self.input_shape = input_shape\n\n    self.optimizer_func = optimizer_func\n    self.optimizer = optimizer_func(self.bounded_model.parameters(), lr=lr)\n\n    self.epoch = 0\n\n    if original_train:\n        self.original_model.train()\n        self.bounded_model.train()\n\n    self.checkpoint_path = checkpoint_save_path\n    if checkpoint_save_path is not None:\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n    self.checkpoint_save_interval = checkpoint_save_interval\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.eval","title":"<code>eval()</code>","text":"<p>Sets the model to evaluation mode.</p> <p>This method sets both the original model and the bounded model to evaluation mode. In evaluation mode, certain layers like dropout and batch normalization behave differently compared to training mode, typically affecting the model's performance and predictions.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Sets the model to evaluation mode.\n\n    This method sets both the original model and the bounded model to evaluation mode.\n    In evaluation mode, certain layers like dropout and batch normalization behave differently\n    compared to training mode, typically affecting the model's performance and predictions.\n    \"\"\"\n    self.original_model.eval()\n    self.bounded_model.eval()\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.evaluate","title":"<code>evaluate(test_loader, test_samples=np.inf, eval_method='ADAPTIVE')</code>","text":"<p>Evaluate the model using the provided test data loader.</p> <p>Parameters:</p> Name Type Description Default <code>test_loader</code> <code>DataLoader</code> <p>DataLoader containing the test dataset.</p> required <code>test_samples</code> <code>int</code> <p>Number of test samples to evaluate. Defaults to np.inf.</p> <code>inf</code> <code>eval_method</code> <code>str or list</code> <p>The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'ADAPTIVE'.</p> <code>'ADAPTIVE'</code> <p>Returns:</p> Type Description <code>Tuple</code> <p>Evaluation results in terms of std_acc, cert_acc and adv_acc.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def evaluate(self, test_loader, test_samples=np.inf, eval_method='ADAPTIVE'):\n    \"\"\"\n    Evaluate the model using the provided test data loader.\n\n    Args:\n        test_loader (DataLoader): DataLoader containing the test dataset.\n        test_samples (int, optional): Number of test samples to evaluate. Defaults to np.inf.\n        eval_method (str or list, optional): The certification method to use. Options are 'IBP', 'CROWN', 'CROWN-IBP', 'ADAPTIVE', or a list of methods (which results in an ADAPTIVE evaluation using these methods). Default is 'ADAPTIVE'.\n\n    Returns:\n        (Tuple): Evaluation results in terms of std_acc, cert_acc and adv_acc.\n    \"\"\"\n    eps_std = self.eps / test_loader.std if test_loader.normalised else torch.tensor(self.eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    return eval_model(self.bounded_model, test_loader, n_classes=self.n_classes, eps=eps_std, test_samples=test_samples, method=eval_method, device=self.device)\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.evaluate_complete","title":"<code>evaluate_complete(test_loader, test_samples=np.inf, timeout=1000, no_cores=4, abcrown_batch_size=512)</code>","text":"<p>Evaluate the model using the complete verification tool abCROWN.</p> <p>Parameters:</p> Name Type Description Default <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for the test set.</p> required <code>test_samples</code> <code>int</code> <p>Number of test samples to evaluate. Defaults to np.inf.</p> <code>inf</code> <code>timeout</code> <code>int</code> <p>Per-instance timeout for the verification process in seconds. Defaults to 1000.</p> <code>1000</code> <code>no_cores</code> <code>int</code> <p>Number of CPU cores to use for verification. Only relevant, if MIP refinement is used in abCROWN. Defaults to 4.</p> <code>4</code> <code>abcrown_batch_size</code> <code>int</code> <p>Batch size for abCROWN. Defaults to 512. Decrease, if you run out of memory.</p> <code>512</code> <p>Returns:</p> Type Description <code>tuple): A tuple containing: std_acc (float): Standard accuracy of the model on the test set, certified_acc (float): Certified accuracy of the model on the test set and adv_acc (float</code> <p>Adversarial accuracy of the model on the test set.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def evaluate_complete(self, test_loader, test_samples=np.inf, timeout=1000, no_cores=4, abcrown_batch_size=512):\n    \"\"\"\n    Evaluate the model using the complete verification tool abCROWN.\n\n    Args:\n        test_loader (DataLoader): DataLoader for the test set.\n        test_samples (int, optional): Number of test samples to evaluate. Defaults to np.inf.\n        timeout (int, optional): Per-instance timeout for the verification process in seconds. Defaults to 1000.\n        no_cores (int, optional): Number of CPU cores to use for verification. Only relevant, if MIP refinement is used in abCROWN. Defaults to 4.\n        abcrown_batch_size (int, optional): Batch size for abCROWN. Defaults to 512. Decrease, if you run out of memory.\n\n    Returns:\n        (tuple): A tuple containing: std_acc (float): Standard accuracy of the model on the test set, certified_acc (float): Certified accuracy of the model on the test set and adv_acc (float): Adversarial accuracy of the model on the test set.\n    \"\"\"\n    eps_std = self.eps / test_loader.std if test_loader.normalised else self.eps\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    std_acc = eval_acc(self.bounded_model, test_loader=test_loader, test_samples=test_samples)\n    certified_acc, adv_acc = eval_complete_abcrown(\n        model=self.bounded_model,\n        eps_std=eps_std,\n        data_loader=test_loader,\n        n_classes=self.n_classes,\n        input_shape=self.input_shape,\n        test_samples=test_samples,\n        timeout=timeout,\n        no_cores=no_cores,\n        abcrown_batch_size=abcrown_batch_size,\n        device=self.device\n    )\n    return std_acc, certified_acc, adv_acc\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.forward","title":"<code>forward(x)</code>","text":"<p>Perform a forward pass through the LiRPA model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to be passed through the model.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor after passing through the bounded model.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Perform a forward pass through the LiRPA model.\n\n    Args:\n        x (torch.Tensor): Input tensor to be passed through the model.\n\n    Returns:\n        torch.Tensor: Output tensor after passing through the bounded model.\n    \"\"\"\n    return self.bounded_model(x)\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.hpo","title":"<code>hpo(train_loader, val_loader, budget=5 * 24 * 60 * 60, defaults=dict(), eval_samples=1000, output_dir='./smac_hpo', include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Perform hyperparameter optimization (HPO) using SMAC3 for the model. After the method returns, the model will have loaded the best hyperparameters found during the optimization and the according trained weights.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training dataset.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation dataset.</p> required <code>budget</code> <code>int</code> <p>Time budget for the HPO process in seconds. Default is 5 days (52460*60).</p> <code>5 * 24 * 60 * 60</code> <code>defaults</code> <code>dict</code> <p>Default hyperparameter values. Default is an empty dictionary.</p> <code>dict()</code> <code>eval_samples</code> <code>int</code> <p>Number of samples to use for loss computation. Default is 1000.</p> <code>1000</code> <code>output_dir</code> <code>str</code> <p>Directory to store HPO results. Default is './smac_hpo'.</p> <code>'./smac_hpo'</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss in the optimization. Default is True.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss in the optimization. Default is True.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certified loss in the optimization. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Configuration</code> <p>The best hyperparameter configuration found during the optimization.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def hpo(self, train_loader, val_loader, budget=5*24*60*60, defaults=dict(), eval_samples=1000, output_dir='./smac_hpo', include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Perform hyperparameter optimization (HPO) using SMAC3 for the model. After the method returns, the model will have loaded the best hyperparameters found during the optimization and the according trained weights.\n\n    Args:\n        train_loader (DataLoader): DataLoader for the training dataset.\n        val_loader (DataLoader): DataLoader for the validation dataset.\n        budget (int, optional): Time budget for the HPO process in seconds. Default is 5 days (5*24*60*60).\n        defaults (dict, optional): Default hyperparameter values. Default is an empty dictionary.\n        eval_samples (int, optional): Number of samples to use for loss computation. Default is 1000.\n        output_dir (str, optional): Directory to store HPO results. Default is './smac_hpo'.\n        include_nat_loss (bool, optional): Whether to include natural loss in the optimization. Default is True.\n        include_adv_loss (bool, optional): Whether to include adversarial loss in the optimization. Default is True.\n        include_cert_loss (bool, optional): Whether to include certified loss in the optimization. Default is True.\n\n    Returns:\n        Configuration: The best hyperparameter configuration found during the optimization.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    if os.listdir(output_dir):\n        assert False, 'Output directory for HPO is not empty!'\n\n    os.makedirs(f'{output_dir}/nets', exist_ok=True)\n    os.makedirs(f'{output_dir}/smac/', exist_ok=True)\n\n    eps_std = self.eps / train_loader.std\n    scenario = Scenario(\n        configspace=get_config_space(self, self.num_epochs, eps_std, defaults=defaults),\n        walltime_limit=budget,\n        n_trials=np.inf,\n        output_directory=f'{output_dir}/smac/',\n        use_default_config=True if len(defaults.values()) &gt; 0 else False\n    )\n    initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs_per_hyperparamter=1)\n    smac = HyperparameterOptimizationFacade(\n        scenario,\n        partial(self._hpo_runner, epochs=self.num_epochs, train_loader=train_loader, val_loader=val_loader, cert_eval_samples=eval_samples, output_dir=output_dir, include_nat_loss=include_nat_loss, include_adv_loss=include_adv_loss, include_cert_loss=include_cert_loss),\n        initial_design=initial_design,\n        overwrite=True,\n    )\n\n    inc = smac.optimize()\n\n    config_hash = get_config_hash(inc, 32)\n    self.load_state_dict(torch.load(f'{output_dir}/nets/{config_hash}.pt'))\n\n    return inc\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.load_state_dict","title":"<code>load_state_dict(state_dict, strict=True)</code>","text":"<p>Load the state dictionary into the bounded LiRPA model.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>A dictionary containing model state parameters.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys                       in <code>state_dict</code> match the keys returned by                       the model's <code>state_dict()</code> function.                       Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NamedTuple</code> <p>A named tuple with fields <code>missing_keys</code> and <code>unexpected_keys</code>.         <code>missing_keys</code> is a list of str containing the missing keys.         <code>unexpected_keys</code> is a list of str containing the unexpected keys.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def load_state_dict(self, state_dict, strict = True):\n    \"\"\"\n    Load the state dictionary into the bounded LiRPA model.\n\n    Args:\n        state_dict (dict): A dictionary containing model state parameters.\n        strict (bool, optional): Whether to strictly enforce that the keys \n                                 in `state_dict` match the keys returned by \n                                 the model's `state_dict()` function. \n                                 Defaults to True.\n\n    Returns:\n        (NamedTuple): A named tuple with fields `missing_keys` and `unexpected_keys`.\n                    `missing_keys` is a list of str containing the missing keys.\n                    `unexpected_keys` is a list of str containing the unexpected keys.\n    \"\"\"\n    return self.bounded_model.load_state_dict(state_dict, strict)\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.resume_from_checkpoint","title":"<code>resume_from_checkpoint(checkpoint_path, train_loader, val_loader=None, end_epoch=None)</code>","text":"<p>Resume training from a given checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to the checkpoint file.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training dataset.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation dataset. Defaults to None.</p> <code>None</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Loads the model and optimizer state from the checkpoint, sets the starting epoch,  and resumes training from that epoch.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def resume_from_checkpoint(self, checkpoint_path:str, train_loader, val_loader=None, end_epoch=None):\n    \"\"\"\n    Resume training from a given checkpoint.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint file.\n        train_loader (DataLoader): DataLoader for the training dataset.\n        val_loader (DataLoader, optional): DataLoader for the validation dataset. Defaults to None.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Loads the model and optimizer state from the checkpoint, sets the starting epoch, \n    and resumes training from that epoch.\n    \"\"\"\n    checkpoint = torch.load(checkpoint_path)\n    model_state_dict = checkpoint['model_state_dict']\n    self.load_state_dict(model_state_dict)\n    self.epoch = checkpoint['epoch']\n    optimizer_state_dict = checkpoint['optimizer_state_dict']\n    self.optimizer.load_state_dict(optimizer_state_dict)\n\n    self.train_model(train_loader, val_loader, start_epoch=self.epoch, end_epoch=end_epoch)\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the state dictionary of the LiRPA model.</p> <p>The state dictionary contains the model parameters and persistent buffers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the model's state.</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def state_dict(self):\n    \"\"\"\n    Returns the state dictionary of the LiRPA model.\n\n    The state dictionary contains the model parameters and persistent buffers.\n\n    Returns:\n        dict: A dictionary containing the model's state.\n    \"\"\"\n    return self.bounded_model.state_dict()\n</code></pre>"},{"location":"api/model_wrappers/ctrain_wrapper/#CTRAIN.model_wrappers.model_wrapper.CTRAINWrapper.train","title":"<code>train()</code>","text":"<p>Sets wrapper into training mode.</p> <p>This method calls the <code>train</code> method on both the <code>original_model</code> and  the <code>bounded_model</code> to set them into training mode</p> Source code in <code>CTRAIN/model_wrappers/model_wrapper.py</code> <pre><code>def train(self):\n    \"\"\"\n    Sets wrapper into training mode.\n\n    This method calls the `train` method on both the `original_model` and \n    the `bounded_model` to set them into training mode\n    \"\"\"\n    self.original_model.train()\n    self.bounded_model.train()\n</code></pre>"},{"location":"api/model_wrappers/mtl_ibp_model_wrapper/","title":"MTL IBP","text":""},{"location":"api/model_wrappers/mtl_ibp_model_wrapper/#CTRAIN.model_wrappers.mtl_ibp_model_wrapper.MTLIBPModelWrapper","title":"<code>MTLIBPModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using MTL-IBP method. For details, see De Palma et al. (2024) Expressive Losses for Verified Robustness via Convex Combinations. https://arxiv.org/pdf/2305.13991</p> Source code in <code>CTRAIN/model_wrappers/mtl_ibp_model_wrapper.py</code> <pre><code>class MTLIBPModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using MTL-IBP method. For details, see De Palma et al. (2024) Expressive Losses for Verified Robustness via Convex Combinations. https://arxiv.org/pdf/2305.13991\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=1, \n                 pgd_alpha=10, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n                 pgd_decay_milestones=(), pgd_eps_factor=1, mtl_ibp_alpha=0.5, checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n        \"\"\"\n        Initializes the MTLIBPModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): Shi regularization weight.\n            shi_reg_decay (bool): Whether to decay Shi regularization during the ramp up phase.\n            pgd_steps (int): Number of PGD steps for adversrial loss computation.\n            pgd_alpha (float): PGD step size for adversarial loss calculation.\n            pgd_restarts (int): Number of PGD restarts for adversarial loss calculation.\n            pgd_early_stopping (bool): Whether to use early stopping in PGD during adversarial loss calculation.\n            pgd_alpha_decay_factor (float): PGD alpha decay factor.\n            pgd_decay_milestones (tuple): Milestones for PGD alpha decay.\n            pgd_eps_factor (float): Factor for PGD epsilon.\n            mtl_ibp_alpha (float): Alpha value for MTL-IBP, i.e. the trade-off between certified and adversarial loss.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'mtl_ibp'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.optimizer_func = optimizer_func\n        self.pgd_steps = pgd_steps\n        self.pgd_alpha = pgd_alpha\n        self.pgd_restarts = pgd_restarts\n        self.pgd_early_stopping = pgd_early_stopping\n        self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n        self.pgd_decay_milestones = pgd_decay_milestones\n        self.pgd_eps_factor = pgd_eps_factor\n        self.mtl_ibp_alpha = mtl_ibp_alpha\n\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n        \"\"\"\n        Trains the model using the MTL-IBP method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n            start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n\n        trained_model = mtl_ibp_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            start_epoch=start_epoch,\n            end_epoch=end_epoch,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            alpha=self.mtl_ibp_alpha,\n            pgd_n_steps=self.pgd_steps,\n            pgd_step_size=self.pgd_alpha,\n            pgd_restarts=self.pgd_restarts,\n            pgd_eps_factor=self.pgd_eps_factor,\n            pgd_early_stopping=self.pgd_early_stopping,\n            pgd_decay_factor=self.pgd_alpha_decay_factor,\n            pgd_decay_checkpoints=self.pgd_decay_milestones,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        model_wrapper = MTLIBPModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            mtl_ibp_alpha=config['mtl_ibp:mtl_ibp_alpha'],\n            pgd_alpha=config['mtl_ibp:pgd_alpha'],\n            pgd_early_stopping=False,\n            pgd_restarts=config['mtl_ibp:pgd_restarts'],\n            pgd_steps=config['mtl_ibp:pgd_steps'],\n            pgd_eps_factor=config['mtl_ibp:mtl_ibp_eps_factor'],\n            pgd_decay_milestones=()\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/mtl_ibp_model_wrapper/#CTRAIN.model_wrappers.mtl_ibp_model_wrapper.MTLIBPModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, pgd_steps=1, pgd_alpha=10, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=0.1, pgd_decay_milestones=(), pgd_eps_factor=1, mtl_ibp_alpha=0.5, checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the MTLIBPModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>Shi regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay Shi regularization during the ramp up phase.</p> <code>True</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps for adversrial loss computation.</p> <code>1</code> <code>pgd_alpha</code> <code>float</code> <p>PGD step size for adversarial loss calculation.</p> <code>10</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for adversarial loss calculation.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD during adversarial loss calculation.</p> <code>False</code> <code>pgd_alpha_decay_factor</code> <code>float</code> <p>PGD alpha decay factor.</p> <code>0.1</code> <code>pgd_decay_milestones</code> <code>tuple</code> <p>Milestones for PGD alpha decay.</p> <code>()</code> <code>pgd_eps_factor</code> <code>float</code> <p>Factor for PGD epsilon.</p> <code>1</code> <code>mtl_ibp_alpha</code> <code>float</code> <p>Alpha value for MTL-IBP, i.e. the trade-off between certified and adversarial loss.</p> <code>0.5</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/mtl_ibp_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=1, \n             pgd_alpha=10, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n             pgd_decay_milestones=(), pgd_eps_factor=1, mtl_ibp_alpha=0.5, checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n    \"\"\"\n    Initializes the MTLIBPModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): Shi regularization weight.\n        shi_reg_decay (bool): Whether to decay Shi regularization during the ramp up phase.\n        pgd_steps (int): Number of PGD steps for adversrial loss computation.\n        pgd_alpha (float): PGD step size for adversarial loss calculation.\n        pgd_restarts (int): Number of PGD restarts for adversarial loss calculation.\n        pgd_early_stopping (bool): Whether to use early stopping in PGD during adversarial loss calculation.\n        pgd_alpha_decay_factor (float): PGD alpha decay factor.\n        pgd_decay_milestones (tuple): Milestones for PGD alpha decay.\n        pgd_eps_factor (float): Factor for PGD epsilon.\n        mtl_ibp_alpha (float): Alpha value for MTL-IBP, i.e. the trade-off between certified and adversarial loss.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'mtl_ibp'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.optimizer_func = optimizer_func\n    self.pgd_steps = pgd_steps\n    self.pgd_alpha = pgd_alpha\n    self.pgd_restarts = pgd_restarts\n    self.pgd_early_stopping = pgd_early_stopping\n    self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n    self.pgd_decay_milestones = pgd_decay_milestones\n    self.pgd_eps_factor = pgd_eps_factor\n    self.mtl_ibp_alpha = mtl_ibp_alpha\n</code></pre>"},{"location":"api/model_wrappers/mtl_ibp_model_wrapper/#CTRAIN.model_wrappers.mtl_ibp_model_wrapper.MTLIBPModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/mtl_ibp_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    model_wrapper = MTLIBPModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        mtl_ibp_alpha=config['mtl_ibp:mtl_ibp_alpha'],\n        pgd_alpha=config['mtl_ibp:pgd_alpha'],\n        pgd_early_stopping=False,\n        pgd_restarts=config['mtl_ibp:pgd_restarts'],\n        pgd_steps=config['mtl_ibp:pgd_steps'],\n        pgd_eps_factor=config['mtl_ibp:mtl_ibp_eps_factor'],\n        pgd_decay_milestones=()\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/mtl_ibp_model_wrapper/#CTRAIN.model_wrappers.mtl_ibp_model_wrapper.MTLIBPModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0, end_epoch=None)</code>","text":"<p>Trains the model using the MTL-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/mtl_ibp_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n    \"\"\"\n    Trains the model using the MTL-IBP method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n        start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n\n    trained_model = mtl_ibp_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        start_epoch=start_epoch,\n        end_epoch=end_epoch,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        alpha=self.mtl_ibp_alpha,\n        pgd_n_steps=self.pgd_steps,\n        pgd_step_size=self.pgd_alpha,\n        pgd_restarts=self.pgd_restarts,\n        pgd_eps_factor=self.pgd_eps_factor,\n        pgd_early_stopping=self.pgd_early_stopping,\n        pgd_decay_factor=self.pgd_alpha_decay_factor,\n        pgd_decay_checkpoints=self.pgd_decay_milestones,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/model_wrappers/sabr_model_wrapper/","title":"SABR","text":""},{"location":"api/model_wrappers/sabr_model_wrapper/#CTRAIN.model_wrappers.sabr_model_wrapper.SABRModelWrapper","title":"<code>SABRModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using SABR method. For details, see M\u00fcller et al. (2023) Certified training: Small boxes are all you need. https://arxiv.org/pdf/2210.04871</p> Source code in <code>CTRAIN/model_wrappers/sabr_model_wrapper.py</code> <pre><code>class SABRModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using SABR method. For details, see M\u00fcller et al. (2023) Certified training: Small boxes are all you need. https://arxiv.org/pdf/2210.04871\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, sabr_subselection_ratio=.2, pgd_steps=8, \n                 pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n                 pgd_decay_milestones=(4,7), checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n        \"\"\"\n        Initializes the SABRModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): SHI regularization weight.\n            shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n            sabr_subselection_ratio (float): Ratio of subselection for SABR (lambda in original publication).\n            pgd_steps (int): Number of PGD steps for adversarial loss computation.\n            pgd_alpha (float): PGD step size for adversarial loss calculation.\n            pgd_restarts (int): Number of PGD restarts for adversarial loss calculation.\n            pgd_early_stopping (bool): Whether to use early stopping in PGD during adversarial loss calculation.\n            pgd_alpha_decay_factor (float): PGD alpha decay factor.\n            pgd_decay_milestones (tuple): Milestones for PGD alpha decay.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'sabr'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.optimizer_func = optimizer_func\n        self.sabr_subselection_ratio = sabr_subselection_ratio\n        self.pgd_steps = pgd_steps\n        self.pgd_alpha = pgd_alpha\n        self.pgd_restarts = pgd_restarts\n        self.pgd_early_stopping = pgd_early_stopping\n        self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n        self.pgd_decay_milestones = pgd_decay_milestones\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n        \"\"\"\n        Trains the model using the SABR method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n            start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        trained_model = sabr_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            start_epoch=start_epoch,\n            end_epoch=end_epoch,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            eps_scheduler_args={},\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        model_wrapper = SABRModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            sabr_subselection_ratio=config['sabr:subselection_ratio'],\n            pgd_alpha=config['sabr:pgd_alpha'],\n            pgd_early_stopping=False,\n            pgd_restarts=config['sabr:pgd_restarts'],\n            pgd_steps=config['sabr:pgd_steps'],\n            pgd_decay_milestones=()\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/sabr_model_wrapper/#CTRAIN.model_wrappers.sabr_model_wrapper.SABRModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, sabr_subselection_ratio=0.2, pgd_steps=8, pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=0.1, pgd_decay_milestones=(4, 7), checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the SABRModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>SHI regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization during the ramp up phase.</p> <code>True</code> <code>sabr_subselection_ratio</code> <code>float</code> <p>Ratio of subselection for SABR (lambda in original publication).</p> <code>0.2</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps for adversarial loss computation.</p> <code>8</code> <code>pgd_alpha</code> <code>float</code> <p>PGD step size for adversarial loss calculation.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for adversarial loss calculation.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD during adversarial loss calculation.</p> <code>False</code> <code>pgd_alpha_decay_factor</code> <code>float</code> <p>PGD alpha decay factor.</p> <code>0.1</code> <code>pgd_decay_milestones</code> <code>tuple</code> <p>Milestones for PGD alpha decay.</p> <code>(4, 7)</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/sabr_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, sabr_subselection_ratio=.2, pgd_steps=8, \n             pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n             pgd_decay_milestones=(4,7), checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n    \"\"\"\n    Initializes the SABRModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): SHI regularization weight.\n        shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n        sabr_subselection_ratio (float): Ratio of subselection for SABR (lambda in original publication).\n        pgd_steps (int): Number of PGD steps for adversarial loss computation.\n        pgd_alpha (float): PGD step size for adversarial loss calculation.\n        pgd_restarts (int): Number of PGD restarts for adversarial loss calculation.\n        pgd_early_stopping (bool): Whether to use early stopping in PGD during adversarial loss calculation.\n        pgd_alpha_decay_factor (float): PGD alpha decay factor.\n        pgd_decay_milestones (tuple): Milestones for PGD alpha decay.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'sabr'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.optimizer_func = optimizer_func\n    self.sabr_subselection_ratio = sabr_subselection_ratio\n    self.pgd_steps = pgd_steps\n    self.pgd_alpha = pgd_alpha\n    self.pgd_restarts = pgd_restarts\n    self.pgd_early_stopping = pgd_early_stopping\n    self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n    self.pgd_decay_milestones = pgd_decay_milestones\n</code></pre>"},{"location":"api/model_wrappers/sabr_model_wrapper/#CTRAIN.model_wrappers.sabr_model_wrapper.SABRModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/sabr_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    model_wrapper = SABRModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        sabr_subselection_ratio=config['sabr:subselection_ratio'],\n        pgd_alpha=config['sabr:pgd_alpha'],\n        pgd_early_stopping=False,\n        pgd_restarts=config['sabr:pgd_restarts'],\n        pgd_steps=config['sabr:pgd_steps'],\n        pgd_decay_milestones=()\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/sabr_model_wrapper/#CTRAIN.model_wrappers.sabr_model_wrapper.SABRModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0, end_epoch=None)</code>","text":"<p>Trains the model using the SABR method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/sabr_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n    \"\"\"\n    Trains the model using the SABR method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n        start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    trained_model = sabr_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        start_epoch=start_epoch,\n        end_epoch=end_epoch,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        eps_scheduler_args={},\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/model_wrappers/shi_ibp_model_wrapper/","title":"SHI IBP","text":""},{"location":"api/model_wrappers/shi_ibp_model_wrapper/#CTRAIN.model_wrappers.shi_ibp_model_wrapper.ShiIBPModelWrapper","title":"<code>ShiIBPModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using SHI-IBP method. For details, see Shi et al. (2021) Fast certified robust training with short warmup. https://proceedings.neurips.cc/paper/2021/file/988f9153ac4fd966ea302dd9ab9bae15-Paper.pdf</p> Source code in <code>CTRAIN/model_wrappers/shi_ibp_model_wrapper.py</code> <pre><code>class ShiIBPModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using SHI-IBP method. For details, see Shi et al. (2021) Fast certified robust training with short warmup. https://proceedings.neurips.cc/paper/2021/file/988f9153ac4fd966ea302dd9ab9bae15-Paper.pdf\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n        \"\"\"\n        Initializes the ShiIBPModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): SHI regularization weight.\n            shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n            start_kappa (float): Starting value of kappa that trades-off IBP and clean loss.\n            end_kappa (float): Ending value of kappa.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'shi'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.start_kappa = start_kappa\n        self.end_kappa = end_kappa\n        self.optimizer_func = optimizer_func\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n        \"\"\"\n        Trains the model using the SHI-IBP method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n            start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        trained_model = shi_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            start_epoch=start_epoch,\n            end_epoch=end_epoch,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            eps_scheduler_args={'start_kappa': self.start_kappa, 'end_kappa': self.end_kappa},\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        model_wrapper = ShiIBPModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            start_kappa=config['shi:start_kappa'],\n            end_kappa=config['shi:end_kappa'] * config['shi:start_kappa'],\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/shi_ibp_model_wrapper/#CTRAIN.model_wrappers.shi_ibp_model_wrapper.ShiIBPModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the ShiIBPModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>SHI regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization during the ramp up phase.</p> <code>True</code> <code>start_kappa</code> <code>float</code> <p>Starting value of kappa that trades-off IBP and clean loss.</p> <code>1</code> <code>end_kappa</code> <code>float</code> <p>Ending value of kappa.</p> <code>0</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/shi_ibp_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, start_kappa=1, end_kappa=0, checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n    \"\"\"\n    Initializes the ShiIBPModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): SHI regularization weight.\n        shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n        start_kappa (float): Starting value of kappa that trades-off IBP and clean loss.\n        end_kappa (float): Ending value of kappa.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'shi'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.start_kappa = start_kappa\n    self.end_kappa = end_kappa\n    self.optimizer_func = optimizer_func\n</code></pre>"},{"location":"api/model_wrappers/shi_ibp_model_wrapper/#CTRAIN.model_wrappers.shi_ibp_model_wrapper.ShiIBPModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/shi_ibp_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    model_wrapper = ShiIBPModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        start_kappa=config['shi:start_kappa'],\n        end_kappa=config['shi:end_kappa'] * config['shi:start_kappa'],\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/shi_ibp_model_wrapper/#CTRAIN.model_wrappers.shi_ibp_model_wrapper.ShiIBPModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0, end_epoch=None)</code>","text":"<p>Trains the model using the SHI-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/shi_ibp_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n    \"\"\"\n    Trains the model using the SHI-IBP method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n        start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    trained_model = shi_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        start_epoch=start_epoch,\n        end_epoch=end_epoch,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        eps_scheduler_args={'start_kappa': self.start_kappa, 'end_kappa': self.end_kappa},\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/model_wrappers/staps_model_wrapper/","title":"STAPS","text":""},{"location":"api/model_wrappers/staps_model_wrapper/#CTRAIN.model_wrappers.staps_model_wrapper.STAPSModelWrapper","title":"<code>STAPSModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using STAPS method. For details, see Mao et al. (2023) Connecting Certified and Adversarial Training https://proceedings.neurips.cc/paper_files/paper/2023/file/e8b0c97b34fdaf58b2f48f8cca85e76a-Paper-Conference.pdf</p> Source code in <code>CTRAIN/model_wrappers/staps_model_wrapper.py</code> <pre><code>class STAPSModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using STAPS method. For details, see Mao et al. (2023) Connecting Certified and Adversarial Training https://proceedings.neurips.cc/paper_files/paper/2023/file/e8b0c97b34fdaf58b2f48f8cca85e76a-Paper-Conference.pdf\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=8, \n                 pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n                 pgd_decay_steps=(4,7), sabr_pgd_steps=8, sabr_pgd_alpha=0.5, sabr_pgd_restarts=1, \n                 sabr_pgd_early_stopping=False, sabr_pgd_alpha_decay_factor=.1, sabr_pgd_decay_milestones=(4,7),\n                 sabr_subselection_ratio=0.8, block_sizes=None, gradient_expansion_alpha=5,\n                 gradient_link_thresh=.5, gradient_link_tol=0.00001,\n                 checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n        \"\"\"\n        Initializes the STAPSModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): SHI regularization weight.\n            shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n            pgd_steps (int): Number of PGD steps for TAPS loss calculation.\n            pgd_alpha (float): PGD step size for TAPS loss calculation.\n            pgd_restarts (int): Number of PGD restarts ffor TAPS loss calculation.\n            pgd_early_stopping (bool): Whether to use early stopping in PGD for TAPS loss calculation.\n            pgd_alpha_decay_factor (float): PGD alpha decay factor.\n            pgd_decay_steps (tuple): Milestones for PGD alpha decay.\n            sabr_pgd_steps (int): Number of PGD steps for SABR.\n            sabr_pgd_alpha (float): PGD step size for SABR.\n            sabr_pgd_restarts (int): Number of PGD restarts for SABR.\n            sabr_pgd_early_stopping (bool): Whether to use early stopping in PGD for SABR.\n            sabr_pgd_alpha_decay_factor (float): PGD alpha decay factor for SABR.\n            sabr_pgd_decay_milestones (tuple): Milestones for PGD alpha decay for SABR.\n            sabr_subselection_ratio (float): Subselection ratio (lambda) for SABR.\n            block_sizes (list): Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.\n            gradient_expansion_alpha (float): Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.\n            gradient_link_thresh (float): Threshold for gradient link.\n            gradient_link_tol (float): Tolerance for gradient link.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'staps'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.optimizer_func = optimizer_func\n        self.pgd_steps = pgd_steps\n        self.pgd_alpha = pgd_alpha\n        self.pgd_restarts = pgd_restarts\n        self.pgd_early_stopping = pgd_early_stopping\n        self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n        self.pgd_decay_steps = pgd_decay_steps\n        self.block_sizes = block_sizes\n        self.gradient_expansion_alpha = gradient_expansion_alpha\n        self.gradient_link_thresh = gradient_link_thresh\n        self.gradient_link_tol = gradient_link_tol\n        self.sabr_pgd_steps = sabr_pgd_steps\n        self.sabr_pgd_alpha = sabr_pgd_alpha\n        self.sabr_pgd_restarts = sabr_pgd_restarts\n        self.sabr_pgd_early_stopping = sabr_pgd_early_stopping\n        self.sabr_pgd_alpha_decay_factor = sabr_pgd_alpha_decay_factor\n        self.sabr_pgd_decay_milestones = sabr_pgd_decay_milestones\n        self.sabr_subselection_ratio = sabr_subselection_ratio\n\n        ori_train = self.original_model.training\n        self.original_model.eval()\n        self.bounded_model.eval()\n\n        assert block_sizes is not None, \"For TAPS training we require a network split!\"\n        print(self.input_shape)\n        dummy_input = torch.zeros(self.input_shape, device=device)\n        blocks = split_network(self.original_model, block_sizes=block_sizes, network_input=dummy_input, device=device)\n        if len(blocks) != 2:\n            raise NotImplementedError(\"Currently we only support two blocks (feature extractor +  classifier) for TAPS/STAPS\")\n        features, classifier = blocks\n        self.bounded_model.bounded_blocks = [\n            BoundedModule(features, global_input=dummy_input, bound_opts=bound_opts, device=device),\n            BoundedModule(classifier, global_input=torch.zeros_like(features(dummy_input), device=device), bound_opts=bound_opts, device=device)\n        ]\n        self.bounded_model.original_blocks = [features, classifier]\n\n        if ori_train:\n            self.original_model.train()\n            self.bounded_model.train()\n\n\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n        \"\"\"\n        Trains the model using the STAPS method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n            start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n            end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        trained_model = staps_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            start_epoch=start_epoch,\n            end_epoch=end_epoch,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            eps_scheduler_args={},\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            gradient_expansion_alpha=self.gradient_expansion_alpha,\n            taps_gradient_link_thresh=self.gradient_link_thresh,\n            taps_gradient_link_tolerance=self.gradient_link_tol,\n            taps_pgd_restarts=self.pgd_restarts,\n            taps_pgd_step_size=self.pgd_alpha,\n            taps_pgd_steps=self.pgd_steps,\n            taps_pgd_decay_checkpoints=self.pgd_decay_steps,\n            taps_pgd_decay_factor=self.pgd_alpha_decay_factor,\n            sabr_pgd_steps=self.sabr_pgd_steps,\n            sabr_pgd_restarts=self.sabr_pgd_restarts,\n            sabr_pgd_step_size=self.sabr_pgd_alpha,\n            sabr_pgd_early_stopping=self.sabr_pgd_early_stopping,\n            sabr_pgd_decay_checkpoints=self.sabr_pgd_decay_milestones,\n            sabr_pgd_decay_factor=self.sabr_pgd_alpha_decay_factor,\n            subselection_ratio=self.sabr_subselection_ratio,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        no_layers = len(self.original_model.layers)\n        feature_extractor_size = math.ceil(config['taps:block_split_point'] * no_layers)\n        classifier_size = no_layers - feature_extractor_size\n        block_sizes = (feature_extractor_size, classifier_size)\n\n        model_wrapper = STAPSModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            sabr_subselection_ratio=config['sabr:subselection_ratio'],\n            sabr_pgd_alpha=config['sabr:pgd_alpha'],\n            sabr_pgd_early_stopping=False,\n            sabr_pgd_restarts=config['sabr:pgd_restarts'],\n            sabr_pgd_steps=config['sabr:pgd_steps'],\n            sabr_pgd_decay_milestones=(),\n            pgd_alpha=config['taps:pgd_alpha'],\n            pgd_restarts=config['taps:pgd_restarts'],\n            pgd_steps=config['taps:pgd_steps'],\n            gradient_expansion_alpha=config['taps:gradient_expansion_alpha'],\n            pgd_early_stopping=False,\n            pgd_decay_steps=(),\n            block_sizes=block_sizes\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/staps_model_wrapper/#CTRAIN.model_wrappers.staps_model_wrapper.STAPSModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, pgd_steps=8, pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=0.1, pgd_decay_steps=(4, 7), sabr_pgd_steps=8, sabr_pgd_alpha=0.5, sabr_pgd_restarts=1, sabr_pgd_early_stopping=False, sabr_pgd_alpha_decay_factor=0.1, sabr_pgd_decay_milestones=(4, 7), sabr_subselection_ratio=0.8, block_sizes=None, gradient_expansion_alpha=5, gradient_link_thresh=0.5, gradient_link_tol=1e-05, checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the STAPSModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>SHI regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization during the ramp up phase.</p> <code>True</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps for TAPS loss calculation.</p> <code>8</code> <code>pgd_alpha</code> <code>float</code> <p>PGD step size for TAPS loss calculation.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts ffor TAPS loss calculation.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD for TAPS loss calculation.</p> <code>False</code> <code>pgd_alpha_decay_factor</code> <code>float</code> <p>PGD alpha decay factor.</p> <code>0.1</code> <code>pgd_decay_steps</code> <code>tuple</code> <p>Milestones for PGD alpha decay.</p> <code>(4, 7)</code> <code>sabr_pgd_steps</code> <code>int</code> <p>Number of PGD steps for SABR.</p> <code>8</code> <code>sabr_pgd_alpha</code> <code>float</code> <p>PGD step size for SABR.</p> <code>0.5</code> <code>sabr_pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for SABR.</p> <code>1</code> <code>sabr_pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD for SABR.</p> <code>False</code> <code>sabr_pgd_alpha_decay_factor</code> <code>float</code> <p>PGD alpha decay factor for SABR.</p> <code>0.1</code> <code>sabr_pgd_decay_milestones</code> <code>tuple</code> <p>Milestones for PGD alpha decay for SABR.</p> <code>(4, 7)</code> <code>sabr_subselection_ratio</code> <code>float</code> <p>Subselection ratio (lambda) for SABR.</p> <code>0.8</code> <code>block_sizes</code> <code>list</code> <p>Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.</p> <code>None</code> <code>gradient_expansion_alpha</code> <code>float</code> <p>Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.</p> <code>5</code> <code>gradient_link_thresh</code> <code>float</code> <p>Threshold for gradient link.</p> <code>0.5</code> <code>gradient_link_tol</code> <code>float</code> <p>Tolerance for gradient link.</p> <code>1e-05</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/staps_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=8, \n             pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n             pgd_decay_steps=(4,7), sabr_pgd_steps=8, sabr_pgd_alpha=0.5, sabr_pgd_restarts=1, \n             sabr_pgd_early_stopping=False, sabr_pgd_alpha_decay_factor=.1, sabr_pgd_decay_milestones=(4,7),\n             sabr_subselection_ratio=0.8, block_sizes=None, gradient_expansion_alpha=5,\n             gradient_link_thresh=.5, gradient_link_tol=0.00001,\n             checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda')):\n    \"\"\"\n    Initializes the STAPSModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): SHI regularization weight.\n        shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n        pgd_steps (int): Number of PGD steps for TAPS loss calculation.\n        pgd_alpha (float): PGD step size for TAPS loss calculation.\n        pgd_restarts (int): Number of PGD restarts ffor TAPS loss calculation.\n        pgd_early_stopping (bool): Whether to use early stopping in PGD for TAPS loss calculation.\n        pgd_alpha_decay_factor (float): PGD alpha decay factor.\n        pgd_decay_steps (tuple): Milestones for PGD alpha decay.\n        sabr_pgd_steps (int): Number of PGD steps for SABR.\n        sabr_pgd_alpha (float): PGD step size for SABR.\n        sabr_pgd_restarts (int): Number of PGD restarts for SABR.\n        sabr_pgd_early_stopping (bool): Whether to use early stopping in PGD for SABR.\n        sabr_pgd_alpha_decay_factor (float): PGD alpha decay factor for SABR.\n        sabr_pgd_decay_milestones (tuple): Milestones for PGD alpha decay for SABR.\n        sabr_subselection_ratio (float): Subselection ratio (lambda) for SABR.\n        block_sizes (list): Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.\n        gradient_expansion_alpha (float): Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.\n        gradient_link_thresh (float): Threshold for gradient link.\n        gradient_link_tol (float): Tolerance for gradient link.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'staps'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.optimizer_func = optimizer_func\n    self.pgd_steps = pgd_steps\n    self.pgd_alpha = pgd_alpha\n    self.pgd_restarts = pgd_restarts\n    self.pgd_early_stopping = pgd_early_stopping\n    self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n    self.pgd_decay_steps = pgd_decay_steps\n    self.block_sizes = block_sizes\n    self.gradient_expansion_alpha = gradient_expansion_alpha\n    self.gradient_link_thresh = gradient_link_thresh\n    self.gradient_link_tol = gradient_link_tol\n    self.sabr_pgd_steps = sabr_pgd_steps\n    self.sabr_pgd_alpha = sabr_pgd_alpha\n    self.sabr_pgd_restarts = sabr_pgd_restarts\n    self.sabr_pgd_early_stopping = sabr_pgd_early_stopping\n    self.sabr_pgd_alpha_decay_factor = sabr_pgd_alpha_decay_factor\n    self.sabr_pgd_decay_milestones = sabr_pgd_decay_milestones\n    self.sabr_subselection_ratio = sabr_subselection_ratio\n\n    ori_train = self.original_model.training\n    self.original_model.eval()\n    self.bounded_model.eval()\n\n    assert block_sizes is not None, \"For TAPS training we require a network split!\"\n    print(self.input_shape)\n    dummy_input = torch.zeros(self.input_shape, device=device)\n    blocks = split_network(self.original_model, block_sizes=block_sizes, network_input=dummy_input, device=device)\n    if len(blocks) != 2:\n        raise NotImplementedError(\"Currently we only support two blocks (feature extractor +  classifier) for TAPS/STAPS\")\n    features, classifier = blocks\n    self.bounded_model.bounded_blocks = [\n        BoundedModule(features, global_input=dummy_input, bound_opts=bound_opts, device=device),\n        BoundedModule(classifier, global_input=torch.zeros_like(features(dummy_input), device=device), bound_opts=bound_opts, device=device)\n    ]\n    self.bounded_model.original_blocks = [features, classifier]\n\n    if ori_train:\n        self.original_model.train()\n        self.bounded_model.train()\n</code></pre>"},{"location":"api/model_wrappers/staps_model_wrapper/#CTRAIN.model_wrappers.staps_model_wrapper.STAPSModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/staps_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    no_layers = len(self.original_model.layers)\n    feature_extractor_size = math.ceil(config['taps:block_split_point'] * no_layers)\n    classifier_size = no_layers - feature_extractor_size\n    block_sizes = (feature_extractor_size, classifier_size)\n\n    model_wrapper = STAPSModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        sabr_subselection_ratio=config['sabr:subselection_ratio'],\n        sabr_pgd_alpha=config['sabr:pgd_alpha'],\n        sabr_pgd_early_stopping=False,\n        sabr_pgd_restarts=config['sabr:pgd_restarts'],\n        sabr_pgd_steps=config['sabr:pgd_steps'],\n        sabr_pgd_decay_milestones=(),\n        pgd_alpha=config['taps:pgd_alpha'],\n        pgd_restarts=config['taps:pgd_restarts'],\n        pgd_steps=config['taps:pgd_steps'],\n        gradient_expansion_alpha=config['taps:gradient_expansion_alpha'],\n        pgd_early_stopping=False,\n        pgd_decay_steps=(),\n        block_sizes=block_sizes\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc}\n</code></pre>"},{"location":"api/model_wrappers/staps_model_wrapper/#CTRAIN.model_wrappers.staps_model_wrapper.STAPSModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0, end_epoch=None)</code>","text":"<p>Trains the model using the STAPS method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/staps_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0, end_epoch=None):\n    \"\"\"\n    Trains the model using the STAPS method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n        start_epoch (int, optional): Epoch to start training from. Initialises learning rate and epsilon schedulers accordingly. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    trained_model = staps_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        start_epoch=start_epoch,\n        end_epoch=end_epoch,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        eps_scheduler_args={},\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        gradient_expansion_alpha=self.gradient_expansion_alpha,\n        taps_gradient_link_thresh=self.gradient_link_thresh,\n        taps_gradient_link_tolerance=self.gradient_link_tol,\n        taps_pgd_restarts=self.pgd_restarts,\n        taps_pgd_step_size=self.pgd_alpha,\n        taps_pgd_steps=self.pgd_steps,\n        taps_pgd_decay_checkpoints=self.pgd_decay_steps,\n        taps_pgd_decay_factor=self.pgd_alpha_decay_factor,\n        sabr_pgd_steps=self.sabr_pgd_steps,\n        sabr_pgd_restarts=self.sabr_pgd_restarts,\n        sabr_pgd_step_size=self.sabr_pgd_alpha,\n        sabr_pgd_early_stopping=self.sabr_pgd_early_stopping,\n        sabr_pgd_decay_checkpoints=self.sabr_pgd_decay_milestones,\n        sabr_pgd_decay_factor=self.sabr_pgd_alpha_decay_factor,\n        subselection_ratio=self.sabr_subselection_ratio,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/model_wrappers/taps_model_wrapper/","title":"TAPS","text":""},{"location":"api/model_wrappers/taps_model_wrapper/#CTRAIN.model_wrappers.taps_model_wrapper.TAPSModelWrapper","title":"<code>TAPSModelWrapper</code>","text":"<p>               Bases: <code>CTRAINWrapper</code></p> <p>Wrapper class for training models using TAPS method. For details, see Mao et al. (2023) Connecting Certified and Adversarial Training https://proceedings.neurips.cc/paper_files/paper/2023/file/e8b0c97b34fdaf58b2f48f8cca85e76a-Paper-Conference.pdf</p> Source code in <code>CTRAIN/model_wrappers/taps_model_wrapper.py</code> <pre><code>class TAPSModelWrapper(CTRAINWrapper):\n    \"\"\"\n    Wrapper class for training models using TAPS method. For details, see Mao et al. (2023) Connecting Certified and Adversarial Training https://proceedings.neurips.cc/paper_files/paper/2023/file/e8b0c97b34fdaf58b2f48f8cca85e76a-Paper-Conference.pdf\n    \"\"\"\n\n    def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n                 lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n                 shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=8, \n                 pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n                 pgd_decay_steps=(4,7), block_sizes=None, gradient_expansion_alpha=5,\n                 gradient_link_thresh=.5, gradient_link_tol=0.00001,\n                 checkpoint_save_path=None, checkpoint_save_interval=10,\n                 bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'),\n                 ):\n        \"\"\"\n        Initializes the TAPSModelWrapper.\n\n        Args:\n            model (torch.nn.Module): The model to be trained.\n            input_shape (tuple): Shape of the input data.\n            eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n            num_epochs (int): Number of epochs for training.\n            train_eps_factor (float): Factor for training epsilon.\n            optimizer_func (torch.optim.Optimizer): Optimizer function.\n            lr (float): Learning rate.\n            warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n            ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n            lr_decay_factor (float): Learning rate decay factor.\n            lr_decay_milestones (tuple): Milestones for learning rate decay.\n            gradient_clip (float): Gradient clipping value.\n            l1_reg_weight (float): L1 regularization weight.\n            shi_reg_weight (float): SHI regularization weight.\n            shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n            pgd_steps (int): Number of PGD steps for TAPS loss calculation.\n            pgd_alpha (float): PGD step size for TAPS loss calculation.\n            pgd_restarts (int): Number of PGD restarts ffor TAPS loss calculation.\n            pgd_early_stopping (bool): Whether to use early stopping in PGD for TAPS loss calculation.\n            pgd_alpha_decay_factor (float): PGD alpha decay factor.\n            pgd_decay_steps (tuple): Milestones for PGD alpha decay.\n            block_sizes (list): Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.\n            gradient_expansion_alpha (float): Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.\n            gradient_link_thresh (float): Threshold for gradient link.\n            gradient_link_tol (float): Tolerance for gradient link.\n            checkpoint_save_path (str): Path to save checkpoints.\n            checkpoint_save_interval (int): Interval for saving checkpoints.\n            bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n            device (torch.device): Device to run the training on.\n        \"\"\"\n        super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n        self.cert_train_method = 'taps'\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.warm_up_epochs = warm_up_epochs\n        self.ramp_up_epochs = ramp_up_epochs\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_milestones = lr_decay_milestones\n        self.gradient_clip = gradient_clip\n        self.l1_reg_weight = l1_reg_weight\n        self.shi_reg_weight = shi_reg_weight\n        self.shi_reg_decay = shi_reg_decay\n        self.optimizer_func = optimizer_func\n        self.pgd_steps = pgd_steps\n        self.pgd_alpha = pgd_alpha\n        self.pgd_restarts = pgd_restarts\n        self.pgd_early_stopping = pgd_early_stopping\n        self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n        self.pgd_decay_steps = pgd_decay_steps\n        self.block_sizes = block_sizes\n        self.gradient_expansion_alpha = gradient_expansion_alpha\n        self.gradient_link_thresh = gradient_link_thresh\n        self.gradient_link_tol = gradient_link_tol\n\n        ori_train = self.original_model.training\n        self.original_model.eval()\n        self.bounded_model.eval()\n\n        assert block_sizes is not None, \"For TAPS training we require a network split!\"\n        dummy_input = torch.zeros(self.input_shape, device=device)\n        blocks = split_network(self.original_model, block_sizes=block_sizes, network_input=dummy_input, device=device)\n        if len(blocks) != 2:\n            raise NotImplementedError(\"Currently we only support two blocks (feature extractor +  classifier) for TAPS/STAPS\")\n        features, classifier = blocks\n        self.bounded_model.bounded_blocks = [\n            BoundedModule(features, global_input=dummy_input, bound_opts=bound_opts, device=device),\n            BoundedModule(classifier, global_input=torch.zeros_like(features(dummy_input), device=device), bound_opts=bound_opts, device=device)\n        ]\n        self.bounded_model.original_blocks = [features, classifier]\n\n        if ori_train:\n            self.original_model.train()\n            self.bounded_model.train()\n\n\n\n    def train_model(self, train_loader, val_loader=None, start_epoch=0):\n        \"\"\"\n        Trains the model using the TAPS method.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n\n        Returns:\n            (auto_LiRPA.BoundedModule): Trained model.\n        \"\"\"\n        eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n        eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n        trained_model = taps_train_model(\n            original_model=self.original_model,\n            hardened_model=self.bounded_model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            num_epochs=self.num_epochs,\n            eps=self.train_eps,\n            eps_std=eps_std,\n            eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n            eps_scheduler_args={},\n            optimizer=self.optimizer,\n            lr_decay_schedule=self.lr_decay_milestones,\n            lr_decay_factor=self.lr_decay_factor,\n            n_classes=self.n_classes,\n            gradient_clip=self.gradient_clip,\n            l1_regularisation_weight=self.l1_reg_weight,\n            shi_regularisation_weight=self.shi_reg_weight,\n            shi_reg_decay=self.shi_reg_decay,\n            gradient_expansion_alpha=self.gradient_expansion_alpha,\n            taps_gradient_link_thresh=self.gradient_link_thresh,\n            taps_gradient_link_tolerance=self.gradient_link_tol,\n            taps_pgd_restarts=self.pgd_restarts,\n            taps_pgd_step_size=self.pgd_alpha,\n            taps_pgd_steps=self.pgd_steps,\n            taps_pgd_decay_checkpoints=self.pgd_decay_steps,\n            taps_pgd_decay_factor=self.pgd_alpha_decay_factor,\n            start_epoch=start_epoch,\n            results_path=self.checkpoint_path,\n            checkpoint_save_interval=self.checkpoint_save_interval,\n            device=self.device\n        )\n\n        return trained_model\n\n    def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n        \"\"\"\n        Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n        Args:\n            config (dict): Configuration of hyperparameters.\n            seed (int): Seed used.\n            epochs (int): Number of epochs for training.\n            train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n            val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n            output_dir (str): Directory to save output.\n            cert_eval_samples (int, optional): Number of samples for certification evaluation.\n            include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n            include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n            include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n        Returns:\n            tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n        \"\"\"\n        config_hash = get_config_hash(config, 32)\n        seed_ctrain(seed)\n\n        if config['optimizer_func'] == 'adam':\n            optimizer_func = torch.optim.Adam\n        elif config['optimizer_func'] == 'radam':\n            optimizer_func = torch.optim.RAdam\n        if config['optimizer_func'] == 'adamw':\n            optimizer_func = torch.optim.AdamW\n\n        lr_decay_milestones = [\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n            config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n        ]\n\n        no_layers = len(self.original_model.layers)\n        feature_extractor_size = math.ceil(config['taps:block_split_point'] * no_layers)\n        classifier_size = no_layers - feature_extractor_size\n        block_sizes = (feature_extractor_size, classifier_size)\n\n        model_wrapper = TAPSModelWrapper(\n            model=copy.deepcopy(self.original_model), \n            input_shape=self.input_shape,\n            eps=self.eps,\n            num_epochs=epochs, \n            bound_opts=self.bound_opts,\n            checkpoint_save_path=None,\n            device=self.device,\n            train_eps_factor=config['train_eps_factor'],\n            optimizer_func=optimizer_func,\n            lr=config['learning_rate'],\n            warm_up_epochs=config['warm_up_epochs'],\n            ramp_up_epochs=config['ramp_up_epochs'],\n            gradient_clip=10,\n            lr_decay_factor=config['lr_decay_factor'],\n            lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n            l1_reg_weight=config['l1_reg_weight'],\n            shi_reg_weight=config['shi_reg_weight'],\n            shi_reg_decay=config['shi_reg_decay'],\n            pgd_alpha=config['taps:pgd_alpha'],\n            pgd_restarts=config['taps:pgd_restarts'],\n            pgd_steps=config['taps:pgd_steps'],\n            gradient_expansion_alpha=config['taps:gradient_expansion_alpha'],\n            pgd_early_stopping=False,\n            pgd_decay_steps=(),\n            block_sizes=block_sizes,\n        )\n\n        model_wrapper.train_model(train_loader=train_loader)\n        torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n        model_wrapper.eval()\n        std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n        loss = 0\n        if include_nat_loss:\n            loss -= std_acc\n        if include_adv_loss:\n            loss -= adv_acc\n        if include_cert_loss:\n            loss -= cert_acc\n\n        return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc} \n</code></pre>"},{"location":"api/model_wrappers/taps_model_wrapper/#CTRAIN.model_wrappers.taps_model_wrapper.TAPSModelWrapper.__init__","title":"<code>__init__(model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70, lr_decay_factor=0.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=1e-06, shi_reg_weight=0.5, shi_reg_decay=True, pgd_steps=8, pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=0.1, pgd_decay_steps=(4, 7), block_sizes=None, gradient_expansion_alpha=5, gradient_link_thresh=0.5, gradient_link_tol=1e-05, checkpoint_save_path=None, checkpoint_save_interval=10, bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'))</code>","text":"<p>Initializes the TAPSModelWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained.</p> required <code>input_shape</code> <code>tuple</code> <p>Shape of the input data.</p> required <code>eps</code> <code>float</code> <p>Epsilon value describing the perturbation the network should be certifiably robust against.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_eps_factor</code> <code>float</code> <p>Factor for training epsilon.</p> <code>1</code> <code>optimizer_func</code> <code>Optimizer</code> <p>Optimizer function.</p> <code>Adam</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>warm_up_epochs</code> <code>int</code> <p>Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.</p> <code>1</code> <code>ramp_up_epochs</code> <code>int</code> <p>Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.</p> <code>70</code> <code>lr_decay_factor</code> <code>float</code> <p>Learning rate decay factor.</p> <code>0.2</code> <code>lr_decay_milestones</code> <code>tuple</code> <p>Milestones for learning rate decay.</p> <code>(80, 90)</code> <code>gradient_clip</code> <code>float</code> <p>Gradient clipping value.</p> <code>10</code> <code>l1_reg_weight</code> <code>float</code> <p>L1 regularization weight.</p> <code>1e-06</code> <code>shi_reg_weight</code> <code>float</code> <p>SHI regularization weight.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization during the ramp up phase.</p> <code>True</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps for TAPS loss calculation.</p> <code>8</code> <code>pgd_alpha</code> <code>float</code> <p>PGD step size for TAPS loss calculation.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts ffor TAPS loss calculation.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD for TAPS loss calculation.</p> <code>False</code> <code>pgd_alpha_decay_factor</code> <code>float</code> <p>PGD alpha decay factor.</p> <code>0.1</code> <code>pgd_decay_steps</code> <code>tuple</code> <p>Milestones for PGD alpha decay.</p> <code>(4, 7)</code> <code>block_sizes</code> <code>list</code> <p>Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.</p> <code>None</code> <code>gradient_expansion_alpha</code> <code>float</code> <p>Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.</p> <code>5</code> <code>gradient_link_thresh</code> <code>float</code> <p>Threshold for gradient link.</p> <code>0.5</code> <code>gradient_link_tol</code> <code>float</code> <p>Tolerance for gradient link.</p> <code>1e-05</code> <code>checkpoint_save_path</code> <code>str</code> <p>Path to save checkpoints.</p> <code>None</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints.</p> <code>10</code> <code>bound_opts</code> <code>dict</code> <p>Options for bounding according to the auto_LiRPA documentation.</p> <code>dict(conv_mode='patches', relu='adaptive')</code> <code>device</code> <code>device</code> <p>Device to run the training on.</p> <code>device('cuda')</code> Source code in <code>CTRAIN/model_wrappers/taps_model_wrapper.py</code> <pre><code>def __init__(self, model, input_shape, eps, num_epochs, train_eps_factor=1, optimizer_func=torch.optim.Adam, lr=0.0005, warm_up_epochs=1, ramp_up_epochs=70,\n             lr_decay_factor=.2, lr_decay_milestones=(80, 90), gradient_clip=10, l1_reg_weight=0.000001,\n             shi_reg_weight=.5, shi_reg_decay=True, pgd_steps=8, \n             pgd_alpha=0.5, pgd_restarts=1, pgd_early_stopping=False, pgd_alpha_decay_factor=.1,\n             pgd_decay_steps=(4,7), block_sizes=None, gradient_expansion_alpha=5,\n             gradient_link_thresh=.5, gradient_link_tol=0.00001,\n             checkpoint_save_path=None, checkpoint_save_interval=10,\n             bound_opts=dict(conv_mode='patches', relu='adaptive'), device=torch.device('cuda'),\n             ):\n    \"\"\"\n    Initializes the TAPSModelWrapper.\n\n    Args:\n        model (torch.nn.Module): The model to be trained.\n        input_shape (tuple): Shape of the input data.\n        eps (float): Epsilon value describing the perturbation the network should be certifiably robust against.\n        num_epochs (int): Number of epochs for training.\n        train_eps_factor (float): Factor for training epsilon.\n        optimizer_func (torch.optim.Optimizer): Optimizer function.\n        lr (float): Learning rate.\n        warm_up_epochs (int): Number of warm-up epochs, i.e. epochs where the model is trained on clean loss.\n        ramp_up_epochs (int): Number of ramp-up epochs, i.e. epochs where the epsilon is gradually increased to the target train epsilon.\n        lr_decay_factor (float): Learning rate decay factor.\n        lr_decay_milestones (tuple): Milestones for learning rate decay.\n        gradient_clip (float): Gradient clipping value.\n        l1_reg_weight (float): L1 regularization weight.\n        shi_reg_weight (float): SHI regularization weight.\n        shi_reg_decay (bool): Whether to decay SHI regularization during the ramp up phase.\n        pgd_steps (int): Number of PGD steps for TAPS loss calculation.\n        pgd_alpha (float): PGD step size for TAPS loss calculation.\n        pgd_restarts (int): Number of PGD restarts ffor TAPS loss calculation.\n        pgd_early_stopping (bool): Whether to use early stopping in PGD for TAPS loss calculation.\n        pgd_alpha_decay_factor (float): PGD alpha decay factor.\n        pgd_decay_steps (tuple): Milestones for PGD alpha decay.\n        block_sizes (list): Sizes of blocks for STAPS. This is used to split up the network into feature extractor and classifier. These must sum up to the number of layers in the network.\n        gradient_expansion_alpha (float): Alpha value for gradient expansion, i.e. the factor the STAPS gradient is multiplied by.\n        gradient_link_thresh (float): Threshold for gradient link.\n        gradient_link_tol (float): Tolerance for gradient link.\n        checkpoint_save_path (str): Path to save checkpoints.\n        checkpoint_save_interval (int): Interval for saving checkpoints.\n        bound_opts (dict): Options for bounding according to the auto_LiRPA documentation.\n        device (torch.device): Device to run the training on.\n    \"\"\"\n    super().__init__(model, eps, input_shape, train_eps_factor, lr, optimizer_func, bound_opts, device, checkpoint_save_path=checkpoint_save_path, checkpoint_save_interval=checkpoint_save_interval)\n    self.cert_train_method = 'taps'\n    self.num_epochs = num_epochs\n    self.lr = lr\n    self.warm_up_epochs = warm_up_epochs\n    self.ramp_up_epochs = ramp_up_epochs\n    self.lr_decay_factor = lr_decay_factor\n    self.lr_decay_milestones = lr_decay_milestones\n    self.gradient_clip = gradient_clip\n    self.l1_reg_weight = l1_reg_weight\n    self.shi_reg_weight = shi_reg_weight\n    self.shi_reg_decay = shi_reg_decay\n    self.optimizer_func = optimizer_func\n    self.pgd_steps = pgd_steps\n    self.pgd_alpha = pgd_alpha\n    self.pgd_restarts = pgd_restarts\n    self.pgd_early_stopping = pgd_early_stopping\n    self.pgd_alpha_decay_factor = pgd_alpha_decay_factor\n    self.pgd_decay_steps = pgd_decay_steps\n    self.block_sizes = block_sizes\n    self.gradient_expansion_alpha = gradient_expansion_alpha\n    self.gradient_link_thresh = gradient_link_thresh\n    self.gradient_link_tol = gradient_link_tol\n\n    ori_train = self.original_model.training\n    self.original_model.eval()\n    self.bounded_model.eval()\n\n    assert block_sizes is not None, \"For TAPS training we require a network split!\"\n    dummy_input = torch.zeros(self.input_shape, device=device)\n    blocks = split_network(self.original_model, block_sizes=block_sizes, network_input=dummy_input, device=device)\n    if len(blocks) != 2:\n        raise NotImplementedError(\"Currently we only support two blocks (feature extractor +  classifier) for TAPS/STAPS\")\n    features, classifier = blocks\n    self.bounded_model.bounded_blocks = [\n        BoundedModule(features, global_input=dummy_input, bound_opts=bound_opts, device=device),\n        BoundedModule(classifier, global_input=torch.zeros_like(features(dummy_input), device=device), bound_opts=bound_opts, device=device)\n    ]\n    self.bounded_model.original_blocks = [features, classifier]\n\n    if ori_train:\n        self.original_model.train()\n        self.bounded_model.train()\n</code></pre>"},{"location":"api/model_wrappers/taps_model_wrapper/#CTRAIN.model_wrappers.taps_model_wrapper.TAPSModelWrapper._hpo_runner","title":"<code>_hpo_runner(config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True)</code>","text":"<p>Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration of hyperparameters.</p> required <code>seed</code> <code>int</code> <p>Seed used.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output.</p> required <code>cert_eval_samples</code> <code>int</code> <p>Number of samples for certification evaluation.</p> <code>1000</code> <code>include_nat_loss</code> <code>bool</code> <p>Whether to include natural loss into HPO loss.</p> <code>True</code> <code>include_adv_loss</code> <code>bool</code> <p>Whether to include adversarial loss into HPO loss.</p> <code>True</code> <code>include_cert_loss</code> <code>bool</code> <p>Whether to include certification loss into HPO loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Loss and dictionary of accuracies that is saved as information to the run by SMAC3.</p> Source code in <code>CTRAIN/model_wrappers/taps_model_wrapper.py</code> <pre><code>def _hpo_runner(self, config, seed, epochs, train_loader, val_loader, output_dir, cert_eval_samples=1000, include_nat_loss=True, include_adv_loss=True, include_cert_loss=True):\n    \"\"\"\n    Function called during hyperparameter optimization (HPO) using SMAC3, returns the loss.\n\n    Args:\n        config (dict): Configuration of hyperparameters.\n        seed (int): Seed used.\n        epochs (int): Number of epochs for training.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        output_dir (str): Directory to save output.\n        cert_eval_samples (int, optional): Number of samples for certification evaluation.\n        include_nat_loss (bool, optional): Whether to include natural loss into HPO loss.\n        include_adv_loss (bool, optional): Whether to include adversarial loss into HPO loss.\n        include_cert_loss (bool, optional): Whether to include certification loss into HPO loss.\n\n    Returns:\n        tuple: Loss and dictionary of accuracies that is saved as information to the run by SMAC3.\n    \"\"\"\n    config_hash = get_config_hash(config, 32)\n    seed_ctrain(seed)\n\n    if config['optimizer_func'] == 'adam':\n        optimizer_func = torch.optim.Adam\n    elif config['optimizer_func'] == 'radam':\n        optimizer_func = torch.optim.RAdam\n    if config['optimizer_func'] == 'adamw':\n        optimizer_func = torch.optim.AdamW\n\n    lr_decay_milestones = [\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'],\n        config['warm_up_epochs'] + config['ramp_up_epochs'] + config['lr_decay_epoch_1'] + config['lr_decay_epoch_2']\n    ]\n\n    no_layers = len(self.original_model.layers)\n    feature_extractor_size = math.ceil(config['taps:block_split_point'] * no_layers)\n    classifier_size = no_layers - feature_extractor_size\n    block_sizes = (feature_extractor_size, classifier_size)\n\n    model_wrapper = TAPSModelWrapper(\n        model=copy.deepcopy(self.original_model), \n        input_shape=self.input_shape,\n        eps=self.eps,\n        num_epochs=epochs, \n        bound_opts=self.bound_opts,\n        checkpoint_save_path=None,\n        device=self.device,\n        train_eps_factor=config['train_eps_factor'],\n        optimizer_func=optimizer_func,\n        lr=config['learning_rate'],\n        warm_up_epochs=config['warm_up_epochs'],\n        ramp_up_epochs=config['ramp_up_epochs'],\n        gradient_clip=10,\n        lr_decay_factor=config['lr_decay_factor'],\n        lr_decay_milestones=[epoch for epoch in lr_decay_milestones if epoch &lt;= epochs],\n        l1_reg_weight=config['l1_reg_weight'],\n        shi_reg_weight=config['shi_reg_weight'],\n        shi_reg_decay=config['shi_reg_decay'],\n        pgd_alpha=config['taps:pgd_alpha'],\n        pgd_restarts=config['taps:pgd_restarts'],\n        pgd_steps=config['taps:pgd_steps'],\n        gradient_expansion_alpha=config['taps:gradient_expansion_alpha'],\n        pgd_early_stopping=False,\n        pgd_decay_steps=(),\n        block_sizes=block_sizes,\n    )\n\n    model_wrapper.train_model(train_loader=train_loader)\n    torch.save(model_wrapper.state_dict(), f'{output_dir}/nets/{config_hash}.pt')\n    model_wrapper.eval()\n    std_acc, cert_acc, adv_acc = model_wrapper.evaluate(test_loader=val_loader, test_samples=cert_eval_samples)\n\n    loss = 0\n    if include_nat_loss:\n        loss -= std_acc\n    if include_adv_loss:\n        loss -= adv_acc\n    if include_cert_loss:\n        loss -= cert_acc\n\n    return loss, {'nat_acc': std_acc, 'adv_acc': adv_acc, 'cert_acc': cert_acc} \n</code></pre>"},{"location":"api/model_wrappers/taps_model_wrapper/#CTRAIN.model_wrappers.taps_model_wrapper.TAPSModelWrapper.train_model","title":"<code>train_model(train_loader, val_loader=None, start_epoch=0)</code>","text":"<p>Trains the model using the TAPS method.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>Trained model.</p> Source code in <code>CTRAIN/model_wrappers/taps_model_wrapper.py</code> <pre><code>def train_model(self, train_loader, val_loader=None, start_epoch=0):\n    \"\"\"\n    Trains the model using the TAPS method.\n\n    Args:\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for validation data.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): Trained model.\n    \"\"\"\n    eps_std = self.train_eps / train_loader.std if train_loader.normalised else torch.tensor(self.train_eps)\n    eps_std = torch.reshape(eps_std, (*eps_std.shape, 1, 1))\n    trained_model = taps_train_model(\n        original_model=self.original_model,\n        hardened_model=self.bounded_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=self.num_epochs,\n        eps=self.train_eps,\n        eps_std=eps_std,\n        eps_schedule=(self.warm_up_epochs, self.ramp_up_epochs),\n        eps_scheduler_args={},\n        optimizer=self.optimizer,\n        lr_decay_schedule=self.lr_decay_milestones,\n        lr_decay_factor=self.lr_decay_factor,\n        n_classes=self.n_classes,\n        gradient_clip=self.gradient_clip,\n        l1_regularisation_weight=self.l1_reg_weight,\n        shi_regularisation_weight=self.shi_reg_weight,\n        shi_reg_decay=self.shi_reg_decay,\n        gradient_expansion_alpha=self.gradient_expansion_alpha,\n        taps_gradient_link_thresh=self.gradient_link_thresh,\n        taps_gradient_link_tolerance=self.gradient_link_tol,\n        taps_pgd_restarts=self.pgd_restarts,\n        taps_pgd_step_size=self.pgd_alpha,\n        taps_pgd_steps=self.pgd_steps,\n        taps_pgd_decay_checkpoints=self.pgd_decay_steps,\n        taps_pgd_decay_factor=self.pgd_alpha_decay_factor,\n        start_epoch=start_epoch,\n        results_path=self.checkpoint_path,\n        checkpoint_save_interval=self.checkpoint_save_interval,\n        device=self.device\n    )\n\n    return trained_model\n</code></pre>"},{"location":"api/train/certified/crown_ibp/","title":"CROWN IBP","text":""},{"location":"api/train/certified/crown_ibp/#CTRAIN.train.certified.crown_ibp.crown_ibp_train_model","title":"<code>crown_ibp_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, lr_decay_schedule=(15, 25), lr_decay_factor=0.2, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, l1_regularisation_weight=1e-05, shi_regularisation_weight=1, shi_reg_decay=1, results_path='./results', checkpoint_save_interval=10, device='cuda')</code>","text":"<p>Train a model using the CROWN-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be trained.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Epsilon value for perturbation. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for epsilon scheduler. Defaults to dict().</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training. Defaults to None.</p> <code>None</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to .2.</p> <code>0.2</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularization. Defaults to 0.00001.</p> <code>1e-05</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for SHI regularization. Defaults to 1.</p> <code>1</code> <code>shi_reg_decay</code> <code>float</code> <p>Decay factor for SHI regularization. Defaults to 1.</p> <code>1</code> <code>results_path</code> <code>str</code> <p>Path to save the training results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained hardened model.</p> Source code in <code>CTRAIN/train/certified/crown_ibp.py</code> <pre><code>def crown_ibp_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=0.2,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    l1_regularisation_weight=0.00001,\n    shi_regularisation_weight=1,\n    shi_reg_decay=1,\n    results_path=\"./results\",\n    checkpoint_save_interval=10,\n    device=\"cuda\",\n):\n    \"\"\"\n    Train a model using the CROWN-IBP method.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be trained.\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Epsilon value for perturbation. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for epsilon scheduler. Defaults to dict().\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training. Defaults to None.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to .2.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        l1_regularisation_weight (float, optional): Weight for L1 regularization. Defaults to 0.00001.\n        shi_regularisation_weight (float, optional): Weight for SHI regularization. Defaults to 1.\n        shi_reg_decay (float, optional): Decay factor for SHI regularization. Defaults to 1.\n        results_path (str, optional): Path to save the training results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): The trained hardened model.\n    \"\"\"\n\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction='none')\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    # Not done in original paper - however it is generally beneficial\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    if start_epoch == 0:\n        # Not done in original paper - however it is SotA and generally beneficial\n        ibp_init_shi(original_model, hardened_model)\n\n    cur_eps, kappa = eps_scheduler.get_cur_eps(), eps_scheduler.get_cur_kappa()\n\n    for epoch in range(start_epoch, end_epoch):\n\n        epoch_rob_err = 0\n        epoch_nat_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}, kappa {kappa:.2f} \"\n        )\n        hardened_model.train()\n        original_model.train()\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n            kappa = eps_scheduler.get_cur_kappa()\n            beta = eps_scheduler.get_cur_beta()\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n\n            clean_output = hardened_model(data)\n            clean_loss = criterion(clean_output, target).mean()\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n\n            if eps_scheduler.get_cur_eps(normalise=False) != 0.0:\n                certified_loss, robust_err = get_crown_ibp_loss(\n                    hardened_model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    criterion=criterion,\n                    beta=beta,\n                    return_bounds=False,\n                    return_stats=True,\n                )\n\n                epoch_rob_err += robust_err\n\n                loss = kappa * clean_loss + (1 - kappa) * certified_loss\n            else:\n                loss = clean_loss\n\n            if eps_scheduler.get_cur_eps(normalise=False) != eps_scheduler.get_max_eps(\n                normalise=False\n            ):\n                # Not done in original paper - however it is SotA and generally beneficial\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n\n                loss_regularisers = shi_regularisation_weight * loss_regularisers\n                loss = loss + loss_regularisers\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss += l1_regularisation\n\n            loss.backward()\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: N/A\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/eps_scheduler/","title":"eps_scheduler","text":""},{"location":"api/train/certified/eps_scheduler/#CTRAIN.train.certified.eps_scheduler.BaseScheduler","title":"<code>BaseScheduler</code>","text":"Source code in <code>CTRAIN/train/certified/eps_scheduler.py</code> <pre><code>class BaseScheduler():\n\n    def __init__(self, num_epochs, eps, mean, std, start_eps=0, start_kappa=1, end_kappa=0, start_beta=0, end_beta=0, eps_schedule_unit='batch', eps_schedule=(0, 20), batches_per_epoch=None, start_epoch=-1):\n        \"\"\"\n        Initializes the Base EpsScheduler.\n\n        Args:\n            num_epochs (int): The number of epochs for training.\n            eps (float): The epsilon value for the scheduler.\n            mean (float): The mean value for normalization.\n            std (float): The standard deviation value for normalization.\n            start_eps (float, optional): The starting epsilon value. Defaults to 0.\n            start_kappa (float, optional): The starting kappa value. Defaults to 1.\n            end_kappa (float, optional): The ending kappa value. Defaults to 0.\n            start_beta (float, optional): The starting beta value. Defaults to 0.\n            end_beta (float, optional): The ending beta value. Defaults to 0.\n            eps_schedule_unit (str, optional): The unit for epsilon scheduling ('batch' or 'epoch'). Defaults to 'batch'.\n            eps_schedule (tuple, optional): The schedule for epsilon values. Defaults to (0, 20).\n            batches_per_epoch (int, optional): The number of batches per epoch. Defaults to None.\n            start_epoch (int, optional): The starting epoch number. Defaults to -1.\n\n        Raises:\n            AssertionError: If num_epochs is None and eps_schedule_unit is 'epoch'.\n            AssertionError: If the length of eps_schedule is not 2 or 3.\n            AssertionError: If num_epochs is incompatible with eps_schedule.\n        \"\"\"\n\n        if num_epochs is None and eps_schedule_unit=='epoch':\n            num_epochs = sum(eps_schedule)\n        elif num_epochs is None:\n            assert False, \"Please provide number of epochs!\"\n        if eps_schedule_unit=='epoch':\n            if len(eps_schedule) == 3:\n                assert num_epochs == sum(eps_schedule), \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n            elif len(eps_schedule)==2:\n                assert num_epochs &gt;= sum(eps_schedule), \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n            else:\n                assert False, \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n\n        self.num_epochs = num_epochs\n        if len(eps_schedule) == 2:\n            self.warm_up, self.ramp_up = eps_schedule\n        elif len(eps_schedule) == 3:\n            self.warm_up, self.ramp_up, _ = eps_schedule\n\n        print(self.warm_up, self.ramp_up)\n        self.cur_eps = start_eps\n        self.cur_kappa = self.start_kappa = start_kappa\n        self.end_kappa = end_kappa\n        self.eps = eps\n        self.start_eps = start_eps\n        self.batches_per_epoch = batches_per_epoch\n        self.start_beta = self.cur_beta = start_beta\n        self.end_beta = end_beta\n        self.mean = mean\n        self.std = std\n\n        if eps_schedule_unit == 'epoch':\n            self.warm_up *= batches_per_epoch\n            self.ramp_up *= batches_per_epoch\n\n        self.training_steps = num_epochs * batches_per_epoch\n\n        self.no_batches = 0\n\n        if start_epoch &gt; 0:\n            self.no_batches = self.batches_per_epoch * start_epoch - 1            \n\n    def get_cur_eps(self, normalise=True):\n        \"\"\"\n        Get the current epsilon value, optionally normalised.\n\n        Args:\n            normalise (bool): If True, the returned epsilon value will be normalised by the standard deviation.\n\n        Returns:\n            torch.Tensor: The current epsilon value, normalised if specified.\n\n        Notes:\n            - The method checks for numerical instabilities and adjusts the current epsilon value if necessary.\n        \"\"\"\n        # Check needed to mitigate numerical instabilities\n        if (torch.tensor(self.get_max_eps(normalise=False) - self.cur_eps)  &lt; 1e-7).all():\n            self.cur_eps = self.get_max_eps(normalise=False)\n        return torch.tensor(self.cur_eps) / torch.tensor(self.std) if normalise else self.cur_eps\n\n    def get_cur_kappa(self):\n        return self.cur_kappa\n\n    def get_cur_beta(self):\n        return self.cur_beta\n\n    def get_max_eps(self, normalise=True):\n        return torch.tensor(self.eps) / torch.tensor(self.std) if normalise else self.eps\n\n    def batch_step(self, ):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/train/certified/eps_scheduler/#CTRAIN.train.certified.eps_scheduler.BaseScheduler.__init__","title":"<code>__init__(num_epochs, eps, mean, std, start_eps=0, start_kappa=1, end_kappa=0, start_beta=0, end_beta=0, eps_schedule_unit='batch', eps_schedule=(0, 20), batches_per_epoch=None, start_epoch=-1)</code>","text":"<p>Initializes the Base EpsScheduler.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>The number of epochs for training.</p> required <code>eps</code> <code>float</code> <p>The epsilon value for the scheduler.</p> required <code>mean</code> <code>float</code> <p>The mean value for normalization.</p> required <code>std</code> <code>float</code> <p>The standard deviation value for normalization.</p> required <code>start_eps</code> <code>float</code> <p>The starting epsilon value. Defaults to 0.</p> <code>0</code> <code>start_kappa</code> <code>float</code> <p>The starting kappa value. Defaults to 1.</p> <code>1</code> <code>end_kappa</code> <code>float</code> <p>The ending kappa value. Defaults to 0.</p> <code>0</code> <code>start_beta</code> <code>float</code> <p>The starting beta value. Defaults to 0.</p> <code>0</code> <code>end_beta</code> <code>float</code> <p>The ending beta value. Defaults to 0.</p> <code>0</code> <code>eps_schedule_unit</code> <code>str</code> <p>The unit for epsilon scheduling ('batch' or 'epoch'). Defaults to 'batch'.</p> <code>'batch'</code> <code>eps_schedule</code> <code>tuple</code> <p>The schedule for epsilon values. Defaults to (0, 20).</p> <code>(0, 20)</code> <code>batches_per_epoch</code> <code>int</code> <p>The number of batches per epoch. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>The starting epoch number. Defaults to -1.</p> <code>-1</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If num_epochs is None and eps_schedule_unit is 'epoch'.</p> <code>AssertionError</code> <p>If the length of eps_schedule is not 2 or 3.</p> <code>AssertionError</code> <p>If num_epochs is incompatible with eps_schedule.</p> Source code in <code>CTRAIN/train/certified/eps_scheduler.py</code> <pre><code>def __init__(self, num_epochs, eps, mean, std, start_eps=0, start_kappa=1, end_kappa=0, start_beta=0, end_beta=0, eps_schedule_unit='batch', eps_schedule=(0, 20), batches_per_epoch=None, start_epoch=-1):\n    \"\"\"\n    Initializes the Base EpsScheduler.\n\n    Args:\n        num_epochs (int): The number of epochs for training.\n        eps (float): The epsilon value for the scheduler.\n        mean (float): The mean value for normalization.\n        std (float): The standard deviation value for normalization.\n        start_eps (float, optional): The starting epsilon value. Defaults to 0.\n        start_kappa (float, optional): The starting kappa value. Defaults to 1.\n        end_kappa (float, optional): The ending kappa value. Defaults to 0.\n        start_beta (float, optional): The starting beta value. Defaults to 0.\n        end_beta (float, optional): The ending beta value. Defaults to 0.\n        eps_schedule_unit (str, optional): The unit for epsilon scheduling ('batch' or 'epoch'). Defaults to 'batch'.\n        eps_schedule (tuple, optional): The schedule for epsilon values. Defaults to (0, 20).\n        batches_per_epoch (int, optional): The number of batches per epoch. Defaults to None.\n        start_epoch (int, optional): The starting epoch number. Defaults to -1.\n\n    Raises:\n        AssertionError: If num_epochs is None and eps_schedule_unit is 'epoch'.\n        AssertionError: If the length of eps_schedule is not 2 or 3.\n        AssertionError: If num_epochs is incompatible with eps_schedule.\n    \"\"\"\n\n    if num_epochs is None and eps_schedule_unit=='epoch':\n        num_epochs = sum(eps_schedule)\n    elif num_epochs is None:\n        assert False, \"Please provide number of epochs!\"\n    if eps_schedule_unit=='epoch':\n        if len(eps_schedule) == 3:\n            assert num_epochs == sum(eps_schedule), \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n        elif len(eps_schedule)==2:\n            assert num_epochs &gt;= sum(eps_schedule), \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n        else:\n            assert False, \"Eps Schedule is incompatible with specified number of epochs. Please adjust!\"\n\n    self.num_epochs = num_epochs\n    if len(eps_schedule) == 2:\n        self.warm_up, self.ramp_up = eps_schedule\n    elif len(eps_schedule) == 3:\n        self.warm_up, self.ramp_up, _ = eps_schedule\n\n    print(self.warm_up, self.ramp_up)\n    self.cur_eps = start_eps\n    self.cur_kappa = self.start_kappa = start_kappa\n    self.end_kappa = end_kappa\n    self.eps = eps\n    self.start_eps = start_eps\n    self.batches_per_epoch = batches_per_epoch\n    self.start_beta = self.cur_beta = start_beta\n    self.end_beta = end_beta\n    self.mean = mean\n    self.std = std\n\n    if eps_schedule_unit == 'epoch':\n        self.warm_up *= batches_per_epoch\n        self.ramp_up *= batches_per_epoch\n\n    self.training_steps = num_epochs * batches_per_epoch\n\n    self.no_batches = 0\n\n    if start_epoch &gt; 0:\n        self.no_batches = self.batches_per_epoch * start_epoch - 1            \n</code></pre>"},{"location":"api/train/certified/eps_scheduler/#CTRAIN.train.certified.eps_scheduler.BaseScheduler.get_cur_eps","title":"<code>get_cur_eps(normalise=True)</code>","text":"<p>Get the current epsilon value, optionally normalised.</p> <p>Parameters:</p> Name Type Description Default <code>normalise</code> <code>bool</code> <p>If True, the returned epsilon value will be normalised by the standard deviation.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: The current epsilon value, normalised if specified.</p> Notes <ul> <li>The method checks for numerical instabilities and adjusts the current epsilon value if necessary.</li> </ul> Source code in <code>CTRAIN/train/certified/eps_scheduler.py</code> <pre><code>def get_cur_eps(self, normalise=True):\n    \"\"\"\n    Get the current epsilon value, optionally normalised.\n\n    Args:\n        normalise (bool): If True, the returned epsilon value will be normalised by the standard deviation.\n\n    Returns:\n        torch.Tensor: The current epsilon value, normalised if specified.\n\n    Notes:\n        - The method checks for numerical instabilities and adjusts the current epsilon value if necessary.\n    \"\"\"\n    # Check needed to mitigate numerical instabilities\n    if (torch.tensor(self.get_max_eps(normalise=False) - self.cur_eps)  &lt; 1e-7).all():\n        self.cur_eps = self.get_max_eps(normalise=False)\n    return torch.tensor(self.cur_eps) / torch.tensor(self.std) if normalise else self.cur_eps\n</code></pre>"},{"location":"api/train/certified/eps_scheduler/#CTRAIN.train.certified.eps_scheduler.LinearScheduler","title":"<code>LinearScheduler</code>","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>A scheduler that linearly adjusts epsilon, kappa, and beta values over a specified number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>Total number of epochs for training.</p> required <code>eps</code> <code>float</code> <p>The target epsilon value.</p> required <code>mean</code> <code>float</code> <p>The mean value for normalization.</p> required <code>std</code> <code>float</code> <p>The standard deviation value for normalization.</p> required <code>start_eps</code> <code>float</code> <p>The starting epsilon value. Defaults to 0.</p> <code>0</code> <code>start_kappa</code> <code>float</code> <p>The starting kappa value. Defaults to 1.</p> <code>1</code> <code>end_kappa</code> <code>float</code> <p>The ending kappa value. Defaults to 0.</p> <code>0</code> <code>start_beta</code> <code>float</code> <p>The starting beta value. Defaults to 1.</p> <code>1</code> <code>end_beta</code> <code>float</code> <p>The ending beta value. Defaults to 0.</p> <code>0</code> <code>eps_schedule_unit</code> <code>str</code> <p>The unit for epsilon scheduling ('batch' or 'epoch'). Defaults to 'batch'.</p> <code>'batch'</code> <code>eps_schedule</code> <code>tuple</code> <p>The schedule for epsilon adjustment. Defaults to (0, 20).</p> <code>(0, 20)</code> <code>batches_per_epoch</code> <code>int</code> <p>Number of batches per epoch. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>The epoch to start the scheduler. Defaults to -1.</p> <code>-1</code> <p>Methods:</p> Name Description <code>batch_step</code> <p>Adjusts the current epsilon, kappa, and beta values based on the current batch number.</p> Source code in <code>CTRAIN/train/certified/eps_scheduler.py</code> <pre><code>class LinearScheduler(BaseScheduler):\n    \"\"\"\n    A scheduler that linearly adjusts epsilon, kappa, and beta values over a specified number of epochs.\n\n    Args:\n        num_epochs (int): Total number of epochs for training.\n        eps (float): The target epsilon value.\n        mean (float): The mean value for normalization.\n        std (float): The standard deviation value for normalization.\n        start_eps (float, optional): The starting epsilon value. Defaults to 0.\n        start_kappa (float, optional): The starting kappa value. Defaults to 1.\n        end_kappa (float, optional): The ending kappa value. Defaults to 0.\n        start_beta (float, optional): The starting beta value. Defaults to 1.\n        end_beta (float, optional): The ending beta value. Defaults to 0.\n        eps_schedule_unit (str, optional): The unit for epsilon scheduling ('batch' or 'epoch'). Defaults to 'batch'.\n        eps_schedule (tuple, optional): The schedule for epsilon adjustment. Defaults to (0, 20).\n        batches_per_epoch (int, optional): Number of batches per epoch. Defaults to None.\n        start_epoch (int, optional): The epoch to start the scheduler. Defaults to -1.\n\n    Methods:\n        batch_step():\n            Adjusts the current epsilon, kappa, and beta values based on the current batch number.\n    \"\"\"\n    def __init__(self, num_epochs, eps, mean, std,start_eps=0, start_kappa=1, end_kappa=0, start_beta=1, end_beta=0, eps_schedule_unit='batch', eps_schedule=(0, 20), batches_per_epoch=None, start_epoch=-1):\n        super().__init__(\n            num_epochs=num_epochs, \n            eps=eps, \n            mean=mean, \n            std=std,\n            start_eps=start_eps, \n            start_kappa=start_kappa, \n            end_kappa=end_kappa, \n            eps_schedule_unit=eps_schedule_unit, \n            eps_schedule=eps_schedule, \n            batches_per_epoch=batches_per_epoch, \n            start_beta=start_beta, \n            end_beta=end_beta\n        )\n\n        if start_epoch &gt; 0:\n            self.batch_step()\n\n    def batch_step(self):\n        if self.warm_up &lt; self.no_batches &lt; (self.warm_up + self.ramp_up):\n            self.cur_eps += (self.eps / self.ramp_up)\n            kappa_step = (self.start_kappa - self.end_kappa) / self.ramp_up\n            self.cur_kappa -= kappa_step\n            beta_step = (self.start_beta - self.end_beta) / self.ramp_up\n            self.cur_beta -= beta_step\n        self.cur_eps = min(self.cur_eps, self.eps)\n        self.cur_kappa = max(self.cur_kappa, self.end_kappa)\n        self.no_batches += 1\n</code></pre>"},{"location":"api/train/certified/eps_scheduler/#CTRAIN.train.certified.eps_scheduler.SmoothedScheduler","title":"<code>SmoothedScheduler</code>","text":"<p>               Bases: <code>BaseScheduler</code></p> <p>A scheduler that smoothly transitions epsilon, kappa, and beta values over the course of training.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>Number of epochs for training.</p> required <code>eps</code> <code>float</code> <p>Final epsilon value.</p> required <code>mean</code> <code>float</code> <p>Mean value for normalization.</p> required <code>std</code> <code>float</code> <p>Standard deviation value for normalization.</p> required <code>start_eps</code> <code>float</code> <p>Initial epsilon value. Default is 0.</p> <code>0</code> <code>start_kappa</code> <code>float</code> <p>Initial kappa value. Default is 1.</p> <code>1</code> <code>end_kappa</code> <code>float</code> <p>Final kappa value. Default is 0.</p> <code>0</code> <code>start_beta</code> <code>float</code> <p>Initial beta value. Default is 1.</p> <code>1</code> <code>end_beta</code> <code>float</code> <p>Final beta value. Default is 0.</p> <code>0</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon scheduling ('batch' or 'epoch'). Default is 'batch'.</p> <code>'batch'</code> <code>batches_per_epoch</code> <code>int</code> <p>Number of batches per epoch. Required if eps_schedule_unit is 'batch'.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start the scheduling. Default is -1.</p> <code>-1</code> <code>eps_schedule</code> <code>tuple</code> <p>Tuple indicating the start and end of epsilon scheduling. Default is (0, 20).</p> <code>(0, 20)</code> <code>midpoint</code> <code>float</code> <p>Midpoint for the transition from exponential to linear schedule. Default is 0.25.</p> <code>0.25</code> <code>exponent</code> <code>float</code> <p>Exponent for the exponential schedule. Default is 4.0.</p> <code>4.0</code> <p>Methods:</p> Name Description <code>batch_step</code> <p>Updates the current epsilon, kappa, and beta values based on the current batch number.</p> Source code in <code>CTRAIN/train/certified/eps_scheduler.py</code> <pre><code>class SmoothedScheduler(BaseScheduler):\n    \"\"\"\n    A scheduler that smoothly transitions epsilon, kappa, and beta values over the course of training.\n\n    Args:\n        num_epochs (int): Number of epochs for training.\n        eps (float): Final epsilon value.\n        mean (float): Mean value for normalization.\n        std (float): Standard deviation value for normalization.\n        start_eps (float, optional): Initial epsilon value. Default is 0.\n        start_kappa (float, optional): Initial kappa value. Default is 1.\n        end_kappa (float, optional): Final kappa value. Default is 0.\n        start_beta (float, optional): Initial beta value. Default is 1.\n        end_beta (float, optional): Final beta value. Default is 0.\n        eps_schedule_unit (str, optional): Unit for epsilon scheduling ('batch' or 'epoch'). Default is 'batch'.\n        batches_per_epoch (int, optional): Number of batches per epoch. Required if eps_schedule_unit is 'batch'.\n        start_epoch (int, optional): Epoch to start the scheduling. Default is -1.\n        eps_schedule (tuple, optional): Tuple indicating the start and end of epsilon scheduling. Default is (0, 20).\n        midpoint (float, optional): Midpoint for the transition from exponential to linear schedule. Default is 0.25.\n        exponent (float, optional): Exponent for the exponential schedule. Default is 4.0.\n\n    Methods:\n        batch_step():\n            Updates the current epsilon, kappa, and beta values based on the current batch number.\n    \"\"\"\n    def __init__(self, num_epochs, eps, mean, std, start_eps=0, start_kappa=1, end_kappa=0, start_beta=1, end_beta=0, eps_schedule_unit='batch', batches_per_epoch=None, start_epoch=-1, eps_schedule=(0, 20), midpoint=.25, exponent=4.0):\n        super().__init__(\n            num_epochs=num_epochs, \n            eps=eps, \n            mean=mean,\n            std=std,\n            start_eps=start_eps, \n            start_kappa=start_kappa, \n            end_kappa=end_kappa, \n            eps_schedule_unit=eps_schedule_unit, \n            eps_schedule=eps_schedule, \n            batches_per_epoch=batches_per_epoch, \n            start_epoch=start_epoch,\n            start_beta=start_beta, \n            end_beta=end_beta)\n        self.midpoint = midpoint\n        self.exponent = exponent\n        if start_epoch &gt; 0:\n            self.batch_step()\n\n\n    def batch_step(self):\n        init_value = self.start_eps\n        final_value = self.eps\n        beta = self.exponent\n        step = self.no_batches\n        # Batch number for schedule start\n        init_step = self.warm_up + 1\n        # Batch number for schedule end\n        final_step = self.warm_up + self.ramp_up\n        # Batch number for switching from exponential to linear schedule\n        mid_step = int((final_step - init_step) * self.midpoint) + init_step\n        t = (mid_step - init_step) ** (beta - 1.)\n        # find coefficient for exponential growth, such that at mid point the gradient is the same as a linear ramp to final value\n        alpha = (final_value - init_value) / ((final_step - mid_step) * beta * t + (mid_step - init_step) * t)\n        # value at switching point\n        mid_value = init_value + alpha * (mid_step - init_step) ** beta\n        # return init_value when we have not started\n        is_ramp = float(step &gt; init_step)\n        # linear schedule after mid step\n        is_linear = float(step &gt;= mid_step)\n        exp_value = init_value + alpha * float(step - init_step) ** beta\n        linear_value = min(mid_value + (final_value - mid_value) * (step - mid_step) / (final_step - mid_step), final_value)\n        self.cur_eps = is_ramp * ((1.0 - is_linear) * exp_value + is_linear * linear_value) + (1.0 - is_ramp) * init_value\n        self.cur_kappa = self.start_kappa * (1 - (self.cur_eps * (1-self.end_kappa)) / self.eps)\n        self.cur_beta = self.start_beta * (1 - (self.cur_eps * (1-self.end_beta)) / self.eps)\n        self.cur_eps = min(self.cur_eps, self.eps)\n        self.cur_kappa = max(self.cur_kappa, self.end_kappa)\n        self.cur_beta = max(self.cur_beta, self.end_beta)\n        self.no_batches += 1\n</code></pre>"},{"location":"api/train/certified/ibp/","title":"IBP","text":""},{"location":"api/train/certified/ibp/#CTRAIN.train.certified.shi_ibp.shi_train_model","title":"<code>shi_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, lr_decay_schedule=(15, 25), lr_decay_factor=0.2, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, l1_regularisation_weight=1e-05, shi_regularisation_weight=1, shi_reg_decay=True, results_path='./results', checkpoint_save_interval=10, device='cuda')</code>","text":"<p>Train a model using the Shi-IBP method for certified robustness.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be trained.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Epsilon value for perturbation. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon scheduling ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for the epsilon scheduler. Defaults to dict().</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training. Defaults to None.</p> <code>None</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to 0.2.</p> <code>0.2</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay scheduling ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularization. Defaults to 0.00001.</p> <code>1e-05</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for SHI regularization. Defaults to 1.</p> <code>1</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization. Defaults to True.</p> <code>True</code> <code>results_path</code> <code>str</code> <p>Path to save the training results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained hardened model.</p> Source code in <code>CTRAIN/train/certified/shi_ibp.py</code> <pre><code>def shi_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=0.2,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    l1_regularisation_weight=0.00001,\n    shi_regularisation_weight=1,\n    shi_reg_decay=True,\n    results_path=\"./results\",\n    checkpoint_save_interval=10,\n    device=\"cuda\",\n):\n    \"\"\"\n    Train a model using the Shi-IBP method for certified robustness.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be trained.\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Epsilon value for perturbation. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon scheduling ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for the epsilon scheduler. Defaults to dict().\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training. Defaults to None.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to 0.2.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay scheduling ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        l1_regularisation_weight (float, optional): Weight for L1 regularization. Defaults to 0.00001.\n        shi_regularisation_weight (float, optional): Weight for SHI regularization. Defaults to 1.\n        shi_reg_decay (bool, optional): Whether to decay SHI regularization. Defaults to True.\n        results_path (str, optional): Path to save the training results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): The trained hardened model.\n    \"\"\"\n\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction='none')\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    # Important Change to Vanilla IBP: Schedule Eps smoothly\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    if start_epoch == 0:\n        # Important Change to Vanilla IBP: Initialise Weights to normal distribution with sigma_i = sqrt(2*pi)/n_i, for layer i and fan in n_i\n        ibp_init_shi(original_model, hardened_model)\n\n    cur_eps, kappa = eps_scheduler.get_cur_eps(), eps_scheduler.get_cur_kappa()\n\n    # Training loop\n    for epoch in range(start_epoch, end_epoch):\n\n        epoch_rob_err = 0\n        epoch_nat_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}, kappa {kappa:.2f} \"\n        )\n        hardened_model.train()\n        original_model.train()\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n            kappa = eps_scheduler.get_cur_kappa()\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            clean_output = hardened_model(data)\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n            clean_loss = criterion(clean_output, target).mean()\n\n            if eps_scheduler.get_cur_eps(normalise=False) != 0.0:\n\n                ibp_loss, robust_err = get_ibp_loss(\n                    hardened_model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    criterion=criterion,\n                    return_bounds=False,\n                    return_stats=True,\n                )\n\n                epoch_rob_err += robust_err\n\n                loss = kappa * clean_loss + (1 - kappa) * ibp_loss\n\n            else:\n                loss = clean_loss\n\n            if eps_scheduler.get_cur_eps(normalise=False) != eps_scheduler.get_max_eps(\n                normalise=False\n            ):\n                # Important Change to Vanilla IBP: Regularise Unstable ReLUs and Bound Tightness during Warm Up/Ramp Up\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n                # print(loss_regularisers)\n\n                loss_regularisers = shi_regularisation_weight * loss_regularisers\n                loss = loss + loss_regularisers\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss += l1_regularisation\n\n            loss.backward()\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: N/A\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/mtl_ibp/","title":"MTL-IBP","text":""},{"location":"api/train/certified/mtl_ibp/#CTRAIN.train.certified.mtl_ibp.mtl_ibp_train_model","title":"<code>mtl_ibp_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, lr_decay_schedule=(15, 25), lr_decay_factor=0.2, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, shi_regularisation_weight=0.5, shi_reg_decay=True, l1_regularisation_weight=1e-05, alpha=0.5, pgd_restarts=1, pgd_step_size=10, pgd_n_steps=1, pgd_eps_factor=1, pgd_decay_factor=0.1, pgd_decay_checkpoints=(), pgd_early_stopping=False, results_path='./results', checkpoint_save_interval=10, device='cuda')</code>","text":"<p>Trains a model using the MTL-IBP method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be trained.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Epsilon value for perturbation. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for epsilon scheduler. Defaults to dict().</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training. Defaults to None.</p> <code>None</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to .2.</p> <code>0.2</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for SHI regularisation. Defaults to 0.5.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularisation. Defaults to True.</p> <code>True</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularisation. Defaults to 0.00001.</p> <code>1e-05</code> <code>alpha</code> <code>float</code> <p>Alpha value for loss calculation. Defaults to 0.5.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of restarts for PGD. Defaults to 1.</p> <code>1</code> <code>pgd_step_size</code> <code>int</code> <p>Step size for PGD. Defaults to 10.</p> <code>10</code> <code>pgd_n_steps</code> <code>int</code> <p>Number of steps for PGD. Defaults to 1.</p> <code>1</code> <code>pgd_eps_factor</code> <code>float</code> <p>Factor for PGD epsilon. Defaults to 1.</p> <code>1</code> <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD. Defaults to 0.1.</p> <code>0.1</code> <code>pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay. Defaults to ().</p> <code>()</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping for PGD. Defaults to False.</p> <code>False</code> <code>results_path</code> <code>str</code> <p>Path to save the results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained hardened model.</p> Source code in <code>CTRAIN/train/certified/mtl_ibp.py</code> <pre><code>def mtl_ibp_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=0.2,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    shi_regularisation_weight=0.5,\n    shi_reg_decay=True,\n    l1_regularisation_weight=0.00001,\n    alpha=0.5,\n    pgd_restarts=1,\n    pgd_step_size=10,\n    pgd_n_steps=1,\n    pgd_eps_factor=1,\n    pgd_decay_factor=0.1,\n    pgd_decay_checkpoints=(),\n    pgd_early_stopping=False,\n    results_path=\"./results\",\n    checkpoint_save_interval=10,\n    device=\"cuda\",\n):\n    \"\"\"\n    Trains a model using the MTL-IBP method.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be trained.\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Epsilon value for perturbation. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for epsilon scheduler. Defaults to dict().\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training. Defaults to None.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to .2.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        shi_regularisation_weight (float, optional): Weight for SHI regularisation. Defaults to 0.5.\n        shi_reg_decay (bool, optional): Whether to decay SHI regularisation. Defaults to True.\n        l1_regularisation_weight (float, optional): Weight for L1 regularisation. Defaults to 0.00001.\n        alpha (float, optional): Alpha value for loss calculation. Defaults to 0.5.\n        pgd_restarts (int, optional): Number of restarts for PGD. Defaults to 1.\n        pgd_step_size (int, optional): Step size for PGD. Defaults to 10.\n        pgd_n_steps (int, optional): Number of steps for PGD. Defaults to 1.\n        pgd_eps_factor (float, optional): Factor for PGD epsilon. Defaults to 1.\n        pgd_decay_factor (float, optional): Decay factor for PGD. Defaults to 0.1.\n        pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay. Defaults to ().\n        pgd_early_stopping (bool, optional): Whether to use early stopping for PGD. Defaults to False.\n        results_path (str, optional): Path to save the results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): The trained hardened model.\n    \"\"\"\n\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction='none')\n\n    if start_epoch == 0:\n        ibp_init_shi(original_model, hardened_model)\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    cur_eps = eps_scheduler.get_cur_eps()\n\n    for epoch in range(start_epoch, end_epoch):\n\n        epoch_adv_err = 0\n        epoch_rob_err = 0\n        epoch_nat_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}\"\n        )\n        hardened_model.train()\n        original_model.train()\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n\n            clean_output = hardened_model(data)\n            clean_loss = criterion(clean_output, target).mean()\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n\n            if eps_scheduler.get_cur_eps(normalise=False) != 0.0:\n\n                if pgd_eps_factor == 1:\n                    pgd_ptb = ptb\n                else:\n                    pgd_eps = (eps_std * pgd_eps_factor).to(device)\n                    data_min, data_max = train_loader.min.to(\n                        device\n                    ), train_loader.max.to(device)\n                    pgd_ptb = PerturbationLpNorm(\n                        eps=pgd_eps,\n                        norm=np.inf,\n                        x_L=torch.clamp(data - pgd_eps, data_min, data_max).to(device),\n                        x_U=torch.clamp(data + pgd_eps, data_min, data_max).to(device),\n                    )\n\n                loss, robust_err, adv_err = get_mtl_ibp_loss(\n                    hardened_model=hardened_model,\n                    original_model=original_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    criterion=criterion,\n                    alpha=alpha,\n                    return_bounds=False,\n                    return_stats=True,\n                    restarts=pgd_restarts,\n                    step_size=pgd_step_size,\n                    n_steps=pgd_n_steps,\n                    pgd_ptb=pgd_ptb,\n                    early_stopping=pgd_early_stopping,\n                    decay_checkpoints=pgd_decay_checkpoints,\n                    decay_factor=pgd_decay_factor,\n                    device=device,\n                )\n                epoch_adv_err += adv_err\n                epoch_rob_err += robust_err\n\n            else:\n                loss = clean_loss\n\n            if eps_scheduler.get_cur_eps(normalise=False) != eps_scheduler.get_max_eps(\n                normalise=False\n            ):\n                # Important Change to Vanilla IBP: Regularise Unstable ReLUs and Bound Tightness during Warm Up/Ramp Up\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n\n                loss_regularisers = (shi_regularisation_weight * loss_regularisers).to(\n                    device\n                )\n                loss = loss + loss_regularisers\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss = loss + l1_regularisation\n\n            loss.backward()\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_adv = 1 - epoch_adv_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: {train_acc_adv:.4f}\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/sabr/","title":"SABR","text":""},{"location":"api/train/certified/sabr/#CTRAIN.train.certified.sabr.sabr_train_model","title":"<code>sabr_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, subselection_ratio=0.4, lr_decay_schedule=(15, 25), lr_decay_factor=0.2, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, l1_regularisation_weight=1e-05, shi_regularisation_weight=1, shi_reg_decay=True, pgd_steps=8, pgd_step_size=0.5, pgd_restarts=1, pgd_early_stopping=True, pgd_decay_factor=0.1, pgd_decay_checkpoints=(4, 7), results_path='./results', checkpoint_save_interval=10, device='cuda')</code>","text":"<p>Trains a model using the SABR method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be hardened.</p> required <code>hardened_model</code> <code>Module</code> <p>The model to be trained with SABR.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Initial epsilon value for adversarial perturbations. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon scheduling ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for the epsilon scheduler. Defaults to an empty dictionary.</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training the model. Defaults to None.</p> <code>None</code> <code>subselection_ratio</code> <code>float</code> <p>Ratio for subselection in SABR. Defaults to 0.4.</p> <code>0.4</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to .2.</p> <code>0.2</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay scheduling ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularization. Defaults to 0.00001.</p> <code>1e-05</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for Shi regularization. Defaults to 1.</p> <code>1</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay Shi regularization. Defaults to True.</p> <code>True</code> <code>pgd_steps</code> <code>int</code> <p>Number of steps for PGD (Projected Gradient Descent). Defaults to 8.</p> <code>8</code> <code>pgd_step_size</code> <code>float</code> <p>Step size for PGD. Defaults to 0.5.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of restarts for PGD. Defaults to 1.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping for PGD. Defaults to True.</p> <code>True</code> <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD. Defaults to 0.1.</p> <code>0.1</code> <code>pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay. Defaults to (4, 7).</p> <code>(4, 7)</code> <code>results_path</code> <code>str</code> <p>Path to save the training results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained hardened model.</p> Source code in <code>CTRAIN/train/certified/sabr.py</code> <pre><code>def sabr_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    subselection_ratio=0.4,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=0.2,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    l1_regularisation_weight=0.00001,\n    shi_regularisation_weight=1,\n    shi_reg_decay=True,\n    pgd_steps=8,\n    pgd_step_size=0.5,\n    pgd_restarts=1,\n    pgd_early_stopping=True,\n    pgd_decay_factor=0.1,\n    pgd_decay_checkpoints=(4, 7),\n    results_path=\"./results\",\n    checkpoint_save_interval=10,\n    device=\"cuda\",\n):\n    \"\"\"\n    Trains a model using the SABR method.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be hardened.\n        hardened_model (torch.nn.Module): The model to be trained with SABR.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Initial epsilon value for adversarial perturbations. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon scheduling ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for the epsilon scheduler. Defaults to an empty dictionary.\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training the model. Defaults to None.\n        subselection_ratio (float, optional): Ratio for subselection in SABR. Defaults to 0.4.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to .2.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay scheduling ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        l1_regularisation_weight (float, optional): Weight for L1 regularization. Defaults to 0.00001.\n        shi_regularisation_weight (float, optional): Weight for Shi regularization. Defaults to 1.\n        shi_reg_decay (bool, optional): Whether to decay Shi regularization. Defaults to True.\n        pgd_steps (int, optional): Number of steps for PGD (Projected Gradient Descent). Defaults to 8.\n        pgd_step_size (float, optional): Step size for PGD. Defaults to 0.5.\n        pgd_restarts (int, optional): Number of restarts for PGD. Defaults to 1.\n        pgd_early_stopping (bool, optional): Whether to use early stopping for PGD. Defaults to True.\n        pgd_decay_factor (float, optional): Decay factor for PGD. Defaults to 0.1.\n        pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay. Defaults to (4, 7).\n        results_path (str, optional): Path to save the training results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): The trained hardened model.\n    \"\"\"\n\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n\n    if start_epoch == 0:\n        # Important Change to Vanilla IBP: Initialise Weights to normal distribution with sigma_i = sqrt(2*pi)/n_i, for layer i and fan in n_i\n        ibp_init_shi(original_model, hardened_model)\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    # Important Change to Vanilla IBP: Schedule Eps smoothly\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    cur_eps = eps_scheduler.get_cur_eps()\n    # Training loop\n    for epoch in range(start_epoch, end_epoch):\n\n        epoch_adv_err = 0\n        epoch_rob_err = 0\n        epoch_nat_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}\"\n        )\n        hardened_model.train()\n        original_model.train()\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            clean_output = hardened_model(data)\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n            if eps_scheduler.get_cur_eps(normalise=False) != 0.0:\n\n                sabr_loss, robust_err, adv_err = get_sabr_loss(\n                    hardened_model=hardened_model,\n                    original_model=original_model,\n                    train_loader=train_loader,\n                    data=data,\n                    data_min=train_loader.min.to(device),\n                    data_max=train_loader.max.to(device),\n                    target=target,\n                    eps=torch.tensor(cur_eps, device=device),\n                    subselection_ratio=subselection_ratio,\n                    criterion=criterion,\n                    device=device,\n                    n_classes=n_classes,\n                    pgd_steps=pgd_steps,\n                    pgd_step_size=pgd_step_size,\n                    pgd_restarts=pgd_restarts,\n                    pgd_early_stopping=pgd_early_stopping,\n                    pgd_decay_factor=pgd_decay_factor,\n                    pgd_decay_checkpoints=pgd_decay_checkpoints,\n                    return_stats=True,\n                )\n\n                epoch_adv_err += adv_err\n                epoch_rob_err += robust_err\n\n                loss = sabr_loss\n\n            else:\n                loss = criterion(clean_output, target).mean()\n\n            if eps_scheduler.get_cur_eps(normalise=False) != eps_scheduler.get_max_eps(\n                normalise=False\n            ):\n                # SABR also uses Shi regularisation during warm up/ramp-up\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n                loss = loss + shi_regularisation_weight * loss_regularisers\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss = loss + l1_regularisation\n\n            loss.backward()\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_adv = 1 - epoch_adv_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: {train_acc_adv:.4f}\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/staps/","title":"STAPS","text":""},{"location":"api/train/certified/staps/#CTRAIN.train.certified.staps.staps_train_model","title":"<code>staps_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, subselection_ratio=0.4, lr_decay_schedule=(15, 25), lr_decay_factor=10, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, l1_regularisation_weight=1e-05, shi_regularisation_weight=0.5, shi_reg_decay=True, gradient_expansion_alpha=5.0, sabr_pgd_steps=8, sabr_pgd_step_size=0.5, sabr_pgd_restarts=1, sabr_pgd_early_stopping=True, sabr_pgd_decay_factor=0.1, sabr_pgd_decay_checkpoints=(4, 7), taps_pgd_steps=20, taps_pgd_step_size=None, taps_pgd_restarts=1, taps_pgd_decay_factor=0.2, taps_pgd_decay_checkpoints=(5, 7), taps_gradient_link_thresh=0.5, taps_gradient_link_tolerance=1e-05, results_path='./results', checkpoint_save_interval=10, device='cuda')</code>","text":"<p>Trains a hardened model using the STAPS training method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be hardened.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Epsilon value for perturbation. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for the epsilon scheduler. Defaults to dict().</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training. Defaults to None.</p> <code>None</code> <code>subselection_ratio</code> <code>float</code> <p>Ratio for subselection in SABR loss. Defaults to 0.4.</p> <code>0.4</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to 10.</p> <code>10</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularization. Defaults to 0.00001.</p> <code>1e-05</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for SHI regularization. Defaults to 0.5.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization. Defaults to True.</p> <code>True</code> <code>gradient_expansion_alpha</code> <code>float</code> <p>Alpha value for gradient expansion. Defaults to 5.</p> <code>5.0</code> <code>sabr_pgd_steps</code> <code>int</code> <p>Number of PGD steps for SABR loss. Defaults to 8.</p> <code>8</code> <code>sabr_pgd_step_size</code> <code>float</code> <p>Step size for PGD in SABR loss. Defaults to 0.5.</p> <code>0.5</code> <code>sabr_pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for SABR loss. Defaults to 1.</p> <code>1</code> <code>sabr_pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD for SABR loss. Defaults to True.</p> <code>True</code> <code>sabr_pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD in SABR loss. Defaults to 0.1.</p> <code>0.1</code> <code>sabr_pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay in SABR loss. Defaults to (4, 7).</p> <code>(4, 7)</code> <code>taps_pgd_steps</code> <code>int</code> <p>Number of PGD steps for TAPS loss. Defaults to 20.</p> <code>20</code> <code>taps_pgd_step_size</code> <code>float</code> <p>Step size for PGD in TAPS loss. Defaults to None.</p> <code>None</code> <code>taps_pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for TAPS loss. Defaults to 1.</p> <code>1</code> <code>taps_pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD in TAPS loss. Defaults to 0.2.</p> <code>0.2</code> <code>taps_pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay in TAPS loss. Defaults to (5, 7).</p> <code>(5, 7)</code> <code>taps_gradient_link_thresh</code> <code>float</code> <p>Threshold for gradient linking in TAPS loss. Defaults to 0.5.</p> <code>0.5</code> <code>taps_gradient_link_tolerance</code> <code>float</code> <p>Tolerance for gradient linking in TAPS loss. Defaults to 0.00001.</p> <code>1e-05</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>results_path</code> <code>str</code> <p>Path to save training results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained bounded model.</p> Source code in <code>CTRAIN/train/certified/staps.py</code> <pre><code>def staps_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    subselection_ratio=0.4,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=10,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    l1_regularisation_weight=0.00001,\n    shi_regularisation_weight=0.5,\n    shi_reg_decay=True,\n    gradient_expansion_alpha=5.0,\n    sabr_pgd_steps=8,\n    sabr_pgd_step_size=0.5,\n    sabr_pgd_restarts=1,\n    sabr_pgd_early_stopping=True,\n    sabr_pgd_decay_factor=0.1,\n    sabr_pgd_decay_checkpoints=(4, 7),\n    taps_pgd_steps=20,\n    taps_pgd_step_size=None,\n    taps_pgd_restarts=1,\n    taps_pgd_decay_factor=0.2,\n    taps_pgd_decay_checkpoints=(5, 7),\n    taps_gradient_link_thresh=0.5,\n    taps_gradient_link_tolerance=0.00001,\n    results_path=\"./results\",\n    checkpoint_save_interval=10,\n    device=\"cuda\",\n):\n    \"\"\"\n    Trains a hardened model using the STAPS training method.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be hardened.\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Epsilon value for perturbation. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for the epsilon scheduler. Defaults to dict().\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training. Defaults to None.\n        subselection_ratio (float, optional): Ratio for subselection in SABR loss. Defaults to 0.4.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to 10.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        l1_regularisation_weight (float, optional): Weight for L1 regularization. Defaults to 0.00001.\n        shi_regularisation_weight (float, optional): Weight for SHI regularization. Defaults to 0.5.\n        shi_reg_decay (bool, optional): Whether to decay SHI regularization. Defaults to True.\n        gradient_expansion_alpha (float, optional): Alpha value for gradient expansion. Defaults to 5.\n        sabr_pgd_steps (int, optional): Number of PGD steps for SABR loss. Defaults to 8.\n        sabr_pgd_step_size (float, optional): Step size for PGD in SABR loss. Defaults to 0.5.\n        sabr_pgd_restarts (int, optional): Number of PGD restarts for SABR loss. Defaults to 1.\n        sabr_pgd_early_stopping (bool, optional): Whether to use early stopping in PGD for SABR loss. Defaults to True.\n        sabr_pgd_decay_factor (float, optional): Decay factor for PGD in SABR loss. Defaults to 0.1.\n        sabr_pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay in SABR loss. Defaults to (4, 7).\n        taps_pgd_steps (int, optional): Number of PGD steps for TAPS loss. Defaults to 20.\n        taps_pgd_step_size (float, optional): Step size for PGD in TAPS loss. Defaults to None.\n        taps_pgd_restarts (int, optional): Number of PGD restarts for TAPS loss. Defaults to 1.\n        taps_pgd_decay_factor (float, optional): Decay factor for PGD in TAPS loss. Defaults to 0.2.\n        taps_pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay in TAPS loss. Defaults to (5, 7).\n        taps_gradient_link_thresh (float, optional): Threshold for gradient linking in TAPS loss. Defaults to 0.5.\n        taps_gradient_link_tolerance (float, optional): Tolerance for gradient linking in TAPS loss. Defaults to 0.00001.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        results_path (str, optional): Path to save training results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (autoLiRPA.BoundedModule): The trained bounded model.\n    \"\"\"\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    if start_epoch == 0:\n        ibp_init_shi(original_model, hardened_model)\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    cur_eps = eps_scheduler.get_cur_eps()\n\n    for epoch in range(start_epoch, end_epoch):\n\n        if start_epoch &gt; epoch:\n            continue\n\n        epoch_nat_err = 0\n        epoch_rob_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}\"\n        )\n\n        for block in hardened_model.bounded_blocks:\n            block.train()\n\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            if start_epoch &gt; epoch:\n                eps_scheduler.batch_step()\n                continue\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            clean_output = hardened_model(data)\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n            clean_loss = criterion(clean_output, target).mean()\n\n            if eps_scheduler.get_cur_eps(normalise=False) == 0.0:\n                loss = clean_loss\n            elif eps_scheduler.get_cur_eps(normalise=False) != 0.0 and (\n                eps_scheduler.get_cur_eps(normalise=False)\n                != eps_scheduler.get_max_eps(normalise=False)\n            ):\n                reg_loss, robust_err, adv_err = get_sabr_loss(\n                    hardened_model=hardened_model,\n                    original_model=original_model,\n                    data_max=train_loader.max.to(device),\n                    data_min=train_loader.min.to(device),\n                    data=data,\n                    target=target,\n                    eps=torch.tensor(cur_eps, device=device),\n                    subselection_ratio=subselection_ratio,\n                    criterion=criterion,\n                    device=device,\n                    n_classes=n_classes,\n                    pgd_steps=sabr_pgd_steps,\n                    pgd_step_size=sabr_pgd_step_size,\n                    pgd_restarts=sabr_pgd_restarts,\n                    pgd_early_stopping=sabr_pgd_early_stopping,\n                    pgd_decay_checkpoints=sabr_pgd_decay_checkpoints,\n                    pgd_decay_factor=sabr_pgd_decay_factor,\n                    return_stats=True,\n                )\n\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n                epoch_rob_err += robust_err\n                loss_regularisers = shi_regularisation_weight * loss_regularisers\n                loss = reg_loss + loss_regularisers\n\n            elif eps_scheduler.get_cur_eps(\n                normalise=False\n            ) == eps_scheduler.get_max_eps(normalise=False):\n                sabr_args = dict(\n                    hardened_model=hardened_model,\n                    original_model=original_model,\n                    data_max=train_loader.max.to(device),\n                    data_min=train_loader.min.to(device),\n                    data=data,\n                    target=target,\n                    eps=torch.tensor(cur_eps, device=device),\n                    subselection_ratio=subselection_ratio,\n                    device=device,\n                    n_classes=n_classes,\n                    n_steps=sabr_pgd_steps,\n                    step_size=sabr_pgd_step_size,\n                    restarts=sabr_pgd_restarts,\n                    early_stopping=sabr_pgd_early_stopping,\n                    decay_checkpoints=sabr_pgd_decay_checkpoints,\n                    decay_factor=sabr_pgd_decay_factor,\n                )\n                loss, robust_err = get_taps_loss(\n                    original_model=original_model,\n                    hardened_model=hardened_model,\n                    bounded_blocks=hardened_model.bounded_blocks,\n                    criterion=criterion,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    ptb=ptb,\n                    device=device,\n                    pgd_steps=taps_pgd_steps,\n                    pgd_restarts=taps_pgd_restarts,\n                    pgd_step_size=taps_pgd_step_size,\n                    pgd_decay_checkpoints=taps_pgd_decay_checkpoints,\n                    pgd_decay_factor=taps_pgd_decay_factor,\n                    gradient_link_thresh=taps_gradient_link_thresh,\n                    gradient_link_tolerance=taps_gradient_link_tolerance,\n                    gradient_expansion_alpha=gradient_expansion_alpha,\n                    propagation=\"SABR\",\n                    sabr_args=sabr_args,\n                    return_stats=True,\n                )\n\n                epoch_rob_err += robust_err\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss += l1_regularisation\n\n            loss.backward()\n\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: N/A\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/taps/","title":"TAPS","text":""},{"location":"api/train/certified/taps/#CTRAIN.train.certified.taps.taps_train_model","title":"<code>taps_train_model(original_model, hardened_model, train_loader, val_loader=None, start_epoch=0, end_epoch=None, num_epochs=None, eps=0.3, eps_std=0.3, eps_schedule=(0, 20, 50), eps_schedule_unit='epoch', eps_scheduler_args=dict(), optimizer=None, lr_decay_schedule=(15, 25), lr_decay_factor=10, lr_decay_schedule_unit='epoch', n_classes=10, gradient_clip=None, l1_regularisation_weight=1e-05, shi_regularisation_weight=0.5, shi_reg_decay=True, gradient_expansion_alpha=5.0, taps_pgd_steps=20, taps_pgd_step_size=None, taps_pgd_restarts=1, taps_pgd_decay_factor=0.2, taps_pgd_decay_checkpoints=(5, 7), taps_gradient_link_thresh=0.5, taps_gradient_link_tolerance=1e-05, checkpoint_save_interval=10, results_path='./results', device='cuda')</code>","text":"<p>Train a model using the TAPS (Training with Adversarial Perturbations and Smoothing) method.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>The original model to be trained.</p> required <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for the training data.</p> required <code>val_loader</code> <code>DataLoader</code> <p>DataLoader for the validation data. Defaults to None.</p> <code>None</code> <code>start_epoch</code> <code>int</code> <p>Epoch to start training from. Defaults to 0.</p> <code>0</code> <code>end_epoch</code> <code>int</code> <p>Epoch to prematurely end training at. Defaults to None.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to None.</p> <code>None</code> <code>eps</code> <code>float</code> <p>Epsilon value for perturbation. Defaults to 0.3.</p> <code>0.3</code> <code>eps_std</code> <code>float</code> <p>Standardised epsilon value. Defaults to 0.3.</p> <code>0.3</code> <code>eps_schedule</code> <code>tuple</code> <p>Schedule for epsilon values. Defaults to (0, 20, 50).</p> <code>(0, 20, 50)</code> <code>eps_schedule_unit</code> <code>str</code> <p>Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>eps_scheduler_args</code> <code>dict</code> <p>Additional arguments for the epsilon scheduler. Defaults to dict().</p> <code>dict()</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training. Defaults to None.</p> <code>None</code> <code>lr_decay_schedule</code> <code>tuple</code> <p>Schedule for learning rate decay. Defaults to (15, 25).</p> <code>(15, 25)</code> <code>lr_decay_factor</code> <code>float</code> <p>Factor by which to decay the learning rate. Defaults to 10.</p> <code>10</code> <code>lr_decay_schedule_unit</code> <code>str</code> <p>Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.</p> <code>'epoch'</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset. Defaults to 10.</p> <code>10</code> <code>gradient_clip</code> <code>float</code> <p>Value for gradient clipping. Defaults to None.</p> <code>None</code> <code>l1_regularisation_weight</code> <code>float</code> <p>Weight for L1 regularization. Defaults to 0.00001.</p> <code>1e-05</code> <code>shi_regularisation_weight</code> <code>float</code> <p>Weight for SHI regularization. Defaults to 0.5.</p> <code>0.5</code> <code>shi_reg_decay</code> <code>bool</code> <p>Whether to decay SHI regularization. Defaults to True.</p> <code>True</code> <code>gradient_expansion_alpha</code> <code>float</code> <p>Alpha value for gradient expansion. Defaults to 5.</p> <code>5.0</code> <code>taps_pgd_steps</code> <code>int</code> <p>Number of PGD steps for TAPS. Defaults to 20.</p> <code>20</code> <code>taps_pgd_step_size</code> <code>float</code> <p>Step size for PGD in TAPS. Defaults to None.</p> <code>None</code> <code>taps_pgd_restarts</code> <code>int</code> <p>Number of PGD restarts for TAPS. Defaults to 1.</p> <code>1</code> <code>taps_pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD in TAPS. Defaults to 0.2.</p> <code>0.2</code> <code>taps_pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay in TAPS. Defaults to (5, 7).</p> <code>(5, 7)</code> <code>taps_gradient_link_thresh</code> <code>float</code> <p>Threshold for gradient linking in TAPS. Defaults to 0.5.</p> <code>0.5</code> <code>taps_gradient_link_tolerance</code> <code>float</code> <p>Tolerance for gradient linking in TAPS. Defaults to 0.00001.</p> <code>1e-05</code> <code>start_epoch</code> <code>int</code> <p>Starting epoch for training. Defaults to 0.</p> <code>0</code> <code>results_path</code> <code>str</code> <p>Path to save training results. Defaults to \"./results\".</p> <code>'./results'</code> <code>checkpoint_save_interval</code> <code>int</code> <p>Interval for saving checkpoints. Defaults to 10.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>BoundedModule</code> <p>The trained hardened model.</p> Source code in <code>CTRAIN/train/certified/taps.py</code> <pre><code>def taps_train_model(\n    original_model,\n    hardened_model,\n    train_loader,\n    val_loader=None,\n    start_epoch=0,\n    end_epoch=None,\n    num_epochs=None,\n    eps=0.3,\n    eps_std=0.3,\n    eps_schedule=(0, 20, 50),\n    eps_schedule_unit=\"epoch\",\n    eps_scheduler_args=dict(),\n    optimizer=None,\n    lr_decay_schedule=(15, 25),\n    lr_decay_factor=10,\n    lr_decay_schedule_unit=\"epoch\",\n    n_classes=10,\n    gradient_clip=None,\n    l1_regularisation_weight=0.00001,\n    shi_regularisation_weight=0.5,\n    shi_reg_decay=True,\n    gradient_expansion_alpha=5.0,\n    taps_pgd_steps=20,\n    taps_pgd_step_size=None,\n    taps_pgd_restarts=1,\n    taps_pgd_decay_factor=0.2,\n    taps_pgd_decay_checkpoints=(5, 7),\n    taps_gradient_link_thresh=0.5,\n    taps_gradient_link_tolerance=0.00001,\n    checkpoint_save_interval=10,\n    results_path=\"./results\",\n    device=\"cuda\",\n):\n    \"\"\"\n    Train a model using the TAPS (Training with Adversarial Perturbations and Smoothing) method.\n\n    Args:\n        original_model (torch.nn.Module): The original model to be trained.\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n        val_loader (torch.utils.data.DataLoader, optional): DataLoader for the validation data. Defaults to None.\n        start_epoch (int, optional): Epoch to start training from. Defaults to 0.\n        end_epoch (int, optional): Epoch to prematurely end training at. Defaults to None.\n        num_epochs (int, optional): Number of epochs to train the model. Defaults to None.\n        eps (float, optional): Epsilon value for perturbation. Defaults to 0.3.\n        eps_std (float, optional): Standardised epsilon value. Defaults to 0.3.\n        eps_schedule (tuple, optional): Schedule for epsilon values. Defaults to (0, 20, 50).\n        eps_schedule_unit (str, optional): Unit for epsilon schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        eps_scheduler_args (dict, optional): Additional arguments for the epsilon scheduler. Defaults to dict().\n        optimizer (torch.optim.Optimizer, optional): Optimizer for training. Defaults to None.\n        lr_decay_schedule (tuple, optional): Schedule for learning rate decay. Defaults to (15, 25).\n        lr_decay_factor (float, optional): Factor by which to decay the learning rate. Defaults to 10.\n        lr_decay_schedule_unit (str, optional): Unit for learning rate decay schedule ('epoch' or 'batch'). Defaults to 'epoch'.\n        n_classes (int, optional): Number of classes in the dataset. Defaults to 10.\n        gradient_clip (float, optional): Value for gradient clipping. Defaults to None.\n        l1_regularisation_weight (float, optional): Weight for L1 regularization. Defaults to 0.00001.\n        shi_regularisation_weight (float, optional): Weight for SHI regularization. Defaults to 0.5.\n        shi_reg_decay (bool, optional): Whether to decay SHI regularization. Defaults to True.\n        gradient_expansion_alpha (float, optional): Alpha value for gradient expansion. Defaults to 5.\n        taps_pgd_steps (int, optional): Number of PGD steps for TAPS. Defaults to 20.\n        taps_pgd_step_size (float, optional): Step size for PGD in TAPS. Defaults to None.\n        taps_pgd_restarts (int, optional): Number of PGD restarts for TAPS. Defaults to 1.\n        taps_pgd_decay_factor (float, optional): Decay factor for PGD in TAPS. Defaults to 0.2.\n        taps_pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay in TAPS. Defaults to (5, 7).\n        taps_gradient_link_thresh (float, optional): Threshold for gradient linking in TAPS. Defaults to 0.5.\n        taps_gradient_link_tolerance (float, optional): Tolerance for gradient linking in TAPS. Defaults to 0.00001.\n        start_epoch (int, optional): Starting epoch for training. Defaults to 0.\n        results_path (str, optional): Path to save training results. Defaults to \"./results\".\n        checkpoint_save_interval (int, optional): Interval for saving checkpoints. Defaults to 10.\n        device (str, optional): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n\n    Returns:\n        (auto_LiRPA.BoundedModule): The trained hardened model.\n    \"\"\"\n    if end_epoch is None:\n        end_epoch = num_epochs\n\n    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n    if start_epoch == 0:\n        ibp_init_shi(original_model, hardened_model)\n\n    no_batches = 0\n    cur_lr = optimizer.param_groups[-1][\"lr\"]\n\n    # Important Change to Vanilla IBP: Schedule Eps smoothly\n    eps_scheduler = SmoothedScheduler(\n        num_epochs=num_epochs,\n        eps=eps,\n        mean=train_loader.mean,\n        std=train_loader.std,\n        eps_schedule_unit=eps_schedule_unit,\n        eps_schedule=eps_schedule,\n        batches_per_epoch=len(train_loader),\n        start_epoch=start_epoch,\n        **eps_scheduler_args,\n    )\n\n    for epoch in range(start_epoch, end_epoch):\n\n        if start_epoch &gt; epoch:\n            continue\n\n        cur_eps = eps_scheduler.get_cur_eps()\n\n        epoch_nat_err = 0\n        epoch_rob_err = 0\n\n        if lr_decay_schedule_unit == \"epoch\":\n            if epoch + 1 in lr_decay_schedule:\n                print(\"LEARNING RATE DECAYED!\")\n                cur_lr = cur_lr * lr_decay_factor\n                for g in optimizer.param_groups:\n                    g[\"lr\"] = cur_lr\n\n        print(\n            f\"[{epoch + 1}/{num_epochs}]: eps {[channel_eps for channel_eps in cur_eps]}\"\n        )\n\n        for block in hardened_model.bounded_blocks:\n            block.train()\n\n        running_loss = 0.0\n\n        for batch_idx, (data, target) in enumerate(train_loader):\n\n            cur_eps = eps_scheduler.get_cur_eps().reshape(-1, 1, 1)\n\n            ptb = PerturbationLpNorm(\n                eps=cur_eps,\n                norm=np.inf,\n                x_L=torch.clamp(data - cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n                x_U=torch.clamp(data + cur_eps, train_loader.min, train_loader.max).to(\n                    device\n                ),\n            )\n\n            if lr_decay_schedule_unit == \"batch\":\n                if no_batches + 1 in lr_decay_schedule:\n                    print(\"LEARNING RATE DECAYED!\")\n                    cur_lr = cur_lr * lr_decay_factor\n                    for g in optimizer.param_groups:\n                        g[\"lr\"] = cur_lr\n\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            clean_output = hardened_model(data)\n            regular_err = torch.sum(\n                torch.argmax(clean_output, dim=1) != target\n            ).item() / data.size(0)\n            epoch_nat_err += regular_err\n            clean_loss = criterion(clean_output, target).mean()\n\n            if eps_scheduler.get_cur_eps(normalise=False) == 0.0:\n                loss = clean_loss\n            elif eps_scheduler.get_cur_eps(normalise=False) != 0.0 and (\n                eps_scheduler.get_cur_eps(normalise=False)\n                != eps_scheduler.get_max_eps(normalise=False)\n            ):\n                reg_loss, robust_err = get_ibp_loss(\n                    hardened_model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    criterion=criterion,\n                    return_bounds=False,\n                    return_stats=True,\n                )\n\n                loss_regularisers = get_shi_regulariser(\n                    model=hardened_model,\n                    ptb=ptb,\n                    data=data,\n                    target=target,\n                    eps_scheduler=eps_scheduler,\n                    n_classes=n_classes,\n                    device=device,\n                    included_regularisers=[\"relu\", \"tightness\"],\n                    verbose=False,\n                    regularisation_decay=shi_reg_decay,\n                )\n                epoch_rob_err += robust_err\n                loss_regularisers = shi_regularisation_weight * loss_regularisers\n                loss = reg_loss + loss_regularisers\n\n            elif eps_scheduler.get_cur_eps(\n                normalise=False\n            ) == eps_scheduler.get_max_eps(normalise=False):\n                loss, robust_err = get_taps_loss(\n                    original_model=original_model,\n                    hardened_model=hardened_model,\n                    bounded_blocks=hardened_model.bounded_blocks,\n                    criterion=criterion,\n                    data=data,\n                    target=target,\n                    n_classes=n_classes,\n                    ptb=ptb,\n                    device=device,\n                    pgd_steps=taps_pgd_steps,\n                    pgd_restarts=taps_pgd_restarts,\n                    pgd_step_size=taps_pgd_step_size,\n                    pgd_decay_checkpoints=taps_pgd_decay_checkpoints,\n                    pgd_decay_factor=taps_pgd_decay_factor,\n                    gradient_link_thresh=taps_gradient_link_thresh,\n                    gradient_link_tolerance=taps_gradient_link_tolerance,\n                    gradient_expansion_alpha=gradient_expansion_alpha,\n                    propagation=\"IBP\",\n                    return_stats=True,\n                )\n\n                epoch_rob_err += robust_err\n            else:\n                assert False, \"One option must be true!\"\n\n            if l1_regularisation_weight is not None:\n                l1_regularisation = l1_regularisation_weight * get_l1_reg(\n                    model=original_model, device=device\n                )\n                loss += l1_regularisation\n\n            loss.backward()\n\n            if gradient_clip is not None:\n                nn.utils.clip_grad_value_(\n                    hardened_model.parameters(), clip_value=gradient_clip\n                )\n\n            optimizer.step()\n\n            running_loss += loss.item()\n            eps_scheduler.batch_step()\n            no_batches += 1\n\n        train_acc_nat = 1 - epoch_nat_err / len(train_loader)\n        train_acc_cert = 1 - epoch_rob_err / len(train_loader)\n\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\"\n        )\n        print(f\"\\t Natural Acc. Train: {train_acc_nat:.4f}\")\n        print(f\"\\t Adv. Acc. Train: N/A\")\n        print(f\"\\t Certified Acc. Train: {train_acc_cert:.4f}\")\n\n        if results_path is not None and (epoch + 1) % checkpoint_save_interval == 0:\n            save_checkpoint(\n                hardened_model, optimizer, running_loss, epoch + 1, results_path\n            )\n\n    return hardened_model\n</code></pre>"},{"location":"api/train/certified/util/","title":"util","text":""},{"location":"api/train/certified/util/#CTRAIN.train.certified.util.split_network","title":"<code>split_network(model, block_sizes, network_input, device)</code>","text":"<p>Splits a neural network model into smaller sequential blocks based on specified block sizes. Needed for TAPS/STAPS. Args:     model (torch.nn.Module): The neural network model to be split.     block_sizes (list of int): A list of integers specifying the sizes of each block.     network_input (torch.Tensor): The input tensor to the network.     device (torch.device): The device to which the tensors should be moved (e.g., 'cpu' or 'cuda'). Returns:     list of torch.nn.Sequential: A list of sequential blocks representing the split network.</p> Source code in <code>CTRAIN/train/certified/util.py</code> <pre><code>def split_network(model, block_sizes, network_input, device):\n    \"\"\"\n    Splits a neural network model into smaller sequential blocks based on specified block sizes. Needed for TAPS/STAPS.\n    Args:\n        model (torch.nn.Module): The neural network model to be split.\n        block_sizes (list of int): A list of integers specifying the sizes of each block.\n        network_input (torch.Tensor): The input tensor to the network.\n        device (torch.device): The device to which the tensors should be moved (e.g., 'cpu' or 'cuda').\n    Returns:\n        list of torch.nn.Sequential: A list of sequential blocks representing the split network.\n    \"\"\"\n    # TODO: Add assertions for robustness\n    start = 0\n    original_blocks = []\n    network_input = network_input.to(device)\n    for size in block_sizes:\n        end = start + size\n        abs_block = nn.Sequential(model.layers[start:end])\n        original_blocks.append(abs_block)\n\n        output_shape = abs_block(network_input).shape\n        network_input = torch.zeros(output_shape).to(device)\n\n        start = end\n    return original_blocks\n</code></pre>"},{"location":"api/train/certified/initialisation/shi/","title":"shi","text":""},{"location":"api/train/certified/initialisation/shi/#CTRAIN.train.certified.initialisation.shi.ibp_init_shi","title":"<code>ibp_init_shi(model_ori, model)</code>","text":"<p>Reinitialize the weights of the given model according to the Shi et al. (2020) initialization scheme.</p> <p>Parameters:</p> Name Type Description Default <code>model_ori</code> <code>Module</code> <p>The original model.</p> required <code>model</code> <code>BoundedModule</code> <p>The LiRPA model.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>CTRAIN/train/certified/initialisation/shi.py</code> <pre><code>def ibp_init_shi(model_ori, model):\n    \"\"\"\n    Reinitialize the weights of the given model according to the Shi et al. (2020) initialization scheme.\n\n    Args:\n        model_ori (torch.nn.Module): The original model.\n        model (auto_LiRPA.BoundedModule): The LiRPA model.\n\n    Returns:\n        None\n    \"\"\"\n    weights, biases = get_params(model_ori)\n    for i in range(len(weights)-1):\n        if weights[i][1].ndim == 1:\n            continue\n        weight = weights[i][1]\n        fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(weight)\n        std = math.sqrt(2 * math.pi / (fan_in**2))     \n        std_before = weight.std().item()\n        torch.nn.init.normal_(weight, mean=0, std=std)\n        print(f'Reinitialize {weights[i][0]}, std before {std_before:.5f}, std now {weight.std():.5f}')\n    for node in model._modules.values():\n        if isinstance(node, BoundConv) or isinstance(node, BoundLinear):\n            if len(node.inputs[0].inputs) &gt; 0 and isinstance(node.inputs[0].inputs[0], BoundAdd):\n                print(f'Adjust weights for node {node.name} due to residual connection')\n                node.inputs[1].param.data /= 2\n</code></pre>"},{"location":"api/train/certified/losses/convex_combinations/","title":"convex_combinations","text":""},{"location":"api/train/certified/losses/convex_combinations/#CTRAIN.train.certified.losses.convex_combinations.get_mtl_ibp_loss","title":"<code>get_mtl_ibp_loss(hardened_model, original_model, ptb, data, target, n_classes, criterion, alpha, return_bounds=False, return_stats=False, restarts=1, step_size=0.2, n_steps=200, pgd_ptb=None, early_stopping=False, decay_checkpoints=(), decay_factor=0.1, device='cuda')</code>","text":"<p>Computes the MTL-IBP loss.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>original_model</code> <code>Module</code> <p>The original model.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation applied to the input data.</p> required <code>data</code> <code>Tensor</code> <p>Input data.</p> required <code>target</code> <code>Tensor</code> <p>Target labels.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>criterion</code> <code>callable</code> <p>Loss function.</p> required <code>alpha</code> <code>float</code> <p>Weighting factor between robust loss and adversarial loss.</p> required <code>return_bounds</code> <code>bool</code> <p>If True, returns bounds. Default is False.</p> <code>False</code> <code>return_stats</code> <code>bool</code> <p>If True, returns robustness and adversarial error statistics. Default is False.</p> <code>False</code> <code>restarts</code> <code>int</code> <p>Number of restarts for PGD attack. Default is 1.</p> <code>1</code> <code>step_size</code> <code>float</code> <p>Step size for PGD attack. Default is 0.2.</p> <code>0.2</code> <code>n_steps</code> <code>int</code> <p>Number of steps for PGD attack. Default is 200.</p> <code>200</code> <code>pgd_ptb</code> <code>object</code> <p>PGD perturbation object. Default is None.</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>If True, stops early during PGD attack. Default is False.</p> <code>False</code> <code>decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for decay during PGD attack. Default is ().</p> <code>()</code> <code>decay_factor</code> <code>float</code> <p>Decay factor for PGD attack. Default is 0.1.</p> <code>0.1</code> <code>device</code> <code>str</code> <p>Device to run the computations on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the loss, and optionally certified and adversarial error statistics.</p> Source code in <code>CTRAIN/train/certified/losses/convex_combinations.py</code> <pre><code>def get_mtl_ibp_loss(hardened_model, original_model, ptb, data, target, n_classes, criterion, alpha, return_bounds=False, return_stats=False, restarts=1, step_size=.2, n_steps=200, pgd_ptb=None, early_stopping=False, decay_checkpoints=(), decay_factor=.1, device='cuda'):    \n    \"\"\"\n    Computes the MTL-IBP loss.\n\n    Parameters:\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        original_model (torch.nn.Module): The original model.\n        ptb (autoLiRPA.PerturbationLpNorm): The perturbation applied to the input data.\n        data (torch.Tensor): Input data.\n        target (torch.Tensor): Target labels.\n        n_classes (int): Number of classes.\n        criterion (callable): Loss function.\n        alpha (float): Weighting factor between robust loss and adversarial loss.\n        return_bounds (bool, optional): If True, returns bounds. Default is False.\n        return_stats (bool, optional): If True, returns robustness and adversarial error statistics. Default is False.\n        restarts (int, optional): Number of restarts for PGD attack. Default is 1.\n        step_size (float, optional): Step size for PGD attack. Default is 0.2.\n        n_steps (int, optional): Number of steps for PGD attack. Default is 200.\n        pgd_ptb (object, optional): PGD perturbation object. Default is None.\n        early_stopping (bool, optional): If True, stops early during PGD attack. Default is False.\n        decay_checkpoints (tuple, optional): Checkpoints for decay during PGD attack. Default is ().\n        decay_factor (float, optional): Decay factor for PGD attack. Default is 0.1.\n        device (str, optional): Device to run the computations on. Default is 'cuda'.\n\n    Returns:\n        (tuple): A tuple containing the loss, and optionally certified and adversarial error statistics.\n    \"\"\"\n    hardened_model.eval()\n    original_model.eval()\n    x_adv = pgd_attack(\n        model=hardened_model,\n        data=data,\n        target=target,\n        x_L=pgd_ptb.x_L,\n        x_U=pgd_ptb.x_U,\n        restarts=restarts,\n        step_size=step_size,\n        n_steps=n_steps,\n        early_stopping=early_stopping,\n        device=device,\n        decay_factor=decay_factor,\n        decay_checkpoints=decay_checkpoints\n    )\n\n    hardened_model.train()\n    original_model.train()\n\n    adv_output = hardened_model(x_adv)\n    adv_loss = criterion(adv_output, target).mean()\n\n    robust_loss, lb, ub = get_ibp_loss(\n        hardened_model=hardened_model,\n        ptb=ptb,\n        data=data,\n        target=target,\n        n_classes=n_classes,\n        criterion=criterion,\n        return_bounds=True\n    )\n\n    loss = alpha * robust_loss + (1 - alpha) * adv_loss\n\n    return_tuple = (loss,)\n\n    if return_bounds:\n        assert False, \"Return bounds is not implemented for MTL-IBP\"\n    elif return_stats:\n        robust_err = torch.sum((lb &lt; 0).any(dim=1)).item() / data.size(0)\n        adv_err = torch.sum(torch.argmax(adv_output, dim=1) != target).item() / data.size(0)\n        return_tuple = return_tuple + (robust_err, adv_err)\n\n    return return_tuple\n</code></pre>"},{"location":"api/train/certified/losses/crown_ibp/","title":"crown_ibp","text":""},{"location":"api/train/certified/losses/crown_ibp/#CTRAIN.train.certified.losses.crown_ibp.get_crown_ibp_loss","title":"<code>get_crown_ibp_loss(hardened_model, ptb, data, target, n_classes, criterion, beta, return_bounds=False, return_stats=True)</code>","text":"<p>Compute the CROWN-IBP loss.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation applied to the input data.</p> required <code>data</code> <code>Tensor</code> <p>The input data.</p> required <code>target</code> <code>Tensor</code> <p>The target labels.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes in the classification task.</p> required <code>criterion</code> <code>callable</code> <p>The loss function to be used.</p> required <code>beta</code> <code>float</code> <p>The interpolation parameter between CROWN_IBP and IBP bounds.</p> required <code>return_bounds</code> <code>bool</code> <p>If True, return the lower bounds. Default is False.</p> <code>False</code> <code>return_stats</code> <code>bool</code> <p>If True, return the robust error statistics. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the certified loss. If return_bounds is True, the tuple also contains the lower bounds. If return_stats is True, the tuple also contains the robust error statistics.</p> Source code in <code>CTRAIN/train/certified/losses/crown_ibp.py</code> <pre><code>def get_crown_ibp_loss(hardened_model, ptb, data, target, n_classes, criterion, beta, return_bounds=False, return_stats=True):\n    \"\"\"\n    Compute the CROWN-IBP loss.\n\n    Parameters:\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        ptb (autoLiRPA.PerturbationLpNorm): The perturbation applied to the input data.\n        data (torch.Tensor): The input data.\n        target (torch.Tensor): The target labels.\n        n_classes (int): The number of classes in the classification task.\n        criterion (callable): The loss function to be used.\n        beta (float): The interpolation parameter between CROWN_IBP and IBP bounds.\n        return_bounds (bool, optional): If True, return the lower bounds. Default is False.\n        return_stats (bool, optional): If True, return the robust error statistics. Default is True.\n\n    Returns:\n        (tuple): A tuple containing the certified loss. If return_bounds is True, the tuple also contains the lower bounds.\n            If return_stats is True, the tuple also contains the robust error statistics.\n    \"\"\"\n    ilb, iub = bound_ibp(\n        model=hardened_model,\n        ptb=ptb,\n        data=data,\n        target=target,\n        n_classes=n_classes,\n        bound_upper=False\n    )\n    if beta &lt; 1e-5:\n        lb = ilb\n    else:\n        # Attention: We have to reuse the input here. Otherwise the memory requirements become too large!\n        # Input is reused from above bound_ibp call!\n        clb, cub = bound_crown_ibp(\n            model=hardened_model,\n            ptb=ptb,\n            data=data,\n            target=target,\n            n_classes=n_classes,\n            reuse_input=True,\n            bound_upper=False\n        )\n\n        lb = clb * beta + ilb * (1 - beta)\n\n    certified_loss = get_loss_from_bounds(lb, criterion)\n\n    return_tuple = (certified_loss,)\n\n    if return_bounds:\n        return_tuple = return_tuple + (lb, None)\n    if return_stats:\n        robust_err = torch.sum((lb &lt; 0).any(dim=1)).item() / data.size(0)\n        return_tuple = return_tuple + (robust_err,)\n\n    return return_tuple\n</code></pre>"},{"location":"api/train/certified/losses/ibp/","title":"ibp","text":""},{"location":"api/train/certified/losses/ibp/#CTRAIN.train.certified.losses.ibp.get_ibp_loss","title":"<code>get_ibp_loss(hardened_model, ptb, data, target, n_classes, criterion, return_bounds=False, return_stats=False)</code>","text":"<p>Compute the Interval Bound Propagation (IBP) loss for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation applied to the input data.</p> required <code>data</code> <code>Tensor</code> <p>Input data.</p> required <code>target</code> <code>Tensor</code> <p>Target labels.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>criterion</code> <code>callable</code> <p>Loss function to be used.</p> required <code>return_bounds</code> <code>bool</code> <p>If True, return the lower and upper bounds. Default is False.</p> <code>False</code> <code>return_stats</code> <code>bool</code> <p>If True, return additional statistics. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the certified loss. If <code>return_bounds</code> is True, the tuple also contains the lower and upper bounds.     If <code>return_stats</code> is True, the tuple also contains the robust error.</p> Source code in <code>CTRAIN/train/certified/losses/ibp.py</code> <pre><code>def get_ibp_loss(hardened_model, ptb, data, target, n_classes, criterion, return_bounds=False, return_stats=False):\n    \"\"\"\n    Compute the Interval Bound Propagation (IBP) loss for a given model.\n\n    Args:\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        ptb (autoLiRPA.PerturbationLpNorm): The perturbation applied to the input data.\n        data (torch.Tensor): Input data.\n        target (torch.Tensor): Target labels.\n        n_classes (int): Number of classes.\n        criterion (callable): Loss function to be used.\n        return_bounds (bool, optional): If True, return the lower and upper bounds. Default is False.\n        return_stats (bool, optional): If True, return additional statistics. Default is False.\n\n    Returns:\n        (tuple): A tuple containing the certified loss. If `return_bounds` is True, the tuple also contains the lower and upper bounds. \n               If `return_stats` is True, the tuple also contains the robust error.\n    \"\"\"\n    lb, ub = bound_ibp(\n        model=hardened_model,\n        ptb=ptb,\n        data=data,\n        target=target,\n        n_classes=n_classes,\n    )\n    certified_loss = get_loss_from_bounds(lb, criterion)\n\n    return_tuple = (certified_loss,)\n\n    if return_bounds:\n        return_tuple = return_tuple + (lb, ub)\n    if return_stats:\n        robust_err = torch.sum((lb &lt; 0).any(dim=1)).item() / data.size(0)\n        return_tuple = return_tuple + (robust_err,)\n\n    return return_tuple\n</code></pre>"},{"location":"api/train/certified/losses/sabr/","title":"sabr","text":""},{"location":"api/train/certified/losses/sabr/#CTRAIN.train.certified.losses.sabr.get_sabr_loss","title":"<code>get_sabr_loss(hardened_model, original_model, data, target, eps, subselection_ratio, criterion, device='cuda', n_classes=10, x_L=None, x_U=None, data_min=None, data_max=None, pgd_steps=8, pgd_step_size=0.5, pgd_restarts=1, pgd_early_stopping=True, pgd_decay_factor=0.1, pgd_decay_checkpoints=(4, 7), return_stats=False, return_bounds=False, **kwargs)</code>","text":"<p>Compute the SABR loss for a given model and data.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>original_model</code> <code>Module</code> <p>The original model.</p> required <code>data</code> <code>Tensor</code> <p>Input data.</p> required <code>target</code> <code>Tensor</code> <p>Target labels.</p> required <code>eps</code> <code>float</code> <p>Perturbation budget for adversarial attacks.</p> required <code>subselection_ratio</code> <code>float</code> <p>Factor to shrink epsilon by.</p> required <code>criterion</code> <code>callable</code> <p>Loss function to use.</p> required <code>device</code> <code>str</code> <p>Device to run the computation on. Default is 'cuda'.</p> <code>'cuda'</code> <code>n_classes</code> <code>int</code> <p>Number of classes. Default is 10.</p> <code>10</code> <code>x_L</code> <code>Tensor</code> <p>Lower bound for input data. Default is None.</p> <code>None</code> <code>x_U</code> <code>Tensor</code> <p>Upper bound for input data. Default is None.</p> <code>None</code> <code>data_min</code> <code>Tensor</code> <p>Minimum value for input data. Default is None.</p> <code>None</code> <code>data_max</code> <code>Tensor</code> <p>Maximum value for input data. Default is None.</p> <code>None</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps. Default is 8.</p> <code>8</code> <code>pgd_step_size</code> <code>float</code> <p>Step size for PGD. Default is 0.5.</p> <code>0.5</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts. Default is 1.</p> <code>1</code> <code>pgd_early_stopping</code> <code>bool</code> <p>Whether to use early stopping in PGD. Default is True.</p> <code>True</code> <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD step size. Default is 0.1.</p> <code>0.1</code> <code>pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD step size decay. Default is (4, 7).</p> <code>(4, 7)</code> <code>return_stats</code> <code>bool</code> <p>Whether to return robustness statistics. Default is False.</p> <code>False</code> <code>return_bounds</code> <code>bool</code> <p>Whether to return bounds. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the loss, and optionally robustness statistics (robust_err, adv_err).</p> Source code in <code>CTRAIN/train/certified/losses/sabr.py</code> <pre><code>def get_sabr_loss(hardened_model, original_model, data, target, eps, subselection_ratio, criterion, device='cuda', \n                      n_classes=10, x_L=None, x_U=None, data_min=None, data_max=None, pgd_steps=8, pgd_step_size=.5, pgd_restarts=1, pgd_early_stopping=True, pgd_decay_factor=.1, pgd_decay_checkpoints=(4,7), return_stats=False, return_bounds=False, **kwargs):\n\n    \"\"\"\n    Compute the SABR loss for a given model and data.\n\n    Parameters:\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        original_model (torch.nn.Module): The original model.\n        data (torch.Tensor): Input data.\n        target (torch.Tensor): Target labels.\n        eps (float): Perturbation budget for adversarial attacks.\n        subselection_ratio (float): Factor to shrink epsilon by.\n        criterion (callable): Loss function to use.\n        device (str, optional): Device to run the computation on. Default is 'cuda'.\n        n_classes (int, optional): Number of classes. Default is 10.\n        x_L (torch.Tensor, optional): Lower bound for input data. Default is None.\n        x_U (torch.Tensor, optional): Upper bound for input data. Default is None.\n        data_min (torch.Tensor, optional): Minimum value for input data. Default is None.\n        data_max (torch.Tensor, optional): Maximum value for input data. Default is None.\n        pgd_steps (int, optional): Number of PGD steps. Default is 8.\n        pgd_step_size (float, optional): Step size for PGD. Default is 0.5.\n        pgd_restarts (int, optional): Number of PGD restarts. Default is 1.\n        pgd_early_stopping (bool, optional): Whether to use early stopping in PGD. Default is True.\n        pgd_decay_factor (float, optional): Decay factor for PGD step size. Default is 0.1.\n        pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD step size decay. Default is (4, 7).\n        return_stats (bool, optional): Whether to return robustness statistics. Default is False.\n        return_bounds (bool, optional): Whether to return bounds. Default is False.\n        **kwargs: Additional arguments.\n\n    Returns:\n        (tuple): A tuple containing the loss, and optionally robustness statistics (robust_err, adv_err).\n    \"\"\"\n    lb, ub, adv_output = bound_sabr(\n        hardened_model=hardened_model,\n        original_model=original_model,\n        data=data,\n        data_min=data_min,\n        data_max=data_max,\n        target=target,\n        eps=eps,\n        subselection_ratio=subselection_ratio,\n        device=device,\n        n_classes=n_classes,\n        x_L=x_L,\n        x_U=x_U,\n        n_steps=pgd_steps,\n        step_size=pgd_step_size,\n        restarts=pgd_restarts,\n        early_stopping=pgd_early_stopping,\n        decay_checkpoints=pgd_decay_checkpoints, \n        decay_factor=pgd_decay_factor,\n        return_adv_output=True\n    )\n    loss = get_loss_from_bounds(lb, criterion=criterion)\n\n    return_tuple = (loss,)\n\n    if return_bounds:\n        assert False, \"Return bounds is not implemented for MTL-IBP\"\n    elif return_stats:\n        robust_err = torch.sum((lb &lt; 0).any(dim=1)).item() / data.size(0)\n        adv_err = torch.sum(torch.argmax(adv_output, dim=1) != target).item() / data.size(0)\n        return_tuple = return_tuple + (robust_err, adv_err)\n\n    return return_tuple\n</code></pre>"},{"location":"api/train/certified/losses/taps/","title":"taps","text":""},{"location":"api/train/certified/losses/taps/#CTRAIN.train.certified.losses.taps.get_taps_loss","title":"<code>get_taps_loss(original_model, hardened_model, bounded_blocks, criterion, data, target, n_classes, ptb, device='cuda', pgd_steps=8, pgd_restarts=1, pgd_step_size=None, pgd_decay_factor=0.2, pgd_decay_checkpoints=(5, 7), gradient_link_thresh=0.5, gradient_link_tolerance=1e-05, gradient_expansion_alpha=5, propagation='IBP', sabr_args=None, return_bounds=False, return_stats=False)</code>","text":"<p>Compute the TAPS loss.</p> <p>Parameters:</p> Name Type Description Default <code>hardened_model</code> <code>BoundedModule</code> <p>The bounded model to be trained.</p> required <code>original_model</code> <code>Module</code> <p>The original model.</p> required <code>bounded_blocks</code> <code>list</code> <p>List of the LiRPA blocks needed for TAPS (feature extractor and classifier).</p> required <code>criterion</code> <code>callable</code> <p>Loss function to be used.</p> required <code>data</code> <code>Tensor</code> <p>Input data.</p> required <code>target</code> <code>Tensor</code> <p>Target labels.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation applied to the input data.</p> required <code>device</code> <code>str</code> <p>Device to run the computation on. Default is 'cuda'.</p> <code>'cuda'</code> <code>pgd_steps</code> <code>int</code> <p>Number of PGD steps. Default is 8.</p> <code>8</code> <code>pgd_restarts</code> <code>int</code> <p>Number of PGD restarts. Default is 1.</p> <code>1</code> <code>pgd_step_size</code> <code>float</code> <p>Step size for PGD. Default is None.</p> <code>None</code> <code>pgd_decay_factor</code> <code>float</code> <p>Decay factor for PGD step size. Default is 0.2.</p> <code>0.2</code> <code>pgd_decay_checkpoints</code> <code>tuple</code> <p>Checkpoints for PGD decay. Default is (5, 7).</p> <code>(5, 7)</code> <code>gradient_link_thresh</code> <code>float</code> <p>Threshold for gradient linking. Default is 0.5.</p> <code>0.5</code> <code>gradient_link_tolerance</code> <code>float</code> <p>Tolerance for gradient linking. Default is 1e-05.</p> <code>1e-05</code> <code>gradient_expansion_alpha</code> <code>float</code> <p>Alpha value for gradient expansion. Default is 5.</p> <code>5</code> <code>propagation</code> <code>str</code> <p>Propagation method to be used (SABR or IBP). Default is \"IBP\".</p> <code>'IBP'</code> <code>sabr_args</code> <code>dict</code> <p>Additional arguments for SABR. Default is None.</p> <code>None</code> <code>return_bounds</code> <code>bool</code> <p>Whether to return bounds. Default is False.</p> <code>False</code> <code>return_stats</code> <code>bool</code> <p>Whether to return statistics. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the loss, and optionally the bounds and robust error statistics.</p> Source code in <code>CTRAIN/train/certified/losses/taps.py</code> <pre><code>def get_taps_loss(original_model, hardened_model, bounded_blocks, criterion, data, target, n_classes, ptb, device='cuda', pgd_steps=8, pgd_restarts=1, \n                  pgd_step_size=None, pgd_decay_factor=.2, pgd_decay_checkpoints=(5,7), gradient_link_thresh=.5,\n                  gradient_link_tolerance=1e-05, gradient_expansion_alpha=5, propagation=\"IBP\", sabr_args=None, return_bounds=False, return_stats=False):\n\n    \"\"\"\n    Compute the TAPS loss.\n\n    Parameters:\n        hardened_model (auto_LiRPA.BoundedModule): The bounded model to be trained.\n        original_model (torch.nn.Module): The original model.\n        bounded_blocks (list): List of the LiRPA blocks needed for TAPS (feature extractor and classifier).\n        criterion (callable): Loss function to be used.\n        data (torch.Tensor): Input data.\n        target (torch.Tensor): Target labels.\n        n_classes (int): Number of classes in the classification task.\n        ptb (autoLiRPA.PerturbationLpNorm): The perturbation applied to the input data.\n        device (str, optional): Device to run the computation on. Default is 'cuda'.\n        pgd_steps (int, optional): Number of PGD steps. Default is 8.\n        pgd_restarts (int, optional): Number of PGD restarts. Default is 1.\n        pgd_step_size (float, optional): Step size for PGD. Default is None.\n        pgd_decay_factor (float, optional): Decay factor for PGD step size. Default is 0.2.\n        pgd_decay_checkpoints (tuple, optional): Checkpoints for PGD decay. Default is (5, 7).\n        gradient_link_thresh (float, optional): Threshold for gradient linking. Default is 0.5.\n        gradient_link_tolerance (float, optional): Tolerance for gradient linking. Default is 1e-05.\n        gradient_expansion_alpha (float, optional): Alpha value for gradient expansion. Default is 5.\n        propagation (str, optional): Propagation method to be used (SABR or IBP). Default is \"IBP\".\n        sabr_args (dict, optional): Additional arguments for SABR. Default is None.\n        return_bounds (bool, optional): Whether to return bounds. Default is False.\n        return_stats (bool, optional): Whether to return statistics. Default is False.\n\n    Returns:\n        (tuple): A tuple containing the loss, and optionally the bounds and robust error statistics.\n    \"\"\"\n    assert len(bounded_blocks) == 2, \"Split not supported!\"\n\n    taps_bound, ibp_bound = bound_taps(\n        original_model=original_model,\n        hardened_model=hardened_model,\n        bounded_blocks=bounded_blocks,\n        data=data,\n        target=target,\n        n_classes=n_classes,\n        ptb=ptb,\n        device=device,\n        pgd_steps=pgd_steps,\n        pgd_restarts=pgd_restarts,\n        pgd_step_size=pgd_step_size, \n        pgd_decay_factor=pgd_decay_factor,\n        pgd_decay_checkpoints=pgd_decay_checkpoints,\n        gradient_link_thresh=gradient_link_thresh,\n        gradient_link_tolerance=gradient_link_tolerance,\n        propagation=propagation,\n        sabr_args=sabr_args\n    )\n\n    taps_loss = get_loss_from_bounds(taps_bound, criterion)\n    ibp_loss = get_loss_from_bounds(ibp_bound, criterion)\n\n    loss = GradExpander.apply(taps_loss, gradient_expansion_alpha) * ibp_loss\n\n    return_tuple = (loss,)\n\n    if return_bounds:\n        return_tuple = return_tuple + (taps_bound, None)\n    if return_stats:\n        robust_err = torch.sum((taps_bound &lt; 0).any(dim=1)).item() / data.size(0)\n        return_tuple = return_tuple + (robust_err,)\n\n    return return_tuple\n</code></pre>"},{"location":"api/train/certified/losses/util/","title":"util","text":""},{"location":"api/train/certified/losses/util/#CTRAIN.train.certified.losses.util.get_loss_from_bounds","title":"<code>get_loss_from_bounds(lb, criterion)</code>","text":"<p>Computes the certified loss from the given lower bounds using the specified criterion.</p> <p>Parameters:</p> Name Type Description Default <code>lb</code> <code>Tensor</code> <p>A tensor containing the lower bounds with shape (batch_size, num_classes - 1).</p> required <code>criterion</code> <code>callable</code> <p>A loss function that takes two arguments: the input tensor and the target tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The mean certified loss computed from the lower bounds.</p> Source code in <code>CTRAIN/train/certified/losses/util.py</code> <pre><code>def get_loss_from_bounds(lb, criterion):\n    \"\"\"\n    Computes the certified loss from the given lower bounds using the specified criterion.\n\n    Args:\n        lb (torch.Tensor): A tensor containing the lower bounds with shape (batch_size, num_classes - 1).\n        criterion (callable): A loss function that takes two arguments: the input tensor and the target tensor.\n\n    Returns:\n        torch.Tensor: The mean certified loss computed from the lower bounds.\n    \"\"\"\n    lb_padded = torch.cat((torch.zeros(size=(lb.size(0),1), dtype=lb.dtype, device=lb.device), lb), dim=1)\n    fake_labels = torch.zeros(size=(lb.size(0),), dtype=torch.int64, device=lb.device)\n    certified_loss = criterion(-lb_padded, fake_labels).mean()\n    return certified_loss\n</code></pre>"},{"location":"api/train/certified/regularisers/l1/","title":"l1","text":""},{"location":"api/train/certified/regularisers/l1/#CTRAIN.train.certified.regularisers.l1.get_l1_reg","title":"<code>get_l1_reg(model, device='cuda')</code>","text":"<p>Calculate the L1 regularization loss for a given model.</p> <p>This function computes the L1 regularization loss by summing the absolute values of the weights of all Linear and Convolutional layers in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The neural network model to regularize. IMPORTANT: Don't pass the bounded model here.</p> required <code>device</code> <code>str</code> <p>The device to perform the computation on. Default is 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The L1 regularization loss.</p> Source code in <code>CTRAIN/train/certified/regularisers/l1.py</code> <pre><code>def get_l1_reg(model, device='cuda'):\n    \"\"\"\n    Calculate the L1 regularization loss for a given model.\n\n    This function computes the L1 regularization loss by summing the absolute values\n    of the weights of all Linear and Convolutional layers in the model.\n\n    Args:\n        model (torch.nn.Module): The neural network model to regularize. IMPORTANT: Don't pass the bounded model here.\n        device (str, optional): The device to perform the computation on. Default is 'cuda'.\n\n    Returns:\n        (torch.Tensor): The L1 regularization loss.\n    \"\"\"\n    loss = torch.zeros(()).to(device)\n    # only regularise Linear and Convolutional layers\n    for module in model.modules():\n        if isinstance(module, nn.Linear):\n            loss += torch.abs(module.weight).sum()\n        elif isinstance(module, nn.Conv2d):\n            loss += torch.abs(module.weight).sum()\n    return loss\n</code></pre>"},{"location":"api/train/certified/regularisers/shi/","title":"shi","text":""},{"location":"api/train/certified/regularisers/shi/#CTRAIN.train.certified.regularisers.shi.get_shi_regulariser","title":"<code>get_shi_regulariser(model, ptb, data, target, eps_scheduler, n_classes, device, tolerance=0.5, verbose=False, included_regularisers=['relu', 'tightness'], regularisation_decay=True)</code>","text":"<p>Compute the Shi regularisation loss for a given model. See Shi et al. (2020) for more details.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BoundedModule</code> <p>The bounded model. IMPORTANT: Do not pass the original model, but the hardened model.</p> required <code>ptb</code> <code>PerturbationLpNorm</code> <p>The perturbation applied to the input data.</p> required <code>data</code> <code>Tensor</code> <p>Input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>Target labels tensor.</p> required <code>eps_scheduler</code> <code>BaseScheduler</code> <p>Scheduler for epsilon values.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the classification task.</p> required <code>device</code> <code>device</code> <p>Device to perform computations on (e.g., 'cpu' or 'cuda').</p> required <code>tolerance</code> <code>float</code> <p>Tolerance value for regularisation. Default is 0.5.</p> <code>0.5</code> <code>verbose</code> <code>bool</code> <p>If True, prints detailed information during computation. Default is False.</p> <code>False</code> <code>included_regularisers</code> <code>list of str</code> <p>List of regularisers to include in the loss computation. Default is ['relu', 'tightness'].</p> <code>['relu', 'tightness']</code> <code>regularisation_decay</code> <code>bool</code> <p>If True, applies decay to the regularisation loss. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: The computed SHI regulariser loss.</p> Source code in <code>CTRAIN/train/certified/regularisers/shi.py</code> <pre><code>def get_shi_regulariser(model, ptb, data, target, eps_scheduler, n_classes, device, tolerance=.5, verbose=False, included_regularisers=['relu', 'tightness'], regularisation_decay=True):\n    \"\"\"\n    Compute the Shi regularisation loss for a given model. See Shi et al. (2020) for more details.\n\n    Args:\n        model (auto_LiRPA.BoundedModule): The bounded model. IMPORTANT: Do not pass the original model, but the hardened model.\n        ptb (autoLiRPA.PerturbationLpNorm): The perturbation applied to the input data.\n        data (torch.Tensor): Input data tensor.\n        target (torch.Tensor): Target labels tensor.\n        eps_scheduler (BaseScheduler): Scheduler for epsilon values.\n        n_classes (int): Number of classes in the classification task.\n        device (torch.device): Device to perform computations on (e.g., 'cpu' or 'cuda').\n        tolerance (float, optional): Tolerance value for regularisation. Default is 0.5.\n        verbose (bool, optional): If True, prints detailed information during computation. Default is False.\n        included_regularisers (list of str, optional): List of regularisers to include in the loss computation. Default is ['relu', 'tightness'].\n        regularisation_decay (bool, optional): If True, applies decay to the regularisation loss. Default is True.\n\n    Returns:\n        torch.Tensor: The computed SHI regulariser loss.\n    \"\"\"\n    loss = torch.zeros(()).to(device)\n\n    # Handle the non-feedforward case\n    l0 = torch.zeros_like(loss)\n    loss_tightness, loss_std, loss_relu, loss_ratio = (l0.clone() for i in range(4))\n\n    if isinstance(model, BoundDataParallel):\n        modules = list(model._modules.values())[0]._modules\n    else:\n        modules = model._modules\n    # print(modules)\n    # print(modules.keys())\n    # print(model)\n    node_inp = modules['/input']#modules['/input.1']\n    if node_inp.upper is None:\n        _, _ = bound_ibp(\n                model=model,\n                ptb=ptb,\n                data=data,\n                target=target,\n                n_classes=n_classes,\n                bound_upper=True\n            )\n    tightness_0 = ((node_inp.upper - node_inp.lower) / 2).mean()\n    ratio_init = tightness_0 / ((node_inp.upper + node_inp.lower) / 2).std()\n    cnt_layers = 0\n    cnt = 0\n    for m in model._modules.values():\n        if isinstance(m, BoundRelu):\n            lower, upper = m.inputs[0].lower, m.inputs[0].upper\n            center = (upper + lower) / 2\n            diff = ((upper - lower) / 2)\n            tightness = diff.mean()\n            mean_ = center.mean()\n            std_ = center.std()            \n\n            loss_tightness += F.relu(tolerance - tightness_0 / tightness.clamp(min=1e-12)) / tolerance\n            loss_std += F.relu(tolerance - std_) / tolerance\n            cnt += 1\n\n            # L_{relu}\n            mask_act, mask_inact = lower&gt;0, upper&lt;0\n            mean_act = (center * mask_act).mean()\n            mean_inact = (center * mask_inact).mean()\n            delta = (center - mean_)**2\n            var_act = (delta * mask_act).sum()# / center.numel()\n            var_inact = (delta * mask_inact).sum()# / center.numel()                        \n\n            mean_ratio = mean_act / -mean_inact\n            var_ratio = var_act / var_inact\n            mean_ratio = torch.min(mean_ratio, 1 / mean_ratio.clamp(min=1e-12))\n            var_ratio = torch.min(var_ratio, 1 / var_ratio.clamp(min=1e-12))\n            loss_relu_ = ((\n                F.relu(tolerance - mean_ratio) + F.relu(tolerance - var_ratio)) \n                / tolerance)       \n            if not torch.isnan(loss_relu_) and not torch.isinf(loss_relu_):\n                loss_relu += loss_relu_ \n\n            if verbose:\n                bn_mean = (lower.mean() + upper.mean()) / 2\n                bn_var = ((upper**2 + lower**2) / 2).mean() - bn_mean**2\n                print(m.name, m, \n                    'tightness {:.4f} gain {:.4f} std {:.4f}'.format(\n                        tightness.item(), (tightness/tightness_0).item(), std_.item()),\n                    'input', m.inputs[0], m.inputs[0].name,\n                    'active {:.4f} inactive {:.4f}'.format(\n                        (lower&gt;0).float().sum()/lower.numel(),\n                        (upper&lt;0).float().sum()/lower.numel()),\n                    'bnv2_mean {:.5f} bnv2_var {:.5f}'.format(bn_mean.item(), bn_var.item())\n                )\n                # pre-bn\n                lower, upper = m.inputs[0].inputs[0].lower, m.inputs[0].inputs[0].upper\n                bn_mean = (lower.mean() + upper.mean()) / 2\n                bn_var = ((upper**2 + lower**2) / 2).mean() - bn_mean**2\n                print('pre-bn',\n                    'bnv2_mean {:.5f} bnv2_var {:.5f}'.format(bn_mean.item(), bn_var.item()))\n\n    loss_tightness /= cnt\n    loss_std /= cnt\n    loss_relu /= cnt\n\n    for item in ['tightness', 'relu', 'std']:\n        loss_ = eval('loss_{}'.format(item))\n        if item in included_regularisers:\n            loss += loss_\n\n    if regularisation_decay:\n        loss = (1 - (eps_scheduler.get_cur_eps(normalise=False) / eps_scheduler.get_max_eps(normalise=False))) * loss\n\n    return loss\n</code></pre>"},{"location":"api/util/util/","title":"util","text":""},{"location":"api/util/util/#CTRAIN.util.util.construct_c","title":"<code>construct_c(data, target, n_classes)</code>","text":"<p>Constructs a tensor <code>c</code> based on the input data, target labels, and the number of classes. This is used to calculate the margins between classes during bound calculation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target labels tensor.</p> required <code>n_classes</code> <code>int</code> <p>The number of classes.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A tensor <code>c</code> of shape (batch_size, n_classes - 1, n_classes) where <code>batch_size</code> is the size of the input data.</p> Source code in <code>CTRAIN/util/util.py</code> <pre><code>def construct_c(data, target, n_classes):\n    \"\"\"\n    Constructs a tensor `c` based on the input data, target labels, and the number of classes.\n    This is used to calculate the margins between classes during bound calculation.\n\n    Args:\n        data (torch.Tensor): The input data tensor.\n        target (torch.Tensor): The target labels tensor.\n        n_classes (int): The number of classes.\n\n    Returns:\n        torch.Tensor: A tensor `c` of shape (batch_size, n_classes - 1, n_classes) where `batch_size` is the size of the input data.\n    \"\"\"\n    c = torch.eye(n_classes).type_as(data)[target].unsqueeze(1) - torch.eye(n_classes).type_as(data).unsqueeze(0)\n    # remove specifications to self\n    I = (~(target.data.unsqueeze(1) == torch.arange(n_classes).type_as(target.data).unsqueeze(0)))\n    c = (c[I].view(data.size(0), n_classes - 1, n_classes))\n    return c\n</code></pre>"},{"location":"api/util/util/#CTRAIN.util.util.export_onnx","title":"<code>export_onnx(model, file_name, batch_size, input_shape)</code>","text":"<p>Exports a PyTorch model to the ONNX format.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to be exported.</p> required <code>file_name</code> <code>str</code> <p>The file path where the ONNX model will be saved.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the input tensor.</p> required <code>input_shape</code> <code>tuple</code> <p>The shape of the input tensor. If the shape has 4 dimensions,                   the first dimension is assumed to be the batch size.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>CTRAIN/util/util.py</code> <pre><code>def export_onnx(model, file_name, batch_size, input_shape):\n    \"\"\"\n    Exports a PyTorch model to the ONNX format.\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to be exported.\n        file_name (str): The file path where the ONNX model will be saved.\n        batch_size (int): The batch size for the input tensor.\n        input_shape (tuple): The shape of the input tensor. If the shape has 4 dimensions, \n                             the first dimension is assumed to be the batch size.\n\n    Returns:\n        None\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n        # Input to the model\n    if len(input_shape) == 4:\n        batch_size = input_shape[0]\n        input_shape = input_shape[1:4]\n    x = torch.randn(batch_size, *input_shape, requires_grad=True).to(device)\n    torch_out = model(x)\n\n    # Export the model\n    torch.onnx.export(model,               # model being run\n                    x,                         # model input (or a tuple for multiple inputs)\n                    file_name,   # where to save the model (can be a file or file-like object)\n                    export_params=True,        # store the trained parameter weights inside the model file\n                    opset_version=10,          # the ONNX version to export the model to\n                    do_constant_folding=False,  # whether to execute constant folding for optimization\n                    input_names = ['input'],   # the model's input names\n                    output_names = ['output'], # the model's output names\n                    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                    'output' : {0 : 'batch_size'}})\n</code></pre>"},{"location":"api/util/util/#CTRAIN.util.util.save_checkpoint","title":"<code>save_checkpoint(model, optimizer, loss, epoch, results_path)</code>","text":"<p>Saves the model checkpoint to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to save.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer used for training the model.</p> required <code>loss</code> <code>float</code> <p>The loss value at the time of saving the checkpoint.</p> required <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>results_path</code> <code>str</code> <p>The directory path where the checkpoint will be saved.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a checkpoint for the given epoch already exists at the specified path.</p> Source code in <code>CTRAIN/util/util.py</code> <pre><code>def save_checkpoint(model, optimizer, loss, epoch, results_path):\n    \"\"\"\n    Saves the model checkpoint to the specified path.\n\n    Args:\n        model (torch.nn.Module): The model to save.\n        optimizer (torch.optim.Optimizer): The optimizer used for training the model.\n        loss (float): The loss value at the time of saving the checkpoint.\n        epoch (int): The current epoch number.\n        results_path (str): The directory path where the checkpoint will be saved.\n\n    Raises:\n        AssertionError: If a checkpoint for the given epoch already exists at the specified path.\n    \"\"\"\n    if os.path.exists(f\"{results_path}/{epoch}_checkpoint.pt\"):\n        assert False, \"Checkpoint already exists!\"\n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, f\"{results_path}/{epoch}_checkpoint.pt\")\n</code></pre>"},{"location":"api/util/util/#CTRAIN.util.util.seed_ctrain","title":"<code>seed_ctrain(seed=42)</code>","text":"<p>Set the seed for random number generation in Python, NumPy, and PyTorch to ensure reproducibility. Parameters: seed (int): The seed value to use for random number generation. Default is 42. This function sets the seed for the following: - Python's built-in random module - NumPy's random module - PyTorch's random number generators (both CPU and CUDA, if available) Additionally, if CUDA is available, it sets the following PyTorch settings: - torch.backends.cudnn.deterministic to True - torch.backends.cudnn.benchmark to False These settings ensure that the results are deterministic and reproducible.</p> Source code in <code>CTRAIN/util/util.py</code> <pre><code>def seed_ctrain(seed=42):\n    \"\"\"\n    Set the seed for random number generation in Python, NumPy, and PyTorch to ensure reproducibility.\n    Parameters:\n    seed (int): The seed value to use for random number generation. Default is 42.\n    This function sets the seed for the following:\n    - Python's built-in random module\n    - NumPy's random module\n    - PyTorch's random number generators (both CPU and CUDA, if available)\n    Additionally, if CUDA is available, it sets the following PyTorch settings:\n    - torch.backends.cudnn.deterministic to True\n    - torch.backends.cudnn.benchmark to False\n    These settings ensure that the results are deterministic and reproducible.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"examples/certified_training/","title":"Certified Training with CTRAIN","text":"<p>First, we load the <code>torch</code> library as well as the required functions from the <code>CTRAIN</code> library.</p> In\u00a0[3]: Copied! <pre>import torch\n\nfrom CTRAIN.model_definitions import CNN7_Shi\nfrom CTRAIN.model_wrappers import ShiIBPModelWrapper\nfrom CTRAIN.data_loaders import load_mnist\n</pre> import torch  from CTRAIN.model_definitions import CNN7_Shi from CTRAIN.model_wrappers import ShiIBPModelWrapper from CTRAIN.data_loaders import load_mnist <p>Now, we load the MNIST dataset using <code>CTRAIN</code> and define the model.</p> In\u00a0[6]: Copied! <pre>in_shape = [1, 28, 28]\ntrain_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")\n\nmodel = CNN7_Shi(in_shape=in_shape, n_classes=10)\n</pre> in_shape = [1, 28, 28] train_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")  model = CNN7_Shi(in_shape=in_shape, n_classes=10) <pre>MNIST dataset - Min value: -0.4242129623889923, Max value: 2.821486711502075\n</pre> <p>To train the network certifiably, we have to wrap it in a <code>CTRAIN</code> model wrapper. If you desire to use another certified training method, please import the respective wrapper from the <code>CTRAIN.model_wrappers</code> package. We initialise the wrapper with the required arguments of the training process, such as the number of warm up epochs, i.e. the number of epochs where the model is trained on natural loss, or the number of ramp up epochs, i.e. the number of epochs where the epsilon value is gradually increased to the final training epsilon. Please consult the documentation to set the other hyperparameters.</p> In\u00a0[7]: Copied! <pre>wrapped_model = ShiIBPModelWrapper(\n    model, \n    input_shape=in_shape, \n    eps=0.1,\n    num_epochs=70,\n    warm_up_epochs=0,\n    ramp_up_epochs=40,\n    lr_decay_milestones=(50, 60),\n)\n</pre> wrapped_model = ShiIBPModelWrapper(     model,      input_shape=in_shape,      eps=0.1,     num_epochs=70,     warm_up_epochs=0,     ramp_up_epochs=40,     lr_decay_milestones=(50, 60), ) <p>We initiate the training process by calling the <code>train_model</code> function of the wrapped model.</p> In\u00a0[\u00a0]: Copied! <pre>wrapped_model.train_model(train_loader)\n</pre> wrapped_model.train_model(train_loader) <p>Finally, we save the resulting model weights.</p> In\u00a0[11]: Copied! <pre>torch.save(wrapped_model.state_dict(), '../../mnist_0.1_model.pt')\n</pre> torch.save(wrapped_model.state_dict(), '../../mnist_0.1_model.pt')"},{"location":"examples/certified_training/#certified-training-with-ctrain","title":"Certified Training with CTRAIN\u00b6","text":"<p>In this example, we train the standard CNN7 Architecture proposed by Shi et al. on the MNIST dataset using CTRAIN. We want to train for certifiable robustness against perturbations in the $l_\\infty$ norm ball around inputs with radius $\\epsilon=0.1$. For that, we utilise IBP training with the improvements by Shi et al.</p>"},{"location":"examples/hyperparameter_optimisation/","title":"Hyperparameter Optimisation using CTRAIN","text":"In\u00a0[1]: Copied! <pre>import torch\n\nfrom CTRAIN.model_definitions import CNN7_Shi\nfrom CTRAIN.model_wrappers import ShiIBPModelWrapper\nfrom CTRAIN.data_loaders import load_mnist\n</pre> import torch  from CTRAIN.model_definitions import CNN7_Shi from CTRAIN.model_wrappers import ShiIBPModelWrapper from CTRAIN.data_loaders import load_mnist <pre>Adding complete_verifier to sys.path\n</pre> <p>Thereafter, we load the MNIST dataset and define the neural network.</p> In\u00a0[2]: Copied! <pre>in_shape = [1, 28, 28]\ntrain_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")\n\nmodel = CNN7_Shi(in_shape=in_shape, n_classes=10)\n</pre> in_shape = [1, 28, 28] train_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")  model = CNN7_Shi(in_shape=in_shape, n_classes=10) <pre>MNIST dataset - Min value: -0.4242129623889923, Max value: 2.821486711502075\n</pre> <p>To perform HPO, we have to wrap the network around one of the model wrappers of <code>CTRAIN</code>. Here, we choose the Shi IBP wrapper.</p> In\u00a0[4]: Copied! <pre>wrapped_model = ShiIBPModelWrapper(\n    model, \n    input_shape=in_shape, \n    eps=0.1,\n    num_epochs=70\n)\n</pre> wrapped_model = ShiIBPModelWrapper(     model,      input_shape=in_shape,      eps=0.1,     num_epochs=70 ) <p>Thereafter we perform the parameter tuning, while evaluating probed configurations on the test set. Furthermore, we provide sensible defaults to guide the optimisation. To save resources, we do not execute the HPO in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>wrapped_model.hpo(train_loader=train_loader, val_loader=test_loader, defaults={\n    'warm_up_epochs': 0,\n    'ramp_up_epochs': 50,\n    'lr_decay_factor': 0.2,\n    'lr_decay_epoch_1': 10, # added unto warm_up and ramp_up epochs\n    'lr_decay_epoch_2': 10, # added unto warm_up, ramp_up and lr_decay_1 epochs\n    'l1_reg_weight': 1e-06,\n    'shi_reg_weight': 1,\n    'shi_reg_decay': True, \n    'train_eps_factor': 1,\n    'optimizer_func': 'adam',\n    'learning_rate': 5e-04,\n    'start_kappa': 1,\n    'end_kappa': 0\n}, output_dir='./smac/shi_mnist_0.1/')\n</pre> wrapped_model.hpo(train_loader=train_loader, val_loader=test_loader, defaults={     'warm_up_epochs': 0,     'ramp_up_epochs': 50,     'lr_decay_factor': 0.2,     'lr_decay_epoch_1': 10, # added unto warm_up and ramp_up epochs     'lr_decay_epoch_2': 10, # added unto warm_up, ramp_up and lr_decay_1 epochs     'l1_reg_weight': 1e-06,     'shi_reg_weight': 1,     'shi_reg_decay': True,      'train_eps_factor': 1,     'optimizer_func': 'adam',     'learning_rate': 5e-04,     'start_kappa': 1,     'end_kappa': 0 }, output_dir='./smac/shi_mnist_0.1/') <p>Finally, we save the model trained on the optimal configuration and evaluate it.</p> In\u00a0[\u00a0]: Copied! <pre>torch.save(wrapped_model.state_dict(), './shi_incumbent_cifar10_2_255.pt')\n\nwrapped_model.eval()\n\nstd_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader=test_loader, test_samples=1_000)\n\nprint(f\"Std Acc: {std_acc}, Cert. Acc: {cert_acc}, Adv. Acc: {adv_acc}\")\n</pre> torch.save(wrapped_model.state_dict(), './shi_incumbent_cifar10_2_255.pt')  wrapped_model.eval()  std_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader=test_loader, test_samples=1_000)  print(f\"Std Acc: {std_acc}, Cert. Acc: {cert_acc}, Adv. Acc: {adv_acc}\")"},{"location":"examples/hyperparameter_optimisation/#hyperparameter-optimisation-using-ctrain","title":"Hyperparameter Optimisation using CTRAIN\u00b6","text":"<p>CTRAIN offers seamless integration of sophisticated hyperparameter optimisation using SMAC3. First, we import the necessary <code>torch</code> library and <code>CTRAIN</code> functions</p>"},{"location":"examples/model_evaluation/","title":"Evaluation of models using CTRAIN","text":"<p>First, we import the necessary <code>torch</code> library and <code>CTRAIN</code> functions</p> In\u00a0[1]: Copied! <pre>import torch\n\nfrom CTRAIN.model_definitions import CNN7_Shi\nfrom CTRAIN.model_wrappers import ShiIBPModelWrapper\nfrom CTRAIN.data_loaders import load_mnist\n</pre> import torch  from CTRAIN.model_definitions import CNN7_Shi from CTRAIN.model_wrappers import ShiIBPModelWrapper from CTRAIN.data_loaders import load_mnist <pre>Adding complete_verifier to sys.path\n</pre> <p>Thereafter, we load the MNIST dataset and define the neural network.</p> In\u00a0[2]: Copied! <pre>in_shape = [1, 28, 28]\ntrain_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")\n\nmodel = CNN7_Shi(in_shape=in_shape, n_classes=10)\n</pre> in_shape = [1, 28, 28] train_loader, test_loader = load_mnist(batch_size=128, val_split=False, data_root=\"../../data\")  model = CNN7_Shi(in_shape=in_shape, n_classes=10) <pre>MNIST dataset - Min value: -0.4242129623889923, Max value: 2.821486711502075\n</pre> <p>To evaluate the network, we have to wrap it around one of the model wrappers of <code>CTRAIN</code>. Here, we choose the Shi IBP wrapper, but all wrappers behave the same regarding evaluation.</p> In\u00a0[3]: Copied! <pre>wrapped_model = ShiIBPModelWrapper(\n    model, \n    input_shape=in_shape, \n    eps=0.1,\n    num_epochs=70\n)\n</pre> wrapped_model = ShiIBPModelWrapper(     model,      input_shape=in_shape,      eps=0.1,     num_epochs=70 ) <p>Now, we load the weights obtained from a previous training run (see the tutorial \"Certified Training with CTRAIN\").</p> In\u00a0[4]: Copied! <pre>wrapped_model.load_state_dict(torch.load('../../mnist_0.1_model.pt'))\n</pre> wrapped_model.load_state_dict(torch.load('../../mnist_0.1_model.pt')) Out[4]: <pre>&lt;All keys matched successfully&gt;</pre> <p>To get a rough assessment of the model performance, we call the evaluate function that uses the cheap incomplete verification methods IBP, CROWN-IBP and CROWN for certification. In addition, the PGD attack is run to identify adversarial examples for which the network is not robust. To save resources, we carry out the evaluation only for the first 1000 images of the test set.</p> In\u00a0[11]: Copied! <pre>std_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader, test_samples=1_000)\n</pre> std_acc, cert_acc, adv_acc = wrapped_model.evaluate(test_loader, test_samples=1_000) <pre>79it [00:00, 118.32it/s]</pre> <pre>certified 990.0 / 1024 using IBP\n</pre> <pre>\n10000it [00:02, 4481.84it/s]</pre> <pre>certified 967.0 / 1000 after using CROWN\n</pre> <pre>\n8it [00:08,  1.12s/it]\n</pre> <p>When printing the accuracy values, we see that the network is provably robust for 96.70% of the first 1000 images in the MNIST test set.</p> In\u00a0[12]: Copied! <pre>print(f\"Standard Accuracy {std_acc}\")\nprint(f\"Certified Accuracy {cert_acc}\")\nprint(f\"Adversarial Accuracy {adv_acc}\")\n</pre> print(f\"Standard Accuracy {std_acc}\") print(f\"Certified Accuracy {cert_acc}\") print(f\"Adversarial Accuracy {adv_acc}\") <pre>Standard Accuracy 0.992\nCertified Accuracy 0.9670000672340393\nAdversarial Accuracy 0.978\n</pre> <p>However, these values were obtained using incomplete methods. Let's investigate whether we can achieve a more precise measurement using complete verification with $\\alpha\\beta$-CROWN.</p> In\u00a0[\u00a0]: Copied! <pre>std_acc, cert_acc, adv_acc = wrapped_model.evaluate_complete(test_loader, test_samples=1_000)\n</pre> std_acc, cert_acc, adv_acc = wrapped_model.evaluate_complete(test_loader, test_samples=1_000) In\u00a0[10]: Copied! <pre>print(f\"Standard Accuracy {std_acc}\")\nprint(f\"Certified Accuracy {cert_acc}\")\nprint(f\"Adversarial Accuracy {adv_acc}\")\n</pre> print(f\"Standard Accuracy {std_acc}\") print(f\"Certified Accuracy {cert_acc}\") print(f\"Adversarial Accuracy {adv_acc}\") <pre>Standard Accuracy 0.992\nCertified Accuracy 0.9780000448226929\nAdversarial Accuracy 0.9779999852180481\n</pre> <p>After the complete evaluation, we see that we got a definitive result for each input as indicated by matching certified and adversarial accuracy. Complete verification revealed, that every input for which we could not find an adversarial example using PGD is actually certifiably robust. Thus, we conclude that the network achieves a certified accuracy of 97.8% on the first 1000 MNIST test images.</p>"},{"location":"examples/model_evaluation/#evaluation-of-models-using-ctrain","title":"Evaluation of models using CTRAIN\u00b6","text":"<p>In this example, we evaluate a network trained on the MNIST dataset against $l_\\infty$ perturbations with radius $\\epsilon = 0.1$ in terms of standard accuracy, adversarial accuracy and certified accuracy.</p>"}]}